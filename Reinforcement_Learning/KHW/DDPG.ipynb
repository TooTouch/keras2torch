{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dying-praise",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-glossary",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-mason",
   "metadata": {},
   "source": [
    "Deep Deterministic Policy Gradient(DDPG)는 continuous actions 학습을 위한 model-free off-policy algorithm이다.\n",
    "\n",
    "DPG(Deteministic Policy Gradient)와 DQN(Deep Q-Network)의 아이디어를 합친 것.\n",
    "\n",
    "DPG를 기반으로 Replay buffer와 target Q network 사용하여 continuous action spaces에서 작동하도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-greensboro",
   "metadata": {},
   "source": [
    "<img src='../img/RL04.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-calculation",
   "metadata": {},
   "source": [
    "### On-policy VS Off-policy\n",
    "\n",
    "<img src='../img/ddpg02.png' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-stability",
   "metadata": {},
   "source": [
    "- On-policy\n",
    "\n",
    "On policy는 behavior policy와 target policy가 같다. \n",
    "\n",
    "즉 현재 행동하는 policy를 그대로 update할 목적으로 환경을 탐색. \n",
    "\n",
    "현재 policy를 통해 얻은 trajectories를 가지고 policy를 update를 하기 때문에 얻어지는 state distribution 자체가 현재 policy에 의존적. \n",
    "\n",
    "그렇기 때문에, data dependent. \n",
    "\n",
    "이는 local optimal에 수렴할 수 있는 가능성. \n",
    "\n",
    "또한 한번 policy를 update한 후, 그 이전의 trajectories는 현재의 policy와 다르기 때문에 더이상 쓸 수 없다. \n",
    "\n",
    "하지만 주로 update할 action selection이 stochastic하기 때문에, exploration strategy에서 off policy보다 편할 수 있다.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha\\cdot(r_s+\\gamma Q(s',a')-Q(s,a))$$\n",
    "\n",
    "- Off-policy\n",
    "\n",
    "Off policy는 behavior policy와 target policy가 다르다. \n",
    "\n",
    "현재 행동하는 policies와 update할 policy가 달라도 된다는 뜻. \n",
    "\n",
    "이는 target policy와 behavior policy에 의한 distribution차이를 Importance Sampling(IS)을 이용해 해결하거나 target policy의 action selection을 주로 max의 연산으로 deterministic하게 취함으로써 해결. \n",
    "\n",
    "approximated action value function $\\hat{Q}$가 있을때, 한 state $s_t$에서의 $\\hat{Q}(s,a)$의 더 정확한 값은 $r(s,a)+max_{a'}\\hat{Q}(s',a')$이기 때문에 업데이트 할 수 있다. \n",
    "\n",
    "이는 target policy의 action을 제한시켰기 때문에, local optimal에 빠지지 않기 위해 exploration strategy가 필요. \n",
    "\n",
    "그중 가장 간단하게는 epsilon-greedy를 사용하지만, 이는 비효율적이기 때문에, 또 다른 많은 exploration 전략이 존재.\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a)+\\alpha\\cdot(r_s+\\gamma \\underset{a'}{max}Q(s',a')-Q(s,a))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-operator",
   "metadata": {},
   "source": [
    "### DQN\n",
    "\n",
    "DQN의 가장 큰 contribution은 두 가지 아이디어로 Q-learning 알고리즘을 개선해서 neural network predictor 적용에 성공한 것. \n",
    "\n",
    "1. experience replay\n",
    "    - 인접한 학습 데이터 사이의 correlation으로 인한 비효율성을 극복하기 위한 기법 \n",
    "    - 게임을 하는 agent의 경험 데이터$(s,a,r,s')$를 replay memory라는 이름의 buffer pool에 매 순간 저장\n",
    "    - update 할 때는 replay memory에서 random하게 minibatch 크기의 sample을 뽑아 계산하는 것\n",
    "\n",
    "\n",
    "2. target network\n",
    "    - DQN과 똑같은 neural network을 하나 더 만들어, 그 weight 값이 가끔씩만 update 되도록 한 것\n",
    "    - $Q(s,a)$를 학습하는 순간, target 값도 따라 변하면서 학습 성능이 떨어지는 문제를 개선하기 위해서\n",
    "    - Target network의 weight 값들은 주기적으로 DQN의 값을 복사. \n",
    "    - Q-learning의 update에서 아래 식과 같은 loss function을 사용하는데, 먼저 나오는 Q는 target network에서 계산한 것이고 뒤의 Q는 원래의 DQN에서 계산한 것\n",
    "    \n",
    "$$L_i(\\theta_i)=E_{(s,a,r,s')~U(D)}[(r+\\gamma\\underset{a'}{\\mathrm{max}}Q(s',a';\\theta^{-}_i)-Q(s,a;\\theta_i))^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-going",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T00:59:31.597533Z",
     "start_time": "2021-05-02T00:59:31.593678Z"
    }
   },
   "source": [
    "### Actor Critic\n",
    "\n",
    "Actor-Critic 알고리즘은 Actor 네트워크와 Critic 네트워크라는 두 개의 네트워크를 사용\n",
    "\n",
    "Actor는 상태가 주어졌을 때 행동을 결정하고, Critic은 상태의 가치를 평가\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-reservoir",
   "metadata": {},
   "source": [
    "## DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-bronze",
   "metadata": {},
   "source": [
    "### DQN의 한계점\n",
    "\n",
    "1. discrete, low-dimentional action spaces만 다룰 수 있다.\n",
    "2. 모든 단계에서 반복적인 최적화 프로세스를 필요로 하는 action-value function을 maximize하는 행동을 찾는 것에 의존하기 때문에 continuous domain에 직접 적용할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-caribbean",
   "metadata": {},
   "source": [
    "### Contribution\n",
    "\n",
    "1. replay buffer\n",
    "\n",
    "- 강화학습을 neural networks를 사용하여 학습할 때 대부분의 optimization algorithm이 sample이 독립적이고 동일하게 분포되어 있다고 가정하는 문제점\n",
    "- 이를 해결하기 위해 replay buffer 사용\n",
    "- $(s_t,a_t,r_t,s_{t+1})$\n",
    "- 각 timestep에서 actor와 critic은 buffer에서 균일하게 mini-batch를 샘플링하여 업데이트하고 uncorrelated transitions를 학습할 수 있는 이점\n",
    "\n",
    "2. soft target update\n",
    "\n",
    "- neural network를 사용하여 Q-learning을 직접 구현하는 것은 불안정\n",
    "- 업데이트중인 network $Q(s,a|\\theta^Q)$도 target value를 계산하는데 사용되기 때문에 업데이트가 발산하는 경향\n",
    "- DQN에서 target network를 사용하는 것과 유사하지만 가중치를 직접 복사하는 대신 Actor, Critic에 \"soft\" target update\n",
    "- target value 생성을 위해 actor($Q'(s,a|\\theta^{Q'}$)와 critic($\\mu'(s|\\theta^{\\mu'}$) network 복사\n",
    "- 그 다음 학습된 network를 천천히 추적하도록 target network의 가중치 업데이트\n",
    "$$\\theta' \\leftarrow \\tau\\theta+(1-\\tau)\\theta' \\;\\;\\; \\mathrm{with} \\;\\;\\tau << 1$$\n",
    "- target value가 천천히 변경되도록 제한하여 학습의 안정성을 향상(학습 속도는 느려질 수 있음)\n",
    "\n",
    "3. batch normalization\n",
    "\n",
    "- 서로 다른 구성요소는 서로 다른 물리적 단위를 가질 수 있고 범위가 environment 마다 다르다.\n",
    "- 이는 network가 효과적으로 학습하기 어렵고 일반화되는 hyper parameter를 찾기 어려울 수 있다.\n",
    "- 이를 해결하기 위해 batch normalization 사용\n",
    "- 단위 평균과 분산을 갖도록 mini-batch의 sample에 대한 각 차원을 정규화\n",
    "- 또한 테스트 중 정규화에 사용할 mean, variance의 running average를 유지\n",
    "\n",
    "4. noise process\n",
    "\n",
    "- noise process $N$에서 샘플링된 noise를 actor policy에 추가하여 탐색 정책 $\\mu'$ 구성\n",
    "\n",
    "$$\\mu'(s_t)=\\mu(s_t|\\theta_t^{\\mu})+N$$\n",
    "\n",
    "- Ornstein-Uhlenbeck process 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-highway",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-novel",
   "metadata": {},
   "source": [
    "Inverted Pendulum control problem을 play\n",
    "\n",
    "Action : swing left or swing right.\n",
    "\n",
    "Actions가 discrete하지 않고 continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fantastic-norway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.825549Z",
     "start_time": "2021-05-01T17:01:28.958393Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beneficial-navigation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.856342Z",
     "start_time": "2021-05-01T17:01:29.829900Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "partial-worthy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.893330Z",
     "start_time": "2021-05-01T17:01:29.859688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-baghdad",
   "metadata": {},
   "source": [
    "### Ornstein-Uhlenbeck process\n",
    "\n",
    "$$dx_t=\\theta (\\mu-x_t)+\\sigma dW_t$$\n",
    "\n",
    "- OU Process는 평균으로 회귀하는 random process\n",
    "- θ는 얼마나 빨리 평균으로 회귀할 지를 나타내는 파라미터이며 μ는 평균\n",
    "- σ는 process의 변동성을 의미하며 Wt는 Wiener process(변수의 변화가 평균이 0이고 분산이 1인 특성을 가지는 확률과정)를 의미\n",
    "- 따라서 이전의 noise들과 temporally correlated\n",
    "- 위와 같은 temporally correlated noise process를 사용하는 이유는 physical control과 같은 관성이 있는 환경에서 학습 시킬 때 보다 효과적이기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "frozen-special",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.901733Z",
     "start_time": "2021-05-01T17:01:29.894772Z"
    }
   },
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-italy",
   "metadata": {},
   "source": [
    "<img src='../img/ddpg01.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-venue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.918901Z",
     "start_time": "2021-05-01T17:01:29.903185Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        critic_optimizer.zero_grad(set_to_none=True)\n",
    "        target_actions = target_actor(next_state_batch)\n",
    "        y = reward_batch + gamma * target_critic(\n",
    "            next_state_batch, target_actions\n",
    "        )\n",
    "        critic_value = critic_model(state_batch, action_batch)\n",
    "        critic_loss = torch.mean(torch.square(y - critic_value))\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        actor_optimizer.zero_grad(set_to_none=True)\n",
    "        actions = actor_model(state_batch)\n",
    "        critic_value = critic_model(state_batch, actions)\n",
    "        actor_loss = -torch.mean(critic_value)\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = torch.from_numpy(self.state_buffer[batch_indices]).float().to(device)\n",
    "        action_batch = torch.from_numpy(self.action_buffer[batch_indices]).float().to(device)\n",
    "        reward_batch = torch.from_numpy(self.reward_buffer[batch_indices])\n",
    "        reward_batch = reward_batch.type(torch.FloatTensor).to(device)\n",
    "        next_state_batch = torch.from_numpy(self.next_state_buffer[batch_indices]).float().to(device)\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.data.copy_(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-pollution",
   "metadata": {},
   "source": [
    "$$L_i(\\theta_i)=E_{(s,a,r,s')~U(D)}[(r+\\gamma\\underset{a'}{\\mathrm{max}}Q(s',a';\\theta^{-}_i)-Q(s,a;\\theta_i))^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-criterion",
   "metadata": {},
   "source": [
    "### actor, critic\n",
    "\n",
    "<img src='../img/ddpg03.png' width='500'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "framed-words",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.931561Z",
     "start_time": "2021-05-01T17:01:29.920160Z"
    }
   },
   "outputs": [],
   "source": [
    "class get_actor(nn.Module):\n",
    "    def __init__(self, num_states, upper_bound):\n",
    "        super(get_actor, self).__init__()\n",
    "        \n",
    "        self.upper_bound = torch.as_tensor(upper_bound).to(device)\n",
    "\n",
    "        Linear1 = nn.Linear(num_states, 256)\n",
    "        Linear2 = nn.Linear(256, 256)\n",
    "        Linear3 = nn.Linear(256, 1)\n",
    "        \n",
    "        nn.init.uniform_(Linear3.weight, a=-0.003, b=0.003)\n",
    "        \n",
    "        self.actor_layer = nn.Sequential(\n",
    "            Linear1,\n",
    "            nn.ReLU(),\n",
    "            Linear2,\n",
    "            nn.ReLU(),\n",
    "            Linear3,\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.actor_layer(inputs)\n",
    "        return output * self.upper_bound\n",
    "\n",
    "class get_critic(nn.Module):\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(get_critic, self).__init__()\n",
    "                \n",
    "        self.state_layer = nn.Sequential(\n",
    "            nn.Linear(num_states, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.action_layer = nn.Sequential(\n",
    "            nn.Linear(num_actions, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.out_layer = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state_input, action_input):\n",
    "        state_out = self.state_layer(state_input)\n",
    "        action_out = self.action_layer(action_input)\n",
    "        concat = torch.cat((state_out, action_out), 1)\n",
    "        output = self.out_layer(concat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "looking-lunch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-01T17:01:29.939564Z",
     "start_time": "2021-05-01T17:01:29.934137Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy(state, noise_object, upper_bound, lower_bound):\n",
    "    sampled_actions = torch.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.detach().cpu().numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "potential-vaccine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T19:18:40.342448Z",
     "start_time": "2021-05-02T19:18:40.312077Z"
    }
   },
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor(num_states, upper_bound).to(device)\n",
    "critic_model = get_critic(num_states, num_actions).to(device)\n",
    "\n",
    "target_actor = get_actor(num_states, upper_bound).to(device)\n",
    "target_critic = get_critic(num_states, num_actions).to(device)\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.load_state_dict(actor_model.state_dict())\n",
    "target_critic.load_state_dict(critic_model.state_dict())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = optim.Adam(critic_model.parameters(), lr=critic_lr)\n",
    "actor_optimizer = optim.Adam(actor_model.parameters(), lr=actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mighty-disorder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T19:21:17.444589Z",
     "start_time": "2021-05-02T19:18:40.901111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1828.1104350351843\n",
      "Episode * 1 * Avg Reward is ==> -1359.005424850464\n",
      "Episode * 2 * Avg Reward is ==> -1201.0969984522255\n",
      "Episode * 3 * Avg Reward is ==> -1181.0072726495396\n",
      "Episode * 4 * Avg Reward is ==> -1158.3771528731752\n",
      "Episode * 5 * Avg Reward is ==> -1174.179801978392\n",
      "Episode * 6 * Avg Reward is ==> -1191.8360957868788\n",
      "Episode * 7 * Avg Reward is ==> -1259.95065625434\n",
      "Episode * 8 * Avg Reward is ==> -1304.9775454291876\n",
      "Episode * 9 * Avg Reward is ==> -1323.647065888557\n",
      "Episode * 10 * Avg Reward is ==> -1296.357717669625\n",
      "Episode * 11 * Avg Reward is ==> -1312.4955347474634\n",
      "Episode * 12 * Avg Reward is ==> -1307.1410655381237\n",
      "Episode * 13 * Avg Reward is ==> -1295.8500341730828\n",
      "Episode * 14 * Avg Reward is ==> -1287.642883260109\n",
      "Episode * 15 * Avg Reward is ==> -1270.251537210312\n",
      "Episode * 16 * Avg Reward is ==> -1259.8241618881261\n",
      "Episode * 17 * Avg Reward is ==> -1245.186677682304\n",
      "Episode * 18 * Avg Reward is ==> -1231.8900052840856\n",
      "Episode * 19 * Avg Reward is ==> -1219.649812791547\n",
      "Episode * 20 * Avg Reward is ==> -1173.9708488912195\n",
      "Episode * 21 * Avg Reward is ==> -1155.992466706447\n",
      "Episode * 22 * Avg Reward is ==> -1105.7732800560223\n",
      "Episode * 23 * Avg Reward is ==> -1081.5517439467337\n",
      "Episode * 24 * Avg Reward is ==> -1043.2857895546822\n",
      "Episode * 25 * Avg Reward is ==> -1007.9312606648083\n",
      "Episode * 26 * Avg Reward is ==> -975.1477451328375\n",
      "Episode * 27 * Avg Reward is ==> -949.5282428850153\n",
      "Episode * 28 * Avg Reward is ==> -925.797827460335\n",
      "Episode * 29 * Avg Reward is ==> -899.4122320102543\n",
      "Episode * 30 * Avg Reward is ==> -878.026577812305\n",
      "Episode * 31 * Avg Reward is ==> -862.5434951910474\n",
      "Episode * 32 * Avg Reward is ==> -836.4596767264514\n",
      "Episode * 33 * Avg Reward is ==> -822.9889979390427\n",
      "Episode * 34 * Avg Reward is ==> -803.101469985593\n",
      "Episode * 35 * Avg Reward is ==> -791.5908079707929\n",
      "Episode * 36 * Avg Reward is ==> -773.526923921291\n",
      "Episode * 37 * Avg Reward is ==> -763.8627952421004\n",
      "Episode * 38 * Avg Reward is ==> -760.5473000200278\n",
      "Episode * 39 * Avg Reward is ==> -744.6751015161642\n",
      "Episode * 40 * Avg Reward is ==> -699.0011802012107\n",
      "Episode * 41 * Avg Reward is ==> -683.2469620285399\n",
      "Episode * 42 * Avg Reward is ==> -667.2925936306524\n",
      "Episode * 43 * Avg Reward is ==> -642.4714093593032\n",
      "Episode * 44 * Avg Reward is ==> -622.056353758579\n",
      "Episode * 45 * Avg Reward is ==> -593.8815738940884\n",
      "Episode * 46 * Avg Reward is ==> -572.696199338611\n",
      "Episode * 47 * Avg Reward is ==> -529.3033794377938\n",
      "Episode * 48 * Avg Reward is ==> -496.9679648083671\n",
      "Episode * 49 * Avg Reward is ==> -462.62659386131656\n",
      "Episode * 50 * Avg Reward is ==> -437.0466933722417\n",
      "Episode * 51 * Avg Reward is ==> -402.7991839930741\n",
      "Episode * 52 * Avg Reward is ==> -374.87805547944396\n",
      "Episode * 53 * Avg Reward is ==> -355.1504843852069\n",
      "Episode * 54 * Avg Reward is ==> -334.75080595399805\n",
      "Episode * 55 * Avg Reward is ==> -312.64162993635006\n",
      "Episode * 56 * Avg Reward is ==> -291.1277545979606\n",
      "Episode * 57 * Avg Reward is ==> -266.247661715163\n",
      "Episode * 58 * Avg Reward is ==> -244.4273641661649\n",
      "Episode * 59 * Avg Reward is ==> -222.91203805609794\n",
      "Episode * 60 * Avg Reward is ==> -225.48683752735982\n",
      "Episode * 61 * Avg Reward is ==> -215.31532016923364\n",
      "Episode * 62 * Avg Reward is ==> -227.45161761177033\n",
      "Episode * 63 * Avg Reward is ==> -225.95299116699016\n",
      "Episode * 64 * Avg Reward is ==> -222.84234791608122\n",
      "Episode * 65 * Avg Reward is ==> -222.71466353503484\n",
      "Episode * 66 * Avg Reward is ==> -219.71240481342085\n",
      "Episode * 67 * Avg Reward is ==> -216.4214459726965\n",
      "Episode * 68 * Avg Reward is ==> -212.87612173605353\n",
      "Episode * 69 * Avg Reward is ==> -217.27467229928885\n",
      "Episode * 70 * Avg Reward is ==> -214.42051938957766\n",
      "Episode * 71 * Avg Reward is ==> -210.81146883345363\n",
      "Episode * 72 * Avg Reward is ==> -214.0255076770496\n",
      "Episode * 73 * Avg Reward is ==> -207.63979628381776\n",
      "Episode * 74 * Avg Reward is ==> -207.407943942955\n",
      "Episode * 75 * Avg Reward is ==> -197.70631046275562\n",
      "Episode * 76 * Avg Reward is ==> -197.69240878967722\n",
      "Episode * 77 * Avg Reward is ==> -193.30643024242363\n",
      "Episode * 78 * Avg Reward is ==> -180.49126785802528\n",
      "Episode * 79 * Avg Reward is ==> -183.07846948823962\n",
      "Episode * 80 * Avg Reward is ==> -186.13219659770283\n",
      "Episode * 81 * Avg Reward is ==> -182.68077079251856\n",
      "Episode * 82 * Avg Reward is ==> -182.19750809336387\n",
      "Episode * 83 * Avg Reward is ==> -182.26525507806164\n",
      "Episode * 84 * Avg Reward is ==> -179.22654162784573\n",
      "Episode * 85 * Avg Reward is ==> -176.11516786739008\n",
      "Episode * 86 * Avg Reward is ==> -167.7582861343606\n",
      "Episode * 87 * Avg Reward is ==> -170.65453743712035\n",
      "Episode * 88 * Avg Reward is ==> -164.33236862898548\n",
      "Episode * 89 * Avg Reward is ==> -164.59075344588388\n",
      "Episode * 90 * Avg Reward is ==> -167.6429483063745\n",
      "Episode * 91 * Avg Reward is ==> -167.77843214148487\n",
      "Episode * 92 * Avg Reward is ==> -170.8297422375668\n",
      "Episode * 93 * Avg Reward is ==> -164.91520929163892\n",
      "Episode * 94 * Avg Reward is ==> -158.97414950155564\n",
      "Episode * 95 * Avg Reward is ==> -158.82731638360968\n",
      "Episode * 96 * Avg Reward is ==> -156.16778885895098\n",
      "Episode * 97 * Avg Reward is ==> -159.14514396881378\n",
      "Episode * 98 * Avg Reward is ==> -162.3229849788955\n",
      "Episode * 99 * Avg Reward is ==> -167.58298048050028\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        torch_prev_state = torch.unsqueeze(torch.Tensor(prev_state), 0).to(device)\n",
    "\n",
    "        action = policy(torch_prev_state, ou_noise, upper_bound, lower_bound)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.parameters(), actor_model.parameters(), tau)\n",
    "        update_target(target_critic.parameters(), critic_model.parameters(), tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adult-andrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T19:38:43.128831Z",
     "start_time": "2021-05-02T19:38:43.103566Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "torch.save(actor_model.state_dict(), \"pendulum_actor.pt\")\n",
    "torch.save(critic_model.state_dict(), \"pendulum_critic.pt\")\n",
    "\n",
    "torch.save(target_actor.state_dict(), \"pendulum_target_actor.pt\")\n",
    "torch.save(target_critic.state_dict(), \"pendulum_target_critic.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "graphic-metro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-02T19:38:46.584013Z",
     "start_time": "2021-05-02T19:38:43.574255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "\n",
    "def render_episode(env): \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    im = Image.fromarray(screen)\n",
    "\n",
    "    images = [im]\n",
    "  \n",
    "    state = env.reset()\n",
    "    while True:\n",
    "\n",
    "        torch_prev_state = torch.unsqueeze(torch.Tensor(prev_state), 0).to(device)\n",
    "\n",
    "        action = actor_model(torch_prev_state).detach().cpu()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        images.append(Image.fromarray(screen))\n",
    "  \n",
    "        if done:\n",
    "            break\n",
    "  \n",
    "    return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env)\n",
    "image_file = 'Pendulum-v0.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-guinea",
   "metadata": {},
   "source": [
    "<img src='../img/ddpg04.gif' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-building",
   "metadata": {},
   "source": [
    "<img src='../img/ddpg05.gif' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-toronto",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
