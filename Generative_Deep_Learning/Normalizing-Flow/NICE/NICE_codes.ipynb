{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2],\n",
       "       [ 4,  6],\n",
       "       [ 8, 10],\n",
       "       [12, 14]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_even = lambda xs: xs[:,0::2]\n",
    "_get_odd = lambda xs: xs[:,1::2]\n",
    "_get_even(np.arange(16).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 5,  7],\n",
       "       [ 9, 11],\n",
       "       [13, 15]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_odd(np.arange(16).reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of NICE bijective triangular-jacobian layers.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# ===== ===== Coupling Layer Implementations ===== =====\n",
    "\n",
    "_get_even = lambda xs: xs[:,0::2]\n",
    "_get_odd = lambda xs: xs[:,1::2]\n",
    "\n",
    "def _interleave(first, second, order):\n",
    "    \"\"\"\n",
    "    Given 2 rank-2 tensors with same batch dimension, interleave their columns.\n",
    "    \n",
    "    The tensors \"first\" and \"second\" are assumed to be of shape (B,M) and (B,N)\n",
    "    where M = N or N+1, repsectively.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    if order == 'even':\n",
    "        for k in range(second.shape[1]):\n",
    "            cols.append(first[:,k])\n",
    "            cols.append(second[:,k])\n",
    "        if first.shape[1] > second.shape[1]:\n",
    "            cols.append(first[:,-1])\n",
    "    else:\n",
    "        for k in range(first.shape[1]):\n",
    "            cols.append(second[:,k])\n",
    "            cols.append(first[:,k])\n",
    "        if second.shape[1] > first.shape[1]:\n",
    "            cols.append(second[:,-1])\n",
    "    return torch.stack(cols, dim=1)\n",
    "\n",
    "\n",
    "class _BaseCouplingLayer(nn.Module):\n",
    "    def __init__(self, dim, partition, nonlinearity):\n",
    "        \"\"\"\n",
    "        Base coupling layer that handles the permutation of the inputs and wraps\n",
    "        an instance of torch.nn.Module.\n",
    "\n",
    "        Usage:\n",
    "        >> layer = AdditiveCouplingLayer(1000, 'even', nn.Sequential(...))\n",
    "        \n",
    "        Args:\n",
    "        * dim: dimension of the inputs.\n",
    "        * partition: str, 'even' or 'odd'. If 'even', the even-valued columns are sent to\n",
    "        pass through the activation module.\n",
    "        * nonlinearity: an instance of torch.nn.Module.\n",
    "        \"\"\"\n",
    "        super(_BaseCouplingLayer, self).__init__()\n",
    "        # store input dimension of incoming values:\n",
    "        self.dim = dim\n",
    "        # store partition choice and make shorthands for 1st and second partitions:\n",
    "        assert (partition in ['even', 'odd']), \"[_BaseCouplingLayer] Partition type must be `even` or `odd`!\"\n",
    "        self.partition = partition\n",
    "        if (partition == 'even'):\n",
    "            self._first = _get_even\n",
    "            self._second = _get_odd\n",
    "        else:\n",
    "            self._first = _get_odd\n",
    "            self._second = _get_even\n",
    "        # store nonlinear function module:\n",
    "        # (n.b. this can be a complex instance of torch.nn.Module, for ex. a deep ReLU network)\n",
    "        self.add_module('nonlinearity', nonlinearity)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Map an input through the partition and nonlinearity.\"\"\"\n",
    "        return _interleave(\n",
    "            self._first(x),\n",
    "            self.coupling_law(self._second(x), self.nonlinearity(self._first(x))),\n",
    "            self.partition\n",
    "        )\n",
    "\n",
    "    def inverse(self, y):\n",
    "        \"\"\"Inverse mapping through the layer. Gradients should be turned off for this pass.\"\"\"\n",
    "        return _interleave(\n",
    "            self._first(y),\n",
    "            self.anticoupling_law(self._second(y), self.nonlinearity(self._first(y))),\n",
    "            self.partition\n",
    "        )\n",
    "\n",
    "    def coupling_law(self, a, b):\n",
    "        # (a,b) --> g(a,b)\n",
    "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
    "\n",
    "    def anticoupling_law(self, a, b):\n",
    "        # (a,b) --> g^{-1}(a,b)\n",
    "        raise NotImplementedError(\"[_BaseCouplingLayer] Don't call abstract base layer!\")\n",
    "\n",
    "\n",
    "class AdditiveCouplingLayer(_BaseCouplingLayer):\n",
    "    \"\"\"Layer with coupling law g(a;b) := a + b.\"\"\"\n",
    "    def coupling_law(self, a, b):\n",
    "        # a : x(sec), b : non-lin(x(first))\n",
    "        return (a + b)\n",
    "    def anticoupling_law(self, a, b):\n",
    "        # a : y(sec), b : non-lin(y(first)) <== changed in x position of 'coupling_law function'\n",
    "        return (a - b)\n",
    "\n",
    "\n",
    "class MultiplicativeCouplingLayer(_BaseCouplingLayer):\n",
    "    \"\"\"Layer with coupling law g(a;b) := a .* b.\"\"\"\n",
    "    def coupling_law(self, a, b):\n",
    "        return torch.mul(a,b)\n",
    "    def anticoupling_law(self, a, b):\n",
    "        return torch.mul(a, torch.reciprocal(b))\n",
    "\n",
    "\n",
    "class AffineCouplingLayer(_BaseCouplingLayer):\n",
    "    \"\"\"Layer with coupling law g(a;b) := a .* b1 + b2, where (b1,b2) is a partition of b.\"\"\"\n",
    "    def coupling_law(self, a, b):\n",
    "        return torch.mul(a, self._first(b)) + self._second(b)\n",
    "    def anticoupling_law(self, a, b):\n",
    "        # TODO\n",
    "        raise NotImplementedError(\"TODO: AffineCouplingLayer (sorry!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of models from paper.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from .layers import AdditiveCouplingLayer\n",
    "\n",
    "def _build_relu_network(latent_dim, hidden_dim, num_layers):\n",
    "    \"\"\"Helper function to construct a ReLU network of varying number of layers.\"\"\"\n",
    "    _modules = [ nn.Linear(latent_dim, hidden_dim) ]\n",
    "    for _ in range(num_layers):\n",
    "        _modules.append( nn.Linear(hidden_dim, hidden_dim) )\n",
    "        _modules.append( nn.ReLU() )\n",
    "        _modules.append( nn.BatchNorm1d(hidden_dim) )\n",
    "    _modules.append( nn.Linear(hidden_dim, latent_dim) )\n",
    "    return nn.Sequential( *_modules )\n",
    "    \n",
    "\n",
    "class NICEModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Replication of model from the paper:\n",
    "      \"Nonlinear Independent Components Estimation\",\n",
    "      Laurent Dinh, David Krueger, Yoshua Bengio (2014)\n",
    "      https://arxiv.org/abs/1410.8516\n",
    "\n",
    "    Contains the following components:\n",
    "    * four additive coupling layers with nonlinearity functions consisting of\n",
    "      five-layer RELUs\n",
    "    * a diagonal scaling matrix output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(NICEModel, self).__init__()\n",
    "        assert (input_dim % 2 == 0), \"[NICEModel] only even input dimensions supported for now\"\n",
    "        assert (num_layers > 2), \"[NICEModel] num_layers must be at least 3\"\n",
    "        self.input_dim = input_dim\n",
    "        half_dim = int(input_dim / 2)\n",
    "        self.layer1 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network(half_dim, hidden_dim, num_layers))\n",
    "        self.layer2 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network(half_dim, hidden_dim, num_layers))\n",
    "        self.layer3 = AdditiveCouplingLayer(input_dim, 'odd', _build_relu_network(half_dim, hidden_dim, num_layers))\n",
    "        self.layer4 = AdditiveCouplingLayer(input_dim, 'even', _build_relu_network(half_dim, hidden_dim, num_layers))\n",
    "        self.scaling_diag = nn.Parameter(torch.ones(input_dim))\n",
    "\n",
    "        # randomly initialize weights:\n",
    "        for p in self.layer1.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity='relu')\n",
    "            else:\n",
    "                init.normal_(p, mean=0., std=0.001)\n",
    "        for p in self.layer2.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity='relu')\n",
    "            else:\n",
    "                init.normal_(p, mean=0., std=0.001)\n",
    "        for p in self.layer3.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity='relu')\n",
    "            else:\n",
    "                init.normal_(p, mean=0., std=0.001)\n",
    "        for p in self.layer4.parameters():\n",
    "            if len(p.shape) > 1:\n",
    "                init.kaiming_uniform_(p, nonlinearity='relu')\n",
    "            else:\n",
    "                init.normal_(p, mean=0., std=0.001)        \n",
    "\n",
    "\n",
    "    def forward(self, xs):\n",
    "        \"\"\"\n",
    "        Forward pass through all invertible coupling layers.\n",
    "        \n",
    "        Args:\n",
    "        * xs: float tensor of shape (B,dim).\n",
    "\n",
    "        Returns:\n",
    "        * ys: float tensor of shape (B,dim).\n",
    "        \"\"\"\n",
    "        ys = self.layer1(xs)\n",
    "        ys = self.layer2(ys)\n",
    "        ys = self.layer3(ys)\n",
    "        ys = self.layer4(ys)\n",
    "        ys = torch.matmul(ys, torch.diag(torch.exp(self.scaling_diag)))\n",
    "        return ys\n",
    "\n",
    "\n",
    "    def inverse(self, ys):\n",
    "        \"\"\"Invert a set of draws from gaussians\"\"\"\n",
    "        with torch.no_grad():\n",
    "            xs = torch.matmul(ys, torch.diag(torch.reciprocal(torch.exp(self.scaling_diag))))\n",
    "            xs = self.layer4.inverse(xs)\n",
    "            xs = self.layer3.inverse(xs)\n",
    "            xs = self.layer2.inverse(xs)\n",
    "            xs = self.layer1.inverse(xs)\n",
    "        return xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for loading, rescaling, image processing.\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "\n",
    "def unflatten_images(input_batch, depth, height, width):\n",
    "    \"\"\"\n",
    "    Take a batch of images and unflatten into a DxHxW grid.\n",
    "    Nearly an inverse of `flatten_images`. (`flatten_images` assumes a list of tensors, not a tensor.)\n",
    "    \n",
    "    Args:\n",
    "    * input_batch: a tensor of dtype=float and shape (bsz, d*h*w).\n",
    "    * depth: int\n",
    "    * height: int\n",
    "    * width: int\n",
    "    \"\"\"\n",
    "    return input_batch.view(input_batch.shape[0], depth, height, width)\n",
    "\n",
    "\n",
    "def rescale(x, lo, hi):\n",
    "    \"\"\"Rescale a tensor to [lo,hi].\"\"\"\n",
    "    assert(lo < hi), \"[rescale] lo={0} must be smaller than hi={1}\".format(lo,hi)\n",
    "    old_width = torch.max(x)-torch.min(x)\n",
    "    old_center = torch.min(x) + (old_width / 2.)\n",
    "    new_width = float(hi-lo)\n",
    "    new_center = lo + (new_width / 2.)\n",
    "    # shift everything back to zero:\n",
    "    x = x - old_center\n",
    "    # rescale to correct width:\n",
    "    x = x * (new_width / old_width)\n",
    "    # shift everything to the new center:\n",
    "    x = x + new_center\n",
    "    # return:\n",
    "    return x\n",
    "\n",
    "\n",
    "def l1_norm(mdl, include_bias=True, device=(torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'))):\n",
    "    \"\"\"Compute L1 norm on all the weights of mdl.\"\"\"\n",
    "    if include_bias:\n",
    "        _norm = torch.tensor(0.0, device=device)\n",
    "        for w in mdl.parameters():\n",
    "            _norm = _norm + w.norm(p=1)\n",
    "        return _norm\n",
    "    else:\n",
    "        _norm = torch.tensor(0.0)\n",
    "        for w in mdl.parameters():\n",
    "            if len(w.shape) > 1:\n",
    "                _norm = _norm + w.norm(p=1)\n",
    "        return _norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
