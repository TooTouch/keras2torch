{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divided-ottawa",
   "metadata": {},
   "source": [
    "# Semantic Similarity with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-worse",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "working-indian",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-consent",
   "metadata": {},
   "source": [
    "의미 론적 유사성은 두 문장이 의미하는 측면에서 얼마나 유사한 지 결정하는 작업입니다. 이 예제는 SNLI (Stanford Natural Language Inference) Corpus를 사용하여 Transformer와의 문장 의미 유사성을 예측하는 방법을 보여줍니다. 두 문장을 입력으로 받아이 두 문장에 대한 유사성 점수를 출력하는 BERT 모델을 미세 조정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-cache",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-bachelor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T00:30:41.875032Z",
     "start_time": "2021-03-14T00:30:41.870595Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abandoned-judge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:08.480903Z",
     "start_time": "2021-03-14T16:14:07.677698Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary as summary_\n",
    "import transformers\n",
    "import sys\n",
    "from torchtext.legacy import data, datasets\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unnecessary-spirit",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:09.454971Z",
     "start_time": "2021-03-14T16:14:09.423776Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unlimited-panama",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:10.339418Z",
     "start_time": "2021-03-14T16:14:10.334770Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-soldier",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ordinary-population",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:11.772985Z",
     "start_time": "2021-03-14T16:14:11.770076Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 128  # 모델 input sentence의 최대 길이\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# 데이터 셋 레이블\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-interstate",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-panic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T00:30:18.535619Z",
     "start_time": "2021-03-14T00:28:00.480007Z"
    }
   },
   "outputs": [],
   "source": [
    "!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n",
    "!tar -xvzf data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polish-discovery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:14.456436Z",
     "start_time": "2021-03-14T16:14:14.286511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples : 100000\n",
      "Total validation samples: 10000\n",
      "Total test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# There are more than 550k samples in total; we will use 100k for this example.\n",
    "train_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n",
    "valid_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_dev.csv\")\n",
    "test_df = pd.read_csv(\"SNLI_Corpus/snli_1.0_test.csv\")\n",
    "\n",
    "# Shape of the data\n",
    "print(f\"Total train samples : {train_df.shape[0]}\")\n",
    "print(f\"Total validation samples: {valid_df.shape[0]}\")\n",
    "print(f\"Total test samples: {valid_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-script",
   "metadata": {},
   "source": [
    "데이터 세트 개요 :\n",
    "\n",
    "- sentence 1 : 저자에게 제공된 전제 캡션.\n",
    "- sentence 2 : 저자가 작성한 가설 캡션.\n",
    "- similarity : 대부분의 어노 테이터가 선택한 레이블. 다수가 존재하지 않는 경우 레이블 \"-\"가 사용 (여기서는 이러한 샘플을 건너 뛴다).\n",
    "\n",
    "데이터 세트의 \"유사성\"라벨 값\n",
    "\n",
    "- Contradiction(모순) : 문장은 유사성을 공유하지 않는다. \n",
    "- Entailment(수반) : 문장의 의미가 비슷하다.\n",
    "- Neutral(중립) : 문장이 중립적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-interval",
   "metadata": {},
   "source": [
    "데이터 세트의 한 샘플"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "limiting-export",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:16.674137Z",
     "start_time": "2021-03-14T16:14:16.665892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence1: A person on a horse jumps over a broken down airplane.\n",
      "Sentence2: A person is at a diner, ordering an omelette.\n",
      "Similarity: contradiction\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence1: {train_df.loc[1, 'sentence1']}\")\n",
    "print(f\"Sentence2: {train_df.loc[1, 'sentence2']}\")\n",
    "print(f\"Similarity: {train_df.loc[1, 'similarity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-graduate",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stone-pathology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:18.257269Z",
     "start_time": "2021-03-14T16:14:18.198062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values\n",
      "similarity    0\n",
      "sentence1     0\n",
      "sentence2     3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We have some NaN entries in our train data, we will simply drop them.\n",
    "print(\"Number of missing values\")\n",
    "print(train_df.isnull().sum())\n",
    "train_df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-property",
   "metadata": {},
   "source": [
    "traing targets의 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "digital-picnic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:19.651560Z",
     "start_time": "2021-03-14T16:14:19.627797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Target Distribution\n",
      "entailment       33384\n",
      "contradiction    33310\n",
      "neutral          33193\n",
      "-                  110\n",
      "Name: similarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Target Distribution\")\n",
    "print(train_df.similarity.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-westminster",
   "metadata": {},
   "source": [
    "vaildation targets의 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "induced-resort",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:21.318096Z",
     "start_time": "2021-03-14T16:14:21.311695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Target Distribution\n",
      "entailment       3329\n",
      "contradiction    3278\n",
      "neutral          3235\n",
      "-                 158\n",
      "Name: similarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Target Distribution\")\n",
    "print(valid_df.similarity.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-confidentiality",
   "metadata": {},
   "source": [
    "\"-\"값은 교육 및 검증 tagets에서 일부 나타난다. 이 샘플은 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wrapped-bacteria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:22.526312Z",
     "start_time": "2021-03-14T16:14:22.483167Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = (\n",
    "    train_df[train_df.similarity != \"-\"]\n",
    "    .sample(frac=1.0, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "valid_df = (\n",
    "    valid_df[valid_df.similarity != \"-\"]\n",
    "    .sample(frac=1.0, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-parking",
   "metadata": {},
   "source": [
    "One-hot encode training, validation, and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nutritional-testament",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:14:26.567391Z",
     "start_time": "2021-03-14T16:14:26.491572Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"label\"] = train_df[\"similarity\"].apply(\n",
    "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
    ")\n",
    "y_train = torch.LongTensor(train_df[\"label\"].values)\n",
    "\n",
    "valid_df[\"label\"] = valid_df[\"similarity\"].apply(\n",
    "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
    ")\n",
    "y_val = torch.LongTensor(valid_df[\"label\"].values)\n",
    "\n",
    "test_df[\"label\"] = test_df[\"similarity\"].apply(\n",
    "    lambda x: 0 if x == \"contradiction\" else 1 if x == \"entailment\" else 2\n",
    ")\n",
    "y_test = torch.LongTensor(test_df[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-billy",
   "metadata": {},
   "source": [
    "## Create a custom data generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-highway",
   "metadata": {},
   "source": [
    "sentence1과 sentence2를 묶어 pair로 사용.\n",
    "\n",
    "batch size 별로 batch data 생성\n",
    "\n",
    "셔플할건지와 타겟도 포함할 건지 선택\n",
    "\n",
    "pretrained된 Bert tokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "conditional-windows",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:30:35.264512Z",
     "start_time": "2021-03-14T16:30:35.251862Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator:\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        \n",
    "        # base-base-uncased pretrained model bert tokenizer 사용\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 배치 검색\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # batch_encode_plus 배치를 사용하면 \n",
    "        # 두 문장의 배치가 함께 있고 [SEP] 토큰으로 구분\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # 인코딩 된 feature를 numpy 배열로 변환\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # 훈련 / 검증에 사용되는 경우 true로 설정\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # 각 Epoch 후에 인덱스를 섞는다.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-gender",
   "metadata": {},
   "source": [
    "## 모델 아키텍쳐 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-louisiana",
   "metadata": {},
   "source": [
    "1. pretrained Bert\n",
    "2. bi-LSTM\n",
    "3. Global Avg Pool 1d\n",
    "4. Global Max Pool 1d\n",
    "5. 3+4 concat\n",
    "6. dropout 0.3\n",
    "7. fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "residential-fiction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:30:39.099035Z",
     "start_time": "2021-03-14T16:30:39.089381Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertSemanticSimilarity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertSemanticSimilarity, self).__init__()\n",
    "        \n",
    "        self.bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model.trainable = False\n",
    "        \n",
    "        self.emb_dim = self.bert_model.config.to_dict()['hidden_size']\n",
    "        self.bi_lstm = nn.LSTM(input_size=self.emb_dim,\n",
    "                              hidden_size=64,\n",
    "                              bidirectional=True)\n",
    "#         self.avg_pool = nn.AdaptiveAvgPool1d()\n",
    "#         self.max_pool = nn.AdaptiveMaxPool1d()\n",
    "        self.linear = nn.Linear(256, 3)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_masks, token_type_ids):\n",
    "        outputs = self.bert_model(input_ids, \n",
    "                                  attention_mask=attention_masks, \n",
    "                                  token_type_ids=token_type_ids)\n",
    "\n",
    "        bi_lstm, (hidden, cell)= self.bi_lstm(outputs.last_hidden_state)\n",
    "        concat = torch.cat([bi_lstm.mean(2), bi_lstm.max(2)[0]], 1)\n",
    "        dropout = self.dropout(concat)\n",
    "        output = self.linear(dropout)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "disciplinary-annual",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:30:43.003767Z",
     "start_time": "2021-03-14T16:30:40.252030Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertSemanticSimilarity().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-graduation",
   "metadata": {},
   "source": [
    "다중 분류 문제이기 때문에 cross entropy loss 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "female-logging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:15:02.927378Z",
     "start_time": "2021-03-14T16:15:02.923004Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    argmax_preds = torch.argmax(preds, dim=1)\n",
    "    correct = (argmax_preds == y).float() \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "intermediate-exclusive",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:30:05.711558Z",
     "start_time": "2021-03-14T16:30:05.702347Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, tain_data, optimizer, loss_fn, idx_epoch, batch_size):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train() \n",
    "\n",
    "    for idx, batch in enumerate(train_data):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = torch.from_numpy(batch[0][0])\n",
    "        attention_masks = torch.from_numpy(batch[0][1])\n",
    "        token_type_ids = torch.from_numpy(batch[0][2])\n",
    "        label = torch.from_numpy(batch[1])\n",
    "\n",
    "        predictions = model.forward(input_ids, \n",
    "                                    attention_masks, \n",
    "                                    token_type_ids).squeeze(1)\n",
    "\n",
    "        loss = loss_fn(predictions, label.long())\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        sys.stdout.write(\n",
    "                    \"\\r\" + f\"[Train] Epoch : {idx_epoch:^3}\"\\\n",
    "                    f\"[{(idx + 1) * batch_size} / {len(train_data) * batch_size} ({100. * (idx + 1) / len(train_data) :.4}%)]\"\\\n",
    "                    f\"  Loss: {loss.item():.4}\"\\\n",
    "                    f\"  Acc : {acc.item():.4}\"\\\n",
    "                    )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss/len(train_data) , epoch_acc/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "prescription-portuguese",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:25:17.756240Z",
     "start_time": "2021-03-14T16:25:17.747332Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, loss_fn):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            input_ids = torch.from_numpy(batch[0][0])\n",
    "            attenttion_masks = torch.from_numpy(batch[0][1])\n",
    "            token_type_ids = torch.from_numpy(batch[0][2])\n",
    "            label = torch.from_numpy(batch[1])\n",
    "            predictions = model.forward(input_ids, \n",
    "                                    attention_masks, \n",
    "                                    token_type_ids).squeeze(1)\n",
    "            loss = criterion(predictions, label.long())\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(train_data), epoch_acc / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    input_ids = torch.from_numpy(data[0][0])\n",
    "    attenttion_masks = torch.from_numpy(data[0][1])\n",
    "    token_type_ids = torch.from_numpy(data[0][2])\n",
    "    label = torch.from_numpy(data[1])\n",
    "    \n",
    "    predictions = model.forward(input_ids,\n",
    "                                attention_masks,\n",
    "                                token_type_ids).squeeze(1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-affair",
   "metadata": {},
   "source": [
    "## Training, Validation Data Generator 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "limiting-morning",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T16:30:44.861096Z",
     "start_time": "2021-03-14T16:30:43.320414Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = BertSemanticDataGenerator(\n",
    "    train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
    "    y_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-wrong",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-swimming",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-03-14T07:31:17.507Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Model name : <class '__main__.BertSemanticSimilarity'>\n",
      "----------------------------------------------------------------\n",
      "[Train] Epoch :  0 [39840 / 99872 (39.89%)]  Loss: 1.105  Acc : 0.34385"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{BertSemanticSimilarity}\"\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print('----------------------------------------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('----------------------------------------------------------------')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, loss_fn, epoch, batch_size)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_data, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-speaker",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-plane",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T04:51:35.081551Z",
     "start_time": "2021-03-14T04:51:35.076906Z"
    }
   },
   "outputs": [],
   "source": [
    "model.bert_model.trainable = True\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "model_name = f\"{BertSemanticSimilarity}\"\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "print('----------------------------------------------------------------')\n",
    "print(f'Model name : {model_name}')\n",
    "print('----------------------------------------------------------------')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_data, optimizer, loss_fn, epoch, batch_size)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_data, loss_fn)\n",
    "    print('')\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), f'./{model_name}.pt')\n",
    "        print(f'\\t Saved at {epoch}-epoch')\n",
    "\n",
    "    print(f'\\t Epoch : {epoch} | Train Loss : {train_loss:.4} | Train Acc : {train_acc:.4}')\n",
    "    print(f'\\t Epoch : {epoch} | Valid Loss : {valid_loss:.4} | Valid Acc : {valid_acc:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-opportunity",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = BertSemanticDataGenerator(\n",
    "    test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"),\n",
    "    y_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-minutes",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'./{model_name}.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_data, loss_fn)\n",
    "print(f'Test Loss : {test_loss:.4} | Test Acc : {test_acc:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence1, sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "    proba = model.predict(test_data)\n",
    "    idx = np.argmax(proba)\n",
    "    proba = f\"{proba[idx]: .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Two women are observing something together.\"\n",
    "sentence2 = \"Two women are standing with their eyes closed.\"\n",
    "check_similarity(sentence1, sentence2)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
