{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opened-gabriel",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using Transformers \n",
    "[source](https://keras.io/examples/nlp/ner_transformers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-highlight",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Named Entity Recognition (NER) is the process of identifying named entities in text. Example of named entities are: \"Person\", \"Location\", \"Organization\", \"Dates\" etc. NER is essentially a token classification task where every token is classified into one or more predetermined categories.\n",
    "\n",
    "In this exercise, we will train a simple Transformer based model to perform NER. We will be using the data from CoNLL 2003 shared task. For more information about the dataset, please visit the dataset website. However, since obtaining this data requires an additional step of getting a free license, we will be using HuggingFace's datasets library which contains a processed version of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-franchise",
   "metadata": {},
   "source": [
    "## Install the open source datasets library from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-assessment",
   "metadata": {},
   "source": [
    "* Huggingface `datasets` library [link](https://huggingface.co/docs/datasets/quicktour.html#loading-a-dataset)\n",
    "> Datasets provides datasets for many NLP tasks like text classification, question answering, language modeling\n",
    "> https://huggingface.co/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "competitive-subsection",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# datasets library download\n",
    "!pip install datasets\n",
    "\n",
    "# script used to evaluate NER models\n",
    "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "negative-jaguar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1136\n",
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, catalonia_independence, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, code_x_glue_cc_clone_detection_big_clone_bench, code_x_glue_cc_clone_detection_poj104, code_x_glue_cc_cloze_testing_all, code_x_glue_cc_cloze_testing_maxmin, code_x_glue_cc_code_completion_line, code_x_glue_cc_code_completion_token, code_x_glue_cc_code_refinement, code_x_glue_cc_code_to_code_trans, code_x_glue_cc_defect_detection, code_x_glue_ct_code_to_text, code_x_glue_tc_nl_code_search_adv, code_x_glue_tc_text_to_code, code_x_glue_tt_text_to_text, com_qa, common_gen, common_voice, commonsense_qa, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, disfl_qa, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, eduge, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hendrycks_test, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jnlpba, journalists_questions, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, klue, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, masakhaner, math_dataset, math_qa, matinf, mc4, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, russian_super_glue, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, superb, swag, swahili, swahili_news, swda, swedish_ner_corpus, swedish_reviews, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, tilde_model, time_dial, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, Abdo1Kamr/Arabic_Nine_Hadiths_Books, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Aslihan/mesinesp, Avishekavi/Avi, Binbin/my_dataset, CAGER/rick, Cropinky/flatearther, Cropinky/rap_lyrics_english, Cropinky/wow_fishing_bobber, Darren/data, EMBO/biolang, EMBO/sd-nlp, ESZER/H, Ebtihal/OSCAR_Arabic, Eymen3455/xsum_tr, FRTNX/cosuju, Felix-ML/quoteli3, Firoj/CrisisBench, Francois/futures_es, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Fraser/wiki_sentences, Gabriel/squad_v2_sv, GalacticAI/Noirset, Gwangho/NCBI-Sars-Cov-2, Halilyesilceng/autonlp-data-nameEntityRecognition, HarleyQ/WitcherDialogue, HarveyBWest/mybot, Husain/intent-classification-en-fr, Jean-Baptiste/wikiner_fr, KETI-AIR/klue, KETI-AIR/kor_corpora, KETI-AIR/korquad, KETI-AIR/nikl, LIAMF-USP/arc-retrieval-c4, MKK/Dhivehi-English, MarianaSahagun/test, Melinoe/TheLabTexts, NTUYG/RAGTest, Narsil/asr_dummy, NbAiLab/nb_nn, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, Ofrit/tmp, Pongsaky/Wiki_SCG, Pyke/patent_abstract, QA/abk-eng, Remesita/tagged_reviews, SCourthial/test, SajjadAyoubi/persian_qa, TRoboto/masc, Tatyana/ru_sentiment_dataset, Terry0107/RiSAWOZ, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, VoVanPhuc/translate_vi2en, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, Wikidepia/mc4-filter, WyrdCurt/AO4W, XiangXiang/clt, Yves/fhnw_swiss_parliament, aapot/mc4_fi_cleaned, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, ajmbell/test-dataset, albertvillanova/dummy_libri2mix, albertvillanova/tests-public-raw-jsonl, albertvillanova/tests-raw-jsonl, alireza655/alireza655, allenai/c4, anukaver/EstQA, artyeth/Dorian, ashish-shrivastava/dont-know-dataset, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bertin-project/mc4-es-sampled, bertin-project/mc4-sampling, bigscience/mc4-sampled, bigscience/open_subtitles_monolingual, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, caca/zscczs, cakiki/args_me, canwenxu/dogwhistle, ccccccc/hdjw_94ejrjr, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, chenyuxuan/wikigold, cheulyop/ksponspeech, clarin-pl/cst-wikinews, clarin-pl/kpwr-ner, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, cnrcastroli/aaaa, cointegrated/ParaNMT-Ru-Leipzig, congpt/dstc23_asr, corypaik/prost, csebuetnlp/xlsum, ctl/ConceptualCaptions, dasago78/dasago78dataset, dataset/wikipedia_bn, david-wb/zeshel, deepset/germandpr, deepset/germanquad, dfgvhxfgv/fghghj, dfki-nlp/few-nerd, dgknrsln/Yorumsepeti, diiogo/brwac-clean, dispenst/jhghdghfd, dispix/test-dataset, dk-crazydiv/huggingface-modelhub, dongpil/test, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ervis/aaa, ervis/qqq, eugenesiow/BSD100, eugenesiow/Div2k, eugenesiow/PIRM, eugenesiow/Set14, eugenesiow/Set5, eugenesiow/Urban100, fatvvs/autonlp-data-entity_model_conll2003, flax-community/code_clippy_data, flax-community/conceptual-12m-mbart-50-multilingual, flax-community/conceptual-12m-multilingual-marian-128, flax-community/conceptual-12m-multilingual-marian-es, flax-community/conceptual-12m-multilingual-marian, flax-community/conceptual-captions-12, flax-community/dummy-oscar-als-32, flax-community/german-common-voice-processed, flax-community/german_common_crawl, flax-community/multilingual-vqa, flax-community/norwegian-clean-dummy, flax-community/swahili-safi, flax-sentence-embeddings/Gender_Bias_Evaluation_Set, flax-sentence-embeddings/paws-jsonl, flax-sentence-embeddings/stackexchange_math_jsonl, flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_title_body_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_xml, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, fuliucansheng/minicoco, fuliucansheng/mininlp, fuliucansheng/pascal_voc, fvillena/cantemist, fvillena/spanish_diagnostics, gabtan99/pex-conversations, german-nlp-group/german_common_crawl, gmnlp/TICO19, godzillavskongonlinetv/ergfdg, godzillavskongonlinetv/godzillavskongfullmovie, gpt3mix/rt20, gpt3mix/sst2, gsarti/clean_mc4_it, gustavecortal/fr_covid_news, hartzeer/kdfjdshfje, hfface/poopi, howardmiddleton382/esuyertusutr, howardmiddleton382/wgweagwege, huggingFaceUser02/air21_grp13_inference_results, huggingFaceUser02/air21_grp13_tokenized_results, huggingartists/asdfgfa, huggingartists/billie-eilish, huggingartists/drake, huggingartists/eminem, huggingartists/fascinoma, huggingartists/kanye-west, huggingartists/lil-nas-x, huggingartists/morgenshtern, huggingartists/nirvana, huggingartists/pop-smoke, huggingartists/sugar-ray, huggingartists/suicideoscope, huggingartists/the-weeknd, huggingartists/tom-waits, huggingartists/xxxtentacion, huggingface/label-files, huseinzol05/translated-The-Pile, iamshsdf/sssssssssss, iarfmoose/question_generator, imthanhlv/binhvq_dedup, imthanhlv/binhvq_news21_raw, imthanhlv/viwiki, jaimin/wav2vec2-large-xlsr-gujarati-demo, jdepoix/junit_test_completion, jglaser/binding_affinity, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jinmang2/ko-dialect, jiyoojeong/targetizer, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, julien-c/dummy-dataset-from-colab, julien-c/reactiongif, k-halid/ar, kaka10/fgfgfgfg, karinev/lanuitdudroit, katoensp/VR-OP, keshan/clean-si-mc4, keshan/large-sinhala-asr-dataset, keshan/multispeaker-tts-sinhala, keshan/wit-dataset, kevinlu1248/personificationgen, kmyoo/klue-tc-dev, lavis-nlp/german_legal_sentences, lewtun/asr-preds-test, lewtun/asr_dummy, lewtun/binary_classification_dummy, lewtun/bulk-superb-s3p-superb-49606, lewtun/mnist-preds, lewtun/s3prl-sd-dummy, lewtun/superb-dummy-eval, lewtun/text_classification_dummy, lhoestq/custom_squad, lhoestq/demo1, lhoestq/squad, lhoestq/test, lhoestq/test2, lhoestq/wikipedia_bn, liam168/nlp_c4_sentiment, lkiouiou/o9ui7877687, lohanna/testedjkcxkf, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, lukasmasuch/my-test-repo-3, lukasmasuch/my-test-repo-4, lukasmasuch/test-2, lukasmasuch/test-3, lukasmasuch/test, m3hrdadfi/recipe_nlg_lite, mad/IndonesiaNewsDataset, majod/CleanNaturalQuestionsDataset, makanan/umich, martodaniel/terere, medzaf/test, metalearning/kaggale-nlp-tutorial, mksaad/Arabic_news, ml6team/cnn_dailymail_nl, mldmm/glass_alloy_composition, mmm-da/rutracker_anime_torrent_titles, mrojas/abbreviation, mrojas/body, mrojas/disease, mrojas/family, mrojas/finding, mrojas/medication, mrojas/procedure, mulcyber/europarl-mono, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, nateraw/auto-cats-and-dogs, nateraw/auto-exp-2, nateraw/beans, nateraw/cats-and-dogs, nateraw/fairface, nateraw/food101, nateraw/image-folder, nateraw/rock_paper_scissors, nateraw/test, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, nbroad/few-nerd, neelalex/raft-predictions, nielsr/FUNSD_layoutlmv2, nielsr/XFUN, nielsr/funsd, nlpufg/brwac-pt, nlpufg/brwac, nlpufg/oscar-pt, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/codeparrot-train, osanseviero/llama_test, osanseviero/test, ought/raft-submission-template, ought/raft, parivartanayurveda/Malesexproblemsayurvedictreatment, pasinit/xlwic, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, pelican/test_100, pere/nb_nn_balanced_shuffled, pere/norwegian_colossal_corpus_v2_short100k, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, pierreant-p/jcvd-or-linkedin, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, pulmo/chest_xray, qfortier/instagram_ny, ramybaly/conll2012, ramybaly/nerd, rays2pix/example, rays2pix/example_dataset, retiol/REGARDER, retiol/celyfilm, retiol/regarderr, rewardsignal/reddit_writing_prompts, rony/soccer-dialogues, roskoN/dailydialog, roskoN/dstc8-reddit-corpus, sagnikrayc/mctest, sagnikrayc/quasar, salesken/Paraphrase_category_detection, sdfufygvjh/fgghuviugviu, seamew/ChnSentiCorp, seamew/Hotel, seamew/THUCNews, seamew/Weibo, seamew/amazon_reviews_zh, seamew/weibo_avg, shahrukhx01/questions-vs-statements, sharejing/BiPaR, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/Genjer, somaimanguyat/Koboy, somaimanguyat/Movieonline2021, somaimanguyat/Salome, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/ms_marco_doc2query, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/openwebtext-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, superb/superb-data, susumu2357/squad_v2_sv, svalabs/all-nli-german-translation-wmt19, svalabs/ms-marco-german-translation-wmt19, tals/test, tanfiona/causenet_wiki, tarudesu/UIT-ViCTSD, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, thomwolf/codeparrot-train, thomwolf/codeparrot-valid, thomwolf/codeparrot, thomwolf/github-dataset, thomwolf/github-python, tianxing1994/temp, toloka/CrowdSpeech, toloka/VoxDIY-RusNews, tommy19970714/common_voice, toriving/kosimcse, toriving/talktalk-sentiment-210713-multi-singleturn-custom-multiturn, ttj/metadata_arxiv, turingbench/TuringBench, uasoyasser/rgfes, uva-irlab/canard_quretec, uva-irlab/trec-cast-2019-multi-turn, vasudevgupta/amazon-ml-hack, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/gsoc-librispeech, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vblagoje/wikipedia_snippets_streamed, vctc92/sdsd, vctc92/test, versae/adobo, vershasaxena91/datasets, vershasaxena91/squad_multitask, w-nicole/childes_data, w-nicole/childes_data_no_tags, w-nicole/childes_data_no_tags_, w-nicole/childes_data_with_tags, w-nicole/childes_data_with_tags_, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, webis/args_me, weijieliu/senteval_cn, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, yluisfern/PBU\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "print(len(datasets_list))\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-chain",
   "metadata": {},
   "source": [
    "## Defining a Transformer(Encoder)Block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afraid-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from datasets import load_dataset\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-january",
   "metadata": {},
   "source": [
    "**[Transformer Implementaion Source from]** \n",
    "- [blog post - tutorial](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec)\n",
    "- [github](https://github.com/SamLynnEvans/Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-delhi",
   "metadata": {},
   "source": [
    "**Attention is All you need**\n",
    "\n",
    "> **3.4 Embeddings and Softmax(p5)**\n",
    "\n",
    "> Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
    "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor\u0002mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
    "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
    "linear transformation, similar to [24]. **In the embedding layers, we multiply those weights by √\n",
    "dmodel.**\n",
    "\n",
    "* The reason we increase the embedding values before addition is **to make the positional encoding relatively smaller**. This means **the original meaning in the embedding vector won’t be lost** when we add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-person",
   "metadata": {},
   "source": [
    "1. Pytorch `self.register_buffer` 로 layer를 등록하면 어떤 특징이 있는가?\n",
    "\n",
    "    1) optimizer가 업데이트하지 않는다.\n",
    "    2) 그러나 값은 존재한다(하나의 layer로써 작용한다고 보면 된다.)\n",
    "    3) state_dict()로 확인이 가능하다.\n",
    "    4) GPU연산이 가능하다.\n",
    "\n",
    "2. `Variable`이란? ([source1](https://medium.com/@poperson1205/%EC%B4%88%EA%B0%84%EB%8B%A8-pytorch%EC%97%90%EC%84%9C-tensor%EC%99%80-variable%EC%9D%98-%EC%B0%A8%EC%9D%B4-a846dfb72119), [source2](https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/examples_autograd/two_layer_net_autograd.html))\n",
    "\n",
    "    * Variable이 최근 버전에서 deprecated 상태 Variable은 원래 autograd를 사용하기 위해서 사용되던 타입이었으나, 현재는 Tensor 타입과 병합되었다고 한다. 즉, Tensor 타입에서 디폴트로 autograd 기능을 지원하도록 되어있다.\n",
    "\n",
    "    * PyTorch Variable은 PyTorch Tensor의 래퍼(Wrapper)이며, 연산 그래프(Computational Graph)에서 노드(Node)로 표현(represent)된다. \n",
    "\n",
    "    * PyTorch 0.4 이상 버전에서는 더이상 Variable을 사용할 필요가 없다. Legacy 코드에 Variable이 있다면 그냥 Tensor라고 생각하고 읽으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "secret-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PE (pos,2i) = sin(pos/10000^(2i/d_model))\n",
    "    PE (pos,2i+1) = cos(pos/10000^(2i/d_model)) \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len = 128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        # add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # calculate attention\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(self.d_k)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = torch.matmul(scores, v)\n",
    "\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=64, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dimensional-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # self attention + add&norm\n",
    "        attn_output = self.attn(x, x, x, mask)\n",
    "        attn_output = self.dropout_1(attn_output)\n",
    "        norm_output = self.norm_1(x + attn_output)\n",
    "        \n",
    "        # ffn + add&norm\n",
    "        ffn_output = self.ff(norm_output)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        out = self.norm_2(norm_output + ffn_output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-ethernet",
   "metadata": {},
   "source": [
    "## Build the NER model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "buried-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(nn.Module):\n",
    "    def __init__(self, num_tags, vocab_size, max_seq_len=128, d_model=32, heads=4, d_ff=64):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.add_positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.transformer_block = TransformerEncoderLayer(d_model, heads)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.ff = nn.Linear(d_model, d_ff) #Relu\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.ff_final = nn.Linear(d_ff, num_tags) # Softmax\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.token_embedding(inputs) \n",
    "        x = self.add_positional_encoding(x)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.ff(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.ff_final(x) # torch.Size([batch_size, max_seq_len, num_tags])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-complexity",
   "metadata": {},
   "source": [
    "## Load the CoNLL 2003 dataset from the datasets library and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-humidity",
   "metadata": {},
   "source": [
    "* length / tokens / tags\n",
    "* 8\tChina\tsays\ttime\tright\tfor\tTaiwan\ttalks\t.\t5\t0\t0\t0\t0\t5\t0\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wrapped-practitioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/home/subinkim/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rapid-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            f.write(\n",
    "                str(len(tokens))\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(tokens)\n",
    "                + \"\\t\"\n",
    "                + \"\\t\".join(map(str, ner_tags))\n",
    "                + \"\\n\"\n",
    "            )\n",
    "\n",
    "export_to_file(\"./datasets/conll2003/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./datasets/conll2003/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-produce",
   "metadata": {},
   "source": [
    "## Make the NER label lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-gregory",
   "metadata": {},
   "source": [
    " **Data Examlpe** \n",
    " ```\n",
    " EU NNP B-NP B-ORG\n",
    "rejects VBZ B-VP O\n",
    "German JJ B-NP B-MISC\n",
    "call NN I-NP O\n",
    "to TO B-VP O\n",
    "boycott VB I-VP O\n",
    "British JJ B-NP B-MISC\n",
    "lamb NN I-NP O\n",
    ". . O O\n",
    "\n",
    "Peter NNP B-NP B-PER\n",
    "Blackburn NNP I-NP I-PER\n",
    "```\n",
    "[내용 출처](https://wikidocs.net/24682)\n",
    "\n",
    "* 데이터의 형식은 [단어] [품사 태깅] [청크 태깅] [개체명 태깅]의 형식으로 되어 있음\n",
    "\n",
    "**[품사 태깅]**\n",
    "\n",
    "- NNP : 고유 명사 단수형\n",
    "- VBZ는 3인칭 단수 동사 현재형\n",
    "- 추가 상세 정보 [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "**[개체명 태깅]**\n",
    "\n",
    "* LOC : location\n",
    "* ORG : organization\n",
    "* PER : person\n",
    "* MISC : miscellaneous(여러 가지 종류의, 이것저것 다양한)\n",
    "\n",
    "\n",
    "   ***BIO 표현 방법을 사용***\n",
    "   \n",
    "\n",
    "* B : Begin, 개체명이 시작되는 부분\n",
    "* I : Inside, 개체명의 내부 부분\n",
    "* O : Outside, 개체명이 앙닌 부분\n",
    "\n",
    "        \n",
    "   ***개체명 인식 상세 설명***    \n",
    "   1) 개체명의 시작 부분이면서 Organization을 의미하는 German에는 `B-ORG`라는 개체명 태깅이 붙음. 다만, German 그 자체로 개체명 하나이기 때문에 거기서 개체명 인식은 종료되면서 뒤에 `I`가 별도로 붙는 단어가 나오지는 않음. 이에 German 뒤에 나오는 call은 개체명이 아니기 때문에 `O`가 태깅됨.\n",
    "    \n",
    "   2) `. . O O` 다음에 11번째 줄 Peter가 나오는 부분 사이에서 10번째 줄은 공란으로 되어 있는데, 이는 9번째 줄에서 문장이 끝나고 11번째 줄에서 새로운 문장이 시작됨을 의미\n",
    "   3) 그 다음 문장이 시작되는 11번째 줄에서는 개체명이 하나의 단어로 끝나지 않았을 때, 어떻게 다음 단어로 개체명 인식이 이어지는지를 보여줌. Peter는 개체명이 시작되면서 person에 해당되기 때문에 `B-PER`이라는 개체명 태깅이 붙고, 아직 개체명에 대한 인식은 끝나지 않았기 때문에 뒤에 붙는 Blackburn에서는 `I`가 나오면서 `I-PER`이 개체명 태깅으로 붙게 됨. 즉, Peter Blackburn이 person에 속하는 하나의 개체명으로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "patient-battery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    # target의 index 0은 padding을 의미하도록 설정\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suited-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens : ['china', 'says', 'time', 'right', 'for', 'taiwan', 'talks', '.']\n",
    "# tags : [6, 1, 1, 1, 1, 6, 1, 1]\n",
    "\n",
    "def map_record_to_training_data(record):\n",
    "    record = record.lower()\n",
    "    record = record.split('\\t')\n",
    "    length = int(record[0])\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = [int(tag) + 1 for tag in tags]\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nonprofit-imagination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['china', 'says', 'time', 'right', 'for', 'taiwan', 'talks .', '50'],\n",
       " [1, 1, 1, 6, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_record_to_training_data('8\\tChina\\tsays\\ttime\\tright\\tfor\\tTaiwan\\ttalks .\\t50\\t0\\t0\\t0\\t5\\t0\\t0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "micro-cartoon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       " [4, 1, 8, 1, 1, 1, 8, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_record_to_training_data('9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-eleven",
   "metadata": {},
   "source": [
    "* Custom Vocab\n",
    "\n",
    "> 데이터셋 자체가 tokenized 된 상태이므로 별도의 tokenizer 활용하지 말고, 직접 vocab을 구축하여 token -> id 변환을 해주어야한다.\n",
    "\n",
    "* Counter 함수\n",
    "> \n",
    "```\n",
    "from collections import Counter\n",
    "Counter('hello world') # Counter({'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1})\n",
    "Counter('hello world').most_common() # [('l', 3), ('o', 2), ('h', 1), ('e', 1), (' ', 1), ('w', 1), ('r', 1), ('d', 1)]\n",
    "```\n",
    "* vocab size 20000개\n",
    "> 20000개 중 2개는 [UNK], [PAD]로 사용하고자 함\n",
    "> training set 기반으로 vocab구성했으므로, 실제 training 시에는 [UNK]가 거의 없겠지만, test/inference시에 사용될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "accredited-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "# token and another one denoting a masking token\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "guided-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "focal-humanitarian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '.',\n",
       " ',',\n",
       " 'of',\n",
       " 'in',\n",
       " 'to',\n",
       " 'a',\n",
       " 'and',\n",
       " '(',\n",
       " ')',\n",
       " '\"',\n",
       " 'on',\n",
       " 'said',\n",
       " \"'s\",\n",
       " 'for',\n",
       " '1',\n",
       " '-',\n",
       " 'at',\n",
       " 'was',\n",
       " '2',\n",
       " '0',\n",
       " '3',\n",
       " 'with',\n",
       " 'that',\n",
       " 'he',\n",
       " 'from',\n",
       " 'it',\n",
       " 'by',\n",
       " 'is',\n",
       " ':',\n",
       " 'as',\n",
       " '4',\n",
       " 'had',\n",
       " 'his',\n",
       " 'has',\n",
       " 'but',\n",
       " 'an',\n",
       " 'not',\n",
       " 'were',\n",
       " 'be',\n",
       " 'after',\n",
       " 'have',\n",
       " 'first',\n",
       " 'new',\n",
       " 'who',\n",
       " 'will',\n",
       " 'they',\n",
       " '5',\n",
       " 'two',\n",
       " 'u.s.',\n",
       " 'been',\n",
       " '$',\n",
       " '--',\n",
       " 'their',\n",
       " 'beat',\n",
       " 'are',\n",
       " '6',\n",
       " 'which',\n",
       " 'would',\n",
       " 'this',\n",
       " 'up',\n",
       " 'its',\n",
       " 'year',\n",
       " 'i',\n",
       " 'last',\n",
       " 'percent',\n",
       " 'out',\n",
       " 'we',\n",
       " 'thursday',\n",
       " 'one',\n",
       " 'million',\n",
       " 'over',\n",
       " 'government',\n",
       " 'wednesday',\n",
       " 'police',\n",
       " '7',\n",
       " 'results',\n",
       " 'against',\n",
       " 'second',\n",
       " 'when',\n",
       " '/',\n",
       " 'also',\n",
       " 'tuesday',\n",
       " 'three',\n",
       " 'soccer',\n",
       " 'president',\n",
       " 'no',\n",
       " 'division',\n",
       " 'told',\n",
       " '10',\n",
       " 'monday',\n",
       " 'people',\n",
       " 'about',\n",
       " 'or',\n",
       " 'friday',\n",
       " 'league',\n",
       " 'some',\n",
       " 'london',\n",
       " 'there',\n",
       " 'world',\n",
       " 'her',\n",
       " 'minister',\n",
       " 'under',\n",
       " 'more',\n",
       " 'york',\n",
       " '9',\n",
       " '1996-08-28',\n",
       " 'won',\n",
       " 'into',\n",
       " 'state',\n",
       " 'sunday',\n",
       " '8',\n",
       " 'before',\n",
       " 'south',\n",
       " 'played',\n",
       " 'group',\n",
       " 'market',\n",
       " 'week',\n",
       " 'england',\n",
       " 'all',\n",
       " \"'\",\n",
       " 'time',\n",
       " 'germany',\n",
       " 'than',\n",
       " 'could',\n",
       " 'other',\n",
       " 'australia',\n",
       " 'she',\n",
       " 'between',\n",
       " 'since',\n",
       " 'points',\n",
       " 'match',\n",
       " 'company',\n",
       " 'bank',\n",
       " 'round',\n",
       " 'britain',\n",
       " 'officials',\n",
       " 'years',\n",
       " 'games',\n",
       " 'if',\n",
       " 'newsroom',\n",
       " 'only',\n",
       " 'saturday',\n",
       " 'national',\n",
       " 'france',\n",
       " 'party',\n",
       " 'six',\n",
       " 'former',\n",
       " '1996-08-22',\n",
       " 'four',\n",
       " 'third',\n",
       " 'home',\n",
       " '1996-08-29',\n",
       " 'city',\n",
       " '1996',\n",
       " 'off',\n",
       " 'cup',\n",
       " 'five',\n",
       " 'win',\n",
       " 'foreign',\n",
       " '11',\n",
       " '1996-08-27',\n",
       " 'open',\n",
       " 'day',\n",
       " 'down',\n",
       " 'august',\n",
       " '13',\n",
       " 'united',\n",
       " 'because',\n",
       " '6-4',\n",
       " '6-3',\n",
       " 'official',\n",
       " 'did',\n",
       " '21',\n",
       " 'just',\n",
       " 'next',\n",
       " '15',\n",
       " 'him',\n",
       " 'spain',\n",
       " 'standings',\n",
       " '1996-08-26',\n",
       " 'expected',\n",
       " 'shares',\n",
       " 'do',\n",
       " 'statement',\n",
       " 'spokesman',\n",
       " 'news',\n",
       " 'pakistan',\n",
       " 'through',\n",
       " 'may',\n",
       " 'women',\n",
       " 'made',\n",
       " '70',\n",
       " 'v',\n",
       " 'while',\n",
       " 'men',\n",
       " '12',\n",
       " 'chicago',\n",
       " '1996-08-23',\n",
       " 'international',\n",
       " 'them',\n",
       " '14',\n",
       " 'lost',\n",
       " 'german',\n",
       " 'july',\n",
       " 'where',\n",
       " 'russian',\n",
       " 'back',\n",
       " '20',\n",
       " '6-2',\n",
       " 'september',\n",
       " \"n't\",\n",
       " 'china',\n",
       " 'italy',\n",
       " 'british',\n",
       " '2.',\n",
       " '3.',\n",
       " 'european',\n",
       " 'any',\n",
       " 'peace',\n",
       " 'team',\n",
       " '1.',\n",
       " 'clinton',\n",
       " 'pct',\n",
       " 'matches',\n",
       " 'japan',\n",
       " 'seconds',\n",
       " 'prime',\n",
       " 'reported',\n",
       " 'billion',\n",
       " '69',\n",
       " 'month',\n",
       " 'earlier',\n",
       " 'june',\n",
       " 'central',\n",
       " 'now',\n",
       " 'russia',\n",
       " 'meeting',\n",
       " 'still',\n",
       " 'metres',\n",
       " 'final',\n",
       " '30',\n",
       " 'being',\n",
       " 'talks',\n",
       " 'west',\n",
       " 'added',\n",
       " '71',\n",
       " '1996-08-25',\n",
       " 'half',\n",
       " '1/2',\n",
       " 'during',\n",
       " 'french',\n",
       " 'season',\n",
       " 'b',\n",
       " 'you',\n",
       " 'set',\n",
       " 'capital',\n",
       " 'leading',\n",
       " 'san',\n",
       " 'take',\n",
       " 'tonnes',\n",
       " 'sweden',\n",
       " 'killed',\n",
       " 'st',\n",
       " 'india',\n",
       " 'lead',\n",
       " 'held',\n",
       " 'elections',\n",
       " 'net',\n",
       " 'around',\n",
       " 'end',\n",
       " 'war',\n",
       " 'tennis',\n",
       " 'security',\n",
       " 'should',\n",
       " 'reuters',\n",
       " 'our',\n",
       " 'most',\n",
       " 'part',\n",
       " 'mark',\n",
       " 'game',\n",
       " 'due',\n",
       " 'early',\n",
       " 'took',\n",
       " 'states',\n",
       " 'days',\n",
       " 'so',\n",
       " 'innings',\n",
       " 'ministry',\n",
       " 'saying',\n",
       " 'number',\n",
       " 'moscow',\n",
       " '68',\n",
       " 'cricket',\n",
       " '6-1',\n",
       " 'american',\n",
       " 'major',\n",
       " 'minutes',\n",
       " 'union',\n",
       " 'political',\n",
       " 'seven',\n",
       " 'per',\n",
       " '7-6',\n",
       " 'netherlands',\n",
       " '100',\n",
       " 'republic',\n",
       " 'hong',\n",
       " 'can',\n",
       " 'north',\n",
       " 'victory',\n",
       " 'championship',\n",
       " 'what',\n",
       " 'both',\n",
       " '22',\n",
       " 'well',\n",
       " 'belgium',\n",
       " 'total',\n",
       " 'country',\n",
       " 'iraq',\n",
       " 'play',\n",
       " 'court',\n",
       " 'kong',\n",
       " '50',\n",
       " 'close',\n",
       " 'africa',\n",
       " 'czech',\n",
       " 'visit',\n",
       " 'dutch',\n",
       " '25',\n",
       " 'result',\n",
       " 'champion',\n",
       " 'profit',\n",
       " 'commission',\n",
       " 'general',\n",
       " 'israel',\n",
       " 'left',\n",
       " 'eight',\n",
       " 'local',\n",
       " 'minute',\n",
       " 'says',\n",
       " 'price',\n",
       " 'trade',\n",
       " 'town',\n",
       " 'paris',\n",
       " '66',\n",
       " 'run',\n",
       " '1996-08-24',\n",
       " 'sales',\n",
       " '4.',\n",
       " 'very',\n",
       " 'press',\n",
       " '67',\n",
       " 'vs.',\n",
       " '5.',\n",
       " '6.',\n",
       " 'support',\n",
       " 'put',\n",
       " '1995',\n",
       " '24',\n",
       " 'leader',\n",
       " 'prices',\n",
       " '7-5',\n",
       " 'then',\n",
       " 'found',\n",
       " 'record',\n",
       " 'go',\n",
       " 'same',\n",
       " 'man',\n",
       " 'western',\n",
       " 'de',\n",
       " 'english',\n",
       " 'washington',\n",
       " 'called',\n",
       " 'northern',\n",
       " 'inc',\n",
       " 'say',\n",
       " 'issue',\n",
       " 'agency',\n",
       " 'test',\n",
       " 'opposition',\n",
       " 'meet',\n",
       " 'months',\n",
       " 'military',\n",
       " 'runs',\n",
       " 'rate',\n",
       " 'top',\n",
       " 'these',\n",
       " 'lower',\n",
       " 'ago',\n",
       " 'singles',\n",
       " '64',\n",
       " 'make',\n",
       " 'amsterdam',\n",
       " '72',\n",
       " 'race',\n",
       " 'newspaper',\n",
       " 'deal',\n",
       " 'goals',\n",
       " '16',\n",
       " 'office',\n",
       " 'democratic',\n",
       " 'ended',\n",
       " '60',\n",
       " 'cents',\n",
       " 'until',\n",
       " 'gave',\n",
       " 'following',\n",
       " 'leaders',\n",
       " 'behind',\n",
       " 'series',\n",
       " 'another',\n",
       " 'tour',\n",
       " 'sri',\n",
       " 'several',\n",
       " 'chief',\n",
       " 'late',\n",
       " 'australian',\n",
       " 'good',\n",
       " 'african',\n",
       " 'rebels',\n",
       " 'near',\n",
       " 'dollar',\n",
       " 'players',\n",
       " '75',\n",
       " 'michael',\n",
       " 'my',\n",
       " 'report',\n",
       " 'iraqi',\n",
       " 'weekend',\n",
       " 'economic',\n",
       " 'start',\n",
       " 'halftime',\n",
       " 'tournament',\n",
       " 'c',\n",
       " 'miles',\n",
       " 'refugees',\n",
       " 'agreed',\n",
       " 'southern',\n",
       " 'many',\n",
       " '74',\n",
       " 'get',\n",
       " 'power',\n",
       " '7.',\n",
       " 'plan',\n",
       " 'going',\n",
       " 'service',\n",
       " 'east',\n",
       " 'those',\n",
       " 'election',\n",
       " '73',\n",
       " 'attendance',\n",
       " ';',\n",
       " 'sydney',\n",
       " 'drawn',\n",
       " 'place',\n",
       " 'date',\n",
       " 'want',\n",
       " 'business',\n",
       " 'paul',\n",
       " 'white',\n",
       " 'stock',\n",
       " 'john',\n",
       " 'television',\n",
       " 'demand',\n",
       " 'israeli',\n",
       " 'david',\n",
       " 'gmt',\n",
       " 'taking',\n",
       " 'baseball',\n",
       " 'atlanta',\n",
       " 'grand',\n",
       " 'including',\n",
       " '8.',\n",
       " 'index',\n",
       " 'arrested',\n",
       " 'authorities',\n",
       " 'quoted',\n",
       " 'later',\n",
       " 'palestinian',\n",
       " 'parliament',\n",
       " 'corp',\n",
       " 'ahmed',\n",
       " 'allowed',\n",
       " 'forces',\n",
       " 'asked',\n",
       " 'cash',\n",
       " 'hit',\n",
       " 'km',\n",
       " 'california',\n",
       " 'zealand',\n",
       " 'brazil',\n",
       " 'house',\n",
       " 'already',\n",
       " 'army',\n",
       " 'arafat',\n",
       " 'rights',\n",
       " 'martin',\n",
       " 'club',\n",
       " '17',\n",
       " '28',\n",
       " 'yeltsin',\n",
       " 'fell',\n",
       " 'toronto',\n",
       " 'current',\n",
       " 'return',\n",
       " 'work',\n",
       " 'higher',\n",
       " 'ahead',\n",
       " 'loss',\n",
       " '31',\n",
       " 'weeks',\n",
       " 'm.',\n",
       " '1-0',\n",
       " '26',\n",
       " 'exchange',\n",
       " 'trading',\n",
       " 'dole',\n",
       " 'whether',\n",
       " 'hours',\n",
       " 'rose',\n",
       " 'quarter',\n",
       " 'came',\n",
       " 'reporters',\n",
       " 'announced',\n",
       " 'jerusalem',\n",
       " 'troops',\n",
       " 'share',\n",
       " 'closed',\n",
       " 'finland',\n",
       " 'way',\n",
       " 'squad',\n",
       " 'draw',\n",
       " 'vs',\n",
       " 'hospital',\n",
       " 'agreement',\n",
       " 'money',\n",
       " '19',\n",
       " 'countries',\n",
       " '...',\n",
       " 'like',\n",
       " '23',\n",
       " 'night',\n",
       " 'oil',\n",
       " 'head',\n",
       " 'austria',\n",
       " '1994',\n",
       " 'best',\n",
       " 'baltimore',\n",
       " '18',\n",
       " '65',\n",
       " 'conference',\n",
       " 'financial',\n",
       " 'began',\n",
       " 'scores',\n",
       " 'away',\n",
       " 'olympic',\n",
       " 'went',\n",
       " 'lanka',\n",
       " 'morning',\n",
       " 'decision',\n",
       " 'high',\n",
       " 'area',\n",
       " 'old',\n",
       " 'plans',\n",
       " 'bonds',\n",
       " 'such',\n",
       " 'aug',\n",
       " 'previous',\n",
       " 'main',\n",
       " 'few',\n",
       " '1997',\n",
       " 'past',\n",
       " 'francisco',\n",
       " 'los',\n",
       " 'hits',\n",
       " 'nations',\n",
       " 'budget',\n",
       " 'lebed',\n",
       " 'taken',\n",
       " 'human',\n",
       " 'public',\n",
       " 'us',\n",
       " 'fighting',\n",
       " 'fourth',\n",
       " 'little',\n",
       " 'champions',\n",
       " 'eastern',\n",
       " 'angeles',\n",
       " 'nine',\n",
       " 'latest',\n",
       " 'gold',\n",
       " 'italian',\n",
       " 'van',\n",
       " 'ban',\n",
       " 'long',\n",
       " 'march',\n",
       " 'growth',\n",
       " 'campaign',\n",
       " 'side',\n",
       " 'u.n.',\n",
       " 'strike',\n",
       " '63',\n",
       " 'fall',\n",
       " 'scored',\n",
       " 'signed',\n",
       " 'canada',\n",
       " 'seattle',\n",
       " 'colorado',\n",
       " 'co',\n",
       " 'much',\n",
       " 'bill',\n",
       " 'europe',\n",
       " 'ukraine',\n",
       " 'daily',\n",
       " 'manager',\n",
       " 'law',\n",
       " 'working',\n",
       " 'control',\n",
       " 'region',\n",
       " 'markets',\n",
       " 'future',\n",
       " 'winning',\n",
       " 'boston',\n",
       " '40',\n",
       " 'times',\n",
       " 'recent',\n",
       " 'think',\n",
       " 'call',\n",
       " 'health',\n",
       " 'accused',\n",
       " 'right',\n",
       " 'hold',\n",
       " 'pay',\n",
       " 'without',\n",
       " 'attack',\n",
       " 'own',\n",
       " 'plane',\n",
       " 'available',\n",
       " '96',\n",
       " 'akram',\n",
       " 'wickets',\n",
       " '62',\n",
       " 'order',\n",
       " 'airport',\n",
       " 'workers',\n",
       " 'again',\n",
       " 'disease',\n",
       " 'led',\n",
       " 'died',\n",
       " '27',\n",
       " 'period',\n",
       " 'give',\n",
       " 'free',\n",
       " 'average',\n",
       " 'interest',\n",
       " 'industry',\n",
       " 'members',\n",
       " 'ireland',\n",
       " 'texas',\n",
       " 'ajax',\n",
       " '9.',\n",
       " 'production',\n",
       " 'forecast',\n",
       " 'seen',\n",
       " 'vote',\n",
       " '---',\n",
       " 'might',\n",
       " 'sent',\n",
       " 'despite',\n",
       " 'embassy',\n",
       " 'ceasefire',\n",
       " 'failed',\n",
       " 'help',\n",
       " 'does',\n",
       " 'centre',\n",
       " 'released',\n",
       " 'louis',\n",
       " 'overs',\n",
       " 'case',\n",
       " \"'re\",\n",
       " 'dutroux',\n",
       " 'further',\n",
       " 'across',\n",
       " 'florida',\n",
       " 'island',\n",
       " 'short',\n",
       " 'me',\n",
       " 'october',\n",
       " 'started',\n",
       " 'county',\n",
       " 'wasim',\n",
       " 'captain',\n",
       " 'costa',\n",
       " 'cut',\n",
       " 'belgian',\n",
       " '29',\n",
       " 'children',\n",
       " 'death',\n",
       " 'least',\n",
       " 'council',\n",
       " 'rugby',\n",
       " 'black',\n",
       " 'mother',\n",
       " 'kurdish',\n",
       " 'jordan',\n",
       " 'planned',\n",
       " 'stories',\n",
       " 'analysts',\n",
       " 'strong',\n",
       " 'april',\n",
       " 'given',\n",
       " 'romania',\n",
       " 'premier',\n",
       " '59',\n",
       " 'thomas',\n",
       " 'coach',\n",
       " 'minnesota',\n",
       " 'kansas',\n",
       " 'diego',\n",
       " 'declared',\n",
       " 'playing',\n",
       " 'chechnya',\n",
       " 'convention',\n",
       " 'wheat',\n",
       " 'brussels',\n",
       " 'yr',\n",
       " 'rates',\n",
       " 'according',\n",
       " 'slovakia',\n",
       " 'title',\n",
       " '54',\n",
       " 'manchester',\n",
       " 'houston',\n",
       " 'shot',\n",
       " 'fifth',\n",
       " '10.',\n",
       " 'yet',\n",
       " 'civil',\n",
       " 'confirmed',\n",
       " 'bosnia',\n",
       " 'bosnian',\n",
       " 'republican',\n",
       " 'reports',\n",
       " 'contract',\n",
       " 'chairman',\n",
       " 'nearly',\n",
       " 'used',\n",
       " 'figures',\n",
       " 'secretary',\n",
       " 'force',\n",
       " 'term',\n",
       " 'must',\n",
       " 'trying',\n",
       " 'today',\n",
       " 'poland',\n",
       " 'possible',\n",
       " 'dealers',\n",
       " 'forced',\n",
       " 'golf',\n",
       " 'a.',\n",
       " 'tabulate',\n",
       " 'detroit',\n",
       " 'oakland',\n",
       " 'each',\n",
       " 'companies',\n",
       " 'seed',\n",
       " 'point',\n",
       " 'come',\n",
       " 'passengers',\n",
       " 'moslem',\n",
       " 'treaty',\n",
       " 'ltd',\n",
       " 'peter',\n",
       " 'action',\n",
       " 'known',\n",
       " 'brought',\n",
       " 'taiwan',\n",
       " 'opening',\n",
       " 'federal',\n",
       " 'sale',\n",
       " 'face',\n",
       " 'received',\n",
       " 'estimated',\n",
       " 'guerrillas',\n",
       " 'exports',\n",
       " 'aggregate',\n",
       " 'named',\n",
       " '2-0',\n",
       " 'robert',\n",
       " 'injured',\n",
       " 'road',\n",
       " 'woman',\n",
       " 'clear',\n",
       " 'small',\n",
       " 'charges',\n",
       " 'mexico',\n",
       " 'traders',\n",
       " 'kenya',\n",
       " 'nigeria',\n",
       " 'committee',\n",
       " 'senior',\n",
       " 'how',\n",
       " 'radio',\n",
       " 'details',\n",
       " 'association',\n",
       " 'straight',\n",
       " 'full',\n",
       " 'turkey',\n",
       " 'red',\n",
       " 'likely',\n",
       " 'earnings',\n",
       " 'immediately',\n",
       " 'services',\n",
       " 'use',\n",
       " 'l',\n",
       " 'wimbledon',\n",
       " 'argentina',\n",
       " 'cleveland',\n",
       " '1,000',\n",
       " '*',\n",
       " 'note',\n",
       " 'soon',\n",
       " 'post',\n",
       " 'investors',\n",
       " 'showed',\n",
       " 'private',\n",
       " 'department',\n",
       " 'yen',\n",
       " 'japanese',\n",
       " 'train',\n",
       " '48',\n",
       " 'bond',\n",
       " 'egypt',\n",
       " 'iran',\n",
       " 'level',\n",
       " 'air',\n",
       " 'spot',\n",
       " 'armed',\n",
       " '76',\n",
       " 'rally',\n",
       " 'unless',\n",
       " 'prix',\n",
       " 'better',\n",
       " 'croft',\n",
       " '0-0',\n",
       " 'cincinnati',\n",
       " '61',\n",
       " 'montreal',\n",
       " 'philadelphia',\n",
       " 'got',\n",
       " 'medical',\n",
       " 'groups',\n",
       " 'although',\n",
       " 'trip',\n",
       " 'stage',\n",
       " 'process',\n",
       " 'policy',\n",
       " 'see',\n",
       " 'conditions',\n",
       " 'believed',\n",
       " 'bonn',\n",
       " 'sold',\n",
       " 'tax',\n",
       " 'port',\n",
       " 'coming',\n",
       " 'board',\n",
       " 'seeding',\n",
       " 'holiday',\n",
       " 'waqar',\n",
       " 'younis',\n",
       " 'mushtaq',\n",
       " 'qualifier',\n",
       " 'johnson',\n",
       " 'milwaukee',\n",
       " 'young',\n",
       " 'psv',\n",
       " 'defence',\n",
       " 'swiss',\n",
       " 'violence',\n",
       " 'even',\n",
       " 'school',\n",
       " 'university',\n",
       " 'prison',\n",
       " 'information',\n",
       " 'rise',\n",
       " 'independence',\n",
       " 'pound',\n",
       " 'serb',\n",
       " '6-0',\n",
       " 'sell',\n",
       " 'here',\n",
       " 'message',\n",
       " 'baghdad',\n",
       " 'border',\n",
       " 'scheduled',\n",
       " 'data',\n",
       " 'keep',\n",
       " 'winner',\n",
       " 's.',\n",
       " 'rule',\n",
       " 'within',\n",
       " 'homer',\n",
       " 'jose',\n",
       " 'making',\n",
       " 'system',\n",
       " 'levels',\n",
       " '56',\n",
       " 'municipal',\n",
       " 'deputy',\n",
       " 'comment',\n",
       " 'nuclear',\n",
       " 'change',\n",
       " 'buy',\n",
       " 'polish',\n",
       " 'based',\n",
       " 'met',\n",
       " 'rebel',\n",
       " 'continue',\n",
       " 'land',\n",
       " 'saudi',\n",
       " 'flight',\n",
       " 'street',\n",
       " '53',\n",
       " 'know',\n",
       " 'course',\n",
       " 'scorers',\n",
       " 'director',\n",
       " 'khan',\n",
       " 'mullally',\n",
       " 'great',\n",
       " 'canadian',\n",
       " 'switzerland',\n",
       " 'pittsburgh',\n",
       " 'inning',\n",
       " 'tried',\n",
       " '55',\n",
       " 'letter',\n",
       " 'grozny',\n",
       " 'completed',\n",
       " 'charged',\n",
       " 'wife',\n",
       " 'volume',\n",
       " 'illegal',\n",
       " 'position',\n",
       " 'indian',\n",
       " 'economy',\n",
       " 'reached',\n",
       " 'struck',\n",
       " 'bid',\n",
       " 'king',\n",
       " 'chinese',\n",
       " 'goal',\n",
       " 'talk',\n",
       " 'netanyahu',\n",
       " 'source',\n",
       " 'kept',\n",
       " 'movement',\n",
       " 'areas',\n",
       " 'outside',\n",
       " 'puk',\n",
       " 'parties',\n",
       " 'attacks',\n",
       " 'problem',\n",
       " 'turkish',\n",
       " 'step',\n",
       " '45',\n",
       " '2-1',\n",
       " '1-1',\n",
       " 'prefix',\n",
       " 'opened',\n",
       " 'tie',\n",
       " 'percentage',\n",
       " '58',\n",
       " 'ground',\n",
       " 'stocks',\n",
       " 'able',\n",
       " 'deficit',\n",
       " 'arrived',\n",
       " \"'m\",\n",
       " 'poor',\n",
       " 'village',\n",
       " 'coast',\n",
       " 'osce',\n",
       " 'leave',\n",
       " 'having',\n",
       " 'drug',\n",
       " 'lot',\n",
       " 'eu',\n",
       " 'needed',\n",
       " 'similar',\n",
       " 'denied',\n",
       " 'paper',\n",
       " 'beijing',\n",
       " 'province',\n",
       " 'executive',\n",
       " 'compared',\n",
       " '300',\n",
       " 'schedule',\n",
       " 'firm',\n",
       " 'digest',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bizarre-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {}\n",
    "token_to_id['[UNK]'] = 0\n",
    "token_to_id['[PAD]'] = 1\n",
    "for i, token in enumerate(vocabulary):\n",
    "    token_to_id[token] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "viral-hacker",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[UNK]': 0,\n",
       " '[PAD]': 1,\n",
       " 'the': 2,\n",
       " '.': 3,\n",
       " ',': 4,\n",
       " 'of': 5,\n",
       " 'in': 6,\n",
       " 'to': 7,\n",
       " 'a': 8,\n",
       " 'and': 9,\n",
       " '(': 10,\n",
       " ')': 11,\n",
       " '\"': 12,\n",
       " 'on': 13,\n",
       " 'said': 14,\n",
       " \"'s\": 15,\n",
       " 'for': 16,\n",
       " '1': 17,\n",
       " '-': 18,\n",
       " 'at': 19,\n",
       " 'was': 20,\n",
       " '2': 21,\n",
       " '0': 22,\n",
       " '3': 23,\n",
       " 'with': 24,\n",
       " 'that': 25,\n",
       " 'he': 26,\n",
       " 'from': 27,\n",
       " 'it': 28,\n",
       " 'by': 29,\n",
       " 'is': 30,\n",
       " ':': 31,\n",
       " 'as': 32,\n",
       " '4': 33,\n",
       " 'had': 34,\n",
       " 'his': 35,\n",
       " 'has': 36,\n",
       " 'but': 37,\n",
       " 'an': 38,\n",
       " 'not': 39,\n",
       " 'were': 40,\n",
       " 'be': 41,\n",
       " 'after': 42,\n",
       " 'have': 43,\n",
       " 'first': 44,\n",
       " 'new': 45,\n",
       " 'who': 46,\n",
       " 'will': 47,\n",
       " 'they': 48,\n",
       " '5': 49,\n",
       " 'two': 50,\n",
       " 'u.s.': 51,\n",
       " 'been': 52,\n",
       " '$': 53,\n",
       " '--': 54,\n",
       " 'their': 55,\n",
       " 'beat': 56,\n",
       " 'are': 57,\n",
       " '6': 58,\n",
       " 'which': 59,\n",
       " 'would': 60,\n",
       " 'this': 61,\n",
       " 'up': 62,\n",
       " 'its': 63,\n",
       " 'year': 64,\n",
       " 'i': 65,\n",
       " 'last': 66,\n",
       " 'percent': 67,\n",
       " 'out': 68,\n",
       " 'we': 69,\n",
       " 'thursday': 70,\n",
       " 'one': 71,\n",
       " 'million': 72,\n",
       " 'over': 73,\n",
       " 'government': 74,\n",
       " 'wednesday': 75,\n",
       " 'police': 76,\n",
       " '7': 77,\n",
       " 'results': 78,\n",
       " 'against': 79,\n",
       " 'second': 80,\n",
       " 'when': 81,\n",
       " '/': 82,\n",
       " 'also': 83,\n",
       " 'tuesday': 84,\n",
       " 'three': 85,\n",
       " 'soccer': 86,\n",
       " 'president': 87,\n",
       " 'no': 88,\n",
       " 'division': 89,\n",
       " 'told': 90,\n",
       " '10': 91,\n",
       " 'monday': 92,\n",
       " 'people': 93,\n",
       " 'about': 94,\n",
       " 'or': 95,\n",
       " 'friday': 96,\n",
       " 'league': 97,\n",
       " 'some': 98,\n",
       " 'london': 99,\n",
       " 'there': 100,\n",
       " 'world': 101,\n",
       " 'her': 102,\n",
       " 'minister': 103,\n",
       " 'under': 104,\n",
       " 'more': 105,\n",
       " 'york': 106,\n",
       " '9': 107,\n",
       " '1996-08-28': 108,\n",
       " 'won': 109,\n",
       " 'into': 110,\n",
       " 'state': 111,\n",
       " 'sunday': 112,\n",
       " '8': 113,\n",
       " 'before': 114,\n",
       " 'south': 115,\n",
       " 'played': 116,\n",
       " 'group': 117,\n",
       " 'market': 118,\n",
       " 'week': 119,\n",
       " 'england': 120,\n",
       " 'all': 121,\n",
       " \"'\": 122,\n",
       " 'time': 123,\n",
       " 'germany': 124,\n",
       " 'than': 125,\n",
       " 'could': 126,\n",
       " 'other': 127,\n",
       " 'australia': 128,\n",
       " 'she': 129,\n",
       " 'between': 130,\n",
       " 'since': 131,\n",
       " 'points': 132,\n",
       " 'match': 133,\n",
       " 'company': 134,\n",
       " 'bank': 135,\n",
       " 'round': 136,\n",
       " 'britain': 137,\n",
       " 'officials': 138,\n",
       " 'years': 139,\n",
       " 'games': 140,\n",
       " 'if': 141,\n",
       " 'newsroom': 142,\n",
       " 'only': 143,\n",
       " 'saturday': 144,\n",
       " 'national': 145,\n",
       " 'france': 146,\n",
       " 'party': 147,\n",
       " 'six': 148,\n",
       " 'former': 149,\n",
       " '1996-08-22': 150,\n",
       " 'four': 151,\n",
       " 'third': 152,\n",
       " 'home': 153,\n",
       " '1996-08-29': 154,\n",
       " 'city': 155,\n",
       " '1996': 156,\n",
       " 'off': 157,\n",
       " 'cup': 158,\n",
       " 'five': 159,\n",
       " 'win': 160,\n",
       " 'foreign': 161,\n",
       " '11': 162,\n",
       " '1996-08-27': 163,\n",
       " 'open': 164,\n",
       " 'day': 165,\n",
       " 'down': 166,\n",
       " 'august': 167,\n",
       " '13': 168,\n",
       " 'united': 169,\n",
       " 'because': 170,\n",
       " '6-4': 171,\n",
       " '6-3': 172,\n",
       " 'official': 173,\n",
       " 'did': 174,\n",
       " '21': 175,\n",
       " 'just': 176,\n",
       " 'next': 177,\n",
       " '15': 178,\n",
       " 'him': 179,\n",
       " 'spain': 180,\n",
       " 'standings': 181,\n",
       " '1996-08-26': 182,\n",
       " 'expected': 183,\n",
       " 'shares': 184,\n",
       " 'do': 185,\n",
       " 'statement': 186,\n",
       " 'spokesman': 187,\n",
       " 'news': 188,\n",
       " 'pakistan': 189,\n",
       " 'through': 190,\n",
       " 'may': 191,\n",
       " 'women': 192,\n",
       " 'made': 193,\n",
       " '70': 194,\n",
       " 'v': 195,\n",
       " 'while': 196,\n",
       " 'men': 197,\n",
       " '12': 198,\n",
       " 'chicago': 199,\n",
       " '1996-08-23': 200,\n",
       " 'international': 201,\n",
       " 'them': 202,\n",
       " '14': 203,\n",
       " 'lost': 204,\n",
       " 'german': 205,\n",
       " 'july': 206,\n",
       " 'where': 207,\n",
       " 'russian': 208,\n",
       " 'back': 209,\n",
       " '20': 210,\n",
       " '6-2': 211,\n",
       " 'september': 212,\n",
       " \"n't\": 213,\n",
       " 'china': 214,\n",
       " 'italy': 215,\n",
       " 'british': 216,\n",
       " '2.': 217,\n",
       " '3.': 218,\n",
       " 'european': 219,\n",
       " 'any': 220,\n",
       " 'peace': 221,\n",
       " 'team': 222,\n",
       " '1.': 223,\n",
       " 'clinton': 224,\n",
       " 'pct': 225,\n",
       " 'matches': 226,\n",
       " 'japan': 227,\n",
       " 'seconds': 228,\n",
       " 'prime': 229,\n",
       " 'reported': 230,\n",
       " 'billion': 231,\n",
       " '69': 232,\n",
       " 'month': 233,\n",
       " 'earlier': 234,\n",
       " 'june': 235,\n",
       " 'central': 236,\n",
       " 'now': 237,\n",
       " 'russia': 238,\n",
       " 'meeting': 239,\n",
       " 'still': 240,\n",
       " 'metres': 241,\n",
       " 'final': 242,\n",
       " '30': 243,\n",
       " 'being': 244,\n",
       " 'talks': 245,\n",
       " 'west': 246,\n",
       " 'added': 247,\n",
       " '71': 248,\n",
       " '1996-08-25': 249,\n",
       " 'half': 250,\n",
       " '1/2': 251,\n",
       " 'during': 252,\n",
       " 'french': 253,\n",
       " 'season': 254,\n",
       " 'b': 255,\n",
       " 'you': 256,\n",
       " 'set': 257,\n",
       " 'capital': 258,\n",
       " 'leading': 259,\n",
       " 'san': 260,\n",
       " 'take': 261,\n",
       " 'tonnes': 262,\n",
       " 'sweden': 263,\n",
       " 'killed': 264,\n",
       " 'st': 265,\n",
       " 'india': 266,\n",
       " 'lead': 267,\n",
       " 'held': 268,\n",
       " 'elections': 269,\n",
       " 'net': 270,\n",
       " 'around': 271,\n",
       " 'end': 272,\n",
       " 'war': 273,\n",
       " 'tennis': 274,\n",
       " 'security': 275,\n",
       " 'should': 276,\n",
       " 'reuters': 277,\n",
       " 'our': 278,\n",
       " 'most': 279,\n",
       " 'part': 280,\n",
       " 'mark': 281,\n",
       " 'game': 282,\n",
       " 'due': 283,\n",
       " 'early': 284,\n",
       " 'took': 285,\n",
       " 'states': 286,\n",
       " 'days': 287,\n",
       " 'so': 288,\n",
       " 'innings': 289,\n",
       " 'ministry': 290,\n",
       " 'saying': 291,\n",
       " 'number': 292,\n",
       " 'moscow': 293,\n",
       " '68': 294,\n",
       " 'cricket': 295,\n",
       " '6-1': 296,\n",
       " 'american': 297,\n",
       " 'major': 298,\n",
       " 'minutes': 299,\n",
       " 'union': 300,\n",
       " 'political': 301,\n",
       " 'seven': 302,\n",
       " 'per': 303,\n",
       " '7-6': 304,\n",
       " 'netherlands': 305,\n",
       " '100': 306,\n",
       " 'republic': 307,\n",
       " 'hong': 308,\n",
       " 'can': 309,\n",
       " 'north': 310,\n",
       " 'victory': 311,\n",
       " 'championship': 312,\n",
       " 'what': 313,\n",
       " 'both': 314,\n",
       " '22': 315,\n",
       " 'well': 316,\n",
       " 'belgium': 317,\n",
       " 'total': 318,\n",
       " 'country': 319,\n",
       " 'iraq': 320,\n",
       " 'play': 321,\n",
       " 'court': 322,\n",
       " 'kong': 323,\n",
       " '50': 324,\n",
       " 'close': 325,\n",
       " 'africa': 326,\n",
       " 'czech': 327,\n",
       " 'visit': 328,\n",
       " 'dutch': 329,\n",
       " '25': 330,\n",
       " 'result': 331,\n",
       " 'champion': 332,\n",
       " 'profit': 333,\n",
       " 'commission': 334,\n",
       " 'general': 335,\n",
       " 'israel': 336,\n",
       " 'left': 337,\n",
       " 'eight': 338,\n",
       " 'local': 339,\n",
       " 'minute': 340,\n",
       " 'says': 341,\n",
       " 'price': 342,\n",
       " 'trade': 343,\n",
       " 'town': 344,\n",
       " 'paris': 345,\n",
       " '66': 346,\n",
       " 'run': 347,\n",
       " '1996-08-24': 348,\n",
       " 'sales': 349,\n",
       " '4.': 350,\n",
       " 'very': 351,\n",
       " 'press': 352,\n",
       " '67': 353,\n",
       " 'vs.': 354,\n",
       " '5.': 355,\n",
       " '6.': 356,\n",
       " 'support': 357,\n",
       " 'put': 358,\n",
       " '1995': 359,\n",
       " '24': 360,\n",
       " 'leader': 361,\n",
       " 'prices': 362,\n",
       " '7-5': 363,\n",
       " 'then': 364,\n",
       " 'found': 365,\n",
       " 'record': 366,\n",
       " 'go': 367,\n",
       " 'same': 368,\n",
       " 'man': 369,\n",
       " 'western': 370,\n",
       " 'de': 371,\n",
       " 'english': 372,\n",
       " 'washington': 373,\n",
       " 'called': 374,\n",
       " 'northern': 375,\n",
       " 'inc': 376,\n",
       " 'say': 377,\n",
       " 'issue': 378,\n",
       " 'agency': 379,\n",
       " 'test': 380,\n",
       " 'opposition': 381,\n",
       " 'meet': 382,\n",
       " 'months': 383,\n",
       " 'military': 384,\n",
       " 'runs': 385,\n",
       " 'rate': 386,\n",
       " 'top': 387,\n",
       " 'these': 388,\n",
       " 'lower': 389,\n",
       " 'ago': 390,\n",
       " 'singles': 391,\n",
       " '64': 392,\n",
       " 'make': 393,\n",
       " 'amsterdam': 394,\n",
       " '72': 395,\n",
       " 'race': 396,\n",
       " 'newspaper': 397,\n",
       " 'deal': 398,\n",
       " 'goals': 399,\n",
       " '16': 400,\n",
       " 'office': 401,\n",
       " 'democratic': 402,\n",
       " 'ended': 403,\n",
       " '60': 404,\n",
       " 'cents': 405,\n",
       " 'until': 406,\n",
       " 'gave': 407,\n",
       " 'following': 408,\n",
       " 'leaders': 409,\n",
       " 'behind': 410,\n",
       " 'series': 411,\n",
       " 'another': 412,\n",
       " 'tour': 413,\n",
       " 'sri': 414,\n",
       " 'several': 415,\n",
       " 'chief': 416,\n",
       " 'late': 417,\n",
       " 'australian': 418,\n",
       " 'good': 419,\n",
       " 'african': 420,\n",
       " 'rebels': 421,\n",
       " 'near': 422,\n",
       " 'dollar': 423,\n",
       " 'players': 424,\n",
       " '75': 425,\n",
       " 'michael': 426,\n",
       " 'my': 427,\n",
       " 'report': 428,\n",
       " 'iraqi': 429,\n",
       " 'weekend': 430,\n",
       " 'economic': 431,\n",
       " 'start': 432,\n",
       " 'halftime': 433,\n",
       " 'tournament': 434,\n",
       " 'c': 435,\n",
       " 'miles': 436,\n",
       " 'refugees': 437,\n",
       " 'agreed': 438,\n",
       " 'southern': 439,\n",
       " 'many': 440,\n",
       " '74': 441,\n",
       " 'get': 442,\n",
       " 'power': 443,\n",
       " '7.': 444,\n",
       " 'plan': 445,\n",
       " 'going': 446,\n",
       " 'service': 447,\n",
       " 'east': 448,\n",
       " 'those': 449,\n",
       " 'election': 450,\n",
       " '73': 451,\n",
       " 'attendance': 452,\n",
       " ';': 453,\n",
       " 'sydney': 454,\n",
       " 'drawn': 455,\n",
       " 'place': 456,\n",
       " 'date': 457,\n",
       " 'want': 458,\n",
       " 'business': 459,\n",
       " 'paul': 460,\n",
       " 'white': 461,\n",
       " 'stock': 462,\n",
       " 'john': 463,\n",
       " 'television': 464,\n",
       " 'demand': 465,\n",
       " 'israeli': 466,\n",
       " 'david': 467,\n",
       " 'gmt': 468,\n",
       " 'taking': 469,\n",
       " 'baseball': 470,\n",
       " 'atlanta': 471,\n",
       " 'grand': 472,\n",
       " 'including': 473,\n",
       " '8.': 474,\n",
       " 'index': 475,\n",
       " 'arrested': 476,\n",
       " 'authorities': 477,\n",
       " 'quoted': 478,\n",
       " 'later': 479,\n",
       " 'palestinian': 480,\n",
       " 'parliament': 481,\n",
       " 'corp': 482,\n",
       " 'ahmed': 483,\n",
       " 'allowed': 484,\n",
       " 'forces': 485,\n",
       " 'asked': 486,\n",
       " 'cash': 487,\n",
       " 'hit': 488,\n",
       " 'km': 489,\n",
       " 'california': 490,\n",
       " 'zealand': 491,\n",
       " 'brazil': 492,\n",
       " 'house': 493,\n",
       " 'already': 494,\n",
       " 'army': 495,\n",
       " 'arafat': 496,\n",
       " 'rights': 497,\n",
       " 'martin': 498,\n",
       " 'club': 499,\n",
       " '17': 500,\n",
       " '28': 501,\n",
       " 'yeltsin': 502,\n",
       " 'fell': 503,\n",
       " 'toronto': 504,\n",
       " 'current': 505,\n",
       " 'return': 506,\n",
       " 'work': 507,\n",
       " 'higher': 508,\n",
       " 'ahead': 509,\n",
       " 'loss': 510,\n",
       " '31': 511,\n",
       " 'weeks': 512,\n",
       " 'm.': 513,\n",
       " '1-0': 514,\n",
       " '26': 515,\n",
       " 'exchange': 516,\n",
       " 'trading': 517,\n",
       " 'dole': 518,\n",
       " 'whether': 519,\n",
       " 'hours': 520,\n",
       " 'rose': 521,\n",
       " 'quarter': 522,\n",
       " 'came': 523,\n",
       " 'reporters': 524,\n",
       " 'announced': 525,\n",
       " 'jerusalem': 526,\n",
       " 'troops': 527,\n",
       " 'share': 528,\n",
       " 'closed': 529,\n",
       " 'finland': 530,\n",
       " 'way': 531,\n",
       " 'squad': 532,\n",
       " 'draw': 533,\n",
       " 'vs': 534,\n",
       " 'hospital': 535,\n",
       " 'agreement': 536,\n",
       " 'money': 537,\n",
       " '19': 538,\n",
       " 'countries': 539,\n",
       " '...': 540,\n",
       " 'like': 541,\n",
       " '23': 542,\n",
       " 'night': 543,\n",
       " 'oil': 544,\n",
       " 'head': 545,\n",
       " 'austria': 546,\n",
       " '1994': 547,\n",
       " 'best': 548,\n",
       " 'baltimore': 549,\n",
       " '18': 550,\n",
       " '65': 551,\n",
       " 'conference': 552,\n",
       " 'financial': 553,\n",
       " 'began': 554,\n",
       " 'scores': 555,\n",
       " 'away': 556,\n",
       " 'olympic': 557,\n",
       " 'went': 558,\n",
       " 'lanka': 559,\n",
       " 'morning': 560,\n",
       " 'decision': 561,\n",
       " 'high': 562,\n",
       " 'area': 563,\n",
       " 'old': 564,\n",
       " 'plans': 565,\n",
       " 'bonds': 566,\n",
       " 'such': 567,\n",
       " 'aug': 568,\n",
       " 'previous': 569,\n",
       " 'main': 570,\n",
       " 'few': 571,\n",
       " '1997': 572,\n",
       " 'past': 573,\n",
       " 'francisco': 574,\n",
       " 'los': 575,\n",
       " 'hits': 576,\n",
       " 'nations': 577,\n",
       " 'budget': 578,\n",
       " 'lebed': 579,\n",
       " 'taken': 580,\n",
       " 'human': 581,\n",
       " 'public': 582,\n",
       " 'us': 583,\n",
       " 'fighting': 584,\n",
       " 'fourth': 585,\n",
       " 'little': 586,\n",
       " 'champions': 587,\n",
       " 'eastern': 588,\n",
       " 'angeles': 589,\n",
       " 'nine': 590,\n",
       " 'latest': 591,\n",
       " 'gold': 592,\n",
       " 'italian': 593,\n",
       " 'van': 594,\n",
       " 'ban': 595,\n",
       " 'long': 596,\n",
       " 'march': 597,\n",
       " 'growth': 598,\n",
       " 'campaign': 599,\n",
       " 'side': 600,\n",
       " 'u.n.': 601,\n",
       " 'strike': 602,\n",
       " '63': 603,\n",
       " 'fall': 604,\n",
       " 'scored': 605,\n",
       " 'signed': 606,\n",
       " 'canada': 607,\n",
       " 'seattle': 608,\n",
       " 'colorado': 609,\n",
       " 'co': 610,\n",
       " 'much': 611,\n",
       " 'bill': 612,\n",
       " 'europe': 613,\n",
       " 'ukraine': 614,\n",
       " 'daily': 615,\n",
       " 'manager': 616,\n",
       " 'law': 617,\n",
       " 'working': 618,\n",
       " 'control': 619,\n",
       " 'region': 620,\n",
       " 'markets': 621,\n",
       " 'future': 622,\n",
       " 'winning': 623,\n",
       " 'boston': 624,\n",
       " '40': 625,\n",
       " 'times': 626,\n",
       " 'recent': 627,\n",
       " 'think': 628,\n",
       " 'call': 629,\n",
       " 'health': 630,\n",
       " 'accused': 631,\n",
       " 'right': 632,\n",
       " 'hold': 633,\n",
       " 'pay': 634,\n",
       " 'without': 635,\n",
       " 'attack': 636,\n",
       " 'own': 637,\n",
       " 'plane': 638,\n",
       " 'available': 639,\n",
       " '96': 640,\n",
       " 'akram': 641,\n",
       " 'wickets': 642,\n",
       " '62': 643,\n",
       " 'order': 644,\n",
       " 'airport': 645,\n",
       " 'workers': 646,\n",
       " 'again': 647,\n",
       " 'disease': 648,\n",
       " 'led': 649,\n",
       " 'died': 650,\n",
       " '27': 651,\n",
       " 'period': 652,\n",
       " 'give': 653,\n",
       " 'free': 654,\n",
       " 'average': 655,\n",
       " 'interest': 656,\n",
       " 'industry': 657,\n",
       " 'members': 658,\n",
       " 'ireland': 659,\n",
       " 'texas': 660,\n",
       " 'ajax': 661,\n",
       " '9.': 662,\n",
       " 'production': 663,\n",
       " 'forecast': 664,\n",
       " 'seen': 665,\n",
       " 'vote': 666,\n",
       " '---': 667,\n",
       " 'might': 668,\n",
       " 'sent': 669,\n",
       " 'despite': 670,\n",
       " 'embassy': 671,\n",
       " 'ceasefire': 672,\n",
       " 'failed': 673,\n",
       " 'help': 674,\n",
       " 'does': 675,\n",
       " 'centre': 676,\n",
       " 'released': 677,\n",
       " 'louis': 678,\n",
       " 'overs': 679,\n",
       " 'case': 680,\n",
       " \"'re\": 681,\n",
       " 'dutroux': 682,\n",
       " 'further': 683,\n",
       " 'across': 684,\n",
       " 'florida': 685,\n",
       " 'island': 686,\n",
       " 'short': 687,\n",
       " 'me': 688,\n",
       " 'october': 689,\n",
       " 'started': 690,\n",
       " 'county': 691,\n",
       " 'wasim': 692,\n",
       " 'captain': 693,\n",
       " 'costa': 694,\n",
       " 'cut': 695,\n",
       " 'belgian': 696,\n",
       " '29': 697,\n",
       " 'children': 698,\n",
       " 'death': 699,\n",
       " 'least': 700,\n",
       " 'council': 701,\n",
       " 'rugby': 702,\n",
       " 'black': 703,\n",
       " 'mother': 704,\n",
       " 'kurdish': 705,\n",
       " 'jordan': 706,\n",
       " 'planned': 707,\n",
       " 'stories': 708,\n",
       " 'analysts': 709,\n",
       " 'strong': 710,\n",
       " 'april': 711,\n",
       " 'given': 712,\n",
       " 'romania': 713,\n",
       " 'premier': 714,\n",
       " '59': 715,\n",
       " 'thomas': 716,\n",
       " 'coach': 717,\n",
       " 'minnesota': 718,\n",
       " 'kansas': 719,\n",
       " 'diego': 720,\n",
       " 'declared': 721,\n",
       " 'playing': 722,\n",
       " 'chechnya': 723,\n",
       " 'convention': 724,\n",
       " 'wheat': 725,\n",
       " 'brussels': 726,\n",
       " 'yr': 727,\n",
       " 'rates': 728,\n",
       " 'according': 729,\n",
       " 'slovakia': 730,\n",
       " 'title': 731,\n",
       " '54': 732,\n",
       " 'manchester': 733,\n",
       " 'houston': 734,\n",
       " 'shot': 735,\n",
       " 'fifth': 736,\n",
       " '10.': 737,\n",
       " 'yet': 738,\n",
       " 'civil': 739,\n",
       " 'confirmed': 740,\n",
       " 'bosnia': 741,\n",
       " 'bosnian': 742,\n",
       " 'republican': 743,\n",
       " 'reports': 744,\n",
       " 'contract': 745,\n",
       " 'chairman': 746,\n",
       " 'nearly': 747,\n",
       " 'used': 748,\n",
       " 'figures': 749,\n",
       " 'secretary': 750,\n",
       " 'force': 751,\n",
       " 'term': 752,\n",
       " 'must': 753,\n",
       " 'trying': 754,\n",
       " 'today': 755,\n",
       " 'poland': 756,\n",
       " 'possible': 757,\n",
       " 'dealers': 758,\n",
       " 'forced': 759,\n",
       " 'golf': 760,\n",
       " 'a.': 761,\n",
       " 'tabulate': 762,\n",
       " 'detroit': 763,\n",
       " 'oakland': 764,\n",
       " 'each': 765,\n",
       " 'companies': 766,\n",
       " 'seed': 767,\n",
       " 'point': 768,\n",
       " 'come': 769,\n",
       " 'passengers': 770,\n",
       " 'moslem': 771,\n",
       " 'treaty': 772,\n",
       " 'ltd': 773,\n",
       " 'peter': 774,\n",
       " 'action': 775,\n",
       " 'known': 776,\n",
       " 'brought': 777,\n",
       " 'taiwan': 778,\n",
       " 'opening': 779,\n",
       " 'federal': 780,\n",
       " 'sale': 781,\n",
       " 'face': 782,\n",
       " 'received': 783,\n",
       " 'estimated': 784,\n",
       " 'guerrillas': 785,\n",
       " 'exports': 786,\n",
       " 'aggregate': 787,\n",
       " 'named': 788,\n",
       " '2-0': 789,\n",
       " 'robert': 790,\n",
       " 'injured': 791,\n",
       " 'road': 792,\n",
       " 'woman': 793,\n",
       " 'clear': 794,\n",
       " 'small': 795,\n",
       " 'charges': 796,\n",
       " 'mexico': 797,\n",
       " 'traders': 798,\n",
       " 'kenya': 799,\n",
       " 'nigeria': 800,\n",
       " 'committee': 801,\n",
       " 'senior': 802,\n",
       " 'how': 803,\n",
       " 'radio': 804,\n",
       " 'details': 805,\n",
       " 'association': 806,\n",
       " 'straight': 807,\n",
       " 'full': 808,\n",
       " 'turkey': 809,\n",
       " 'red': 810,\n",
       " 'likely': 811,\n",
       " 'earnings': 812,\n",
       " 'immediately': 813,\n",
       " 'services': 814,\n",
       " 'use': 815,\n",
       " 'l': 816,\n",
       " 'wimbledon': 817,\n",
       " 'argentina': 818,\n",
       " 'cleveland': 819,\n",
       " '1,000': 820,\n",
       " '*': 821,\n",
       " 'note': 822,\n",
       " 'soon': 823,\n",
       " 'post': 824,\n",
       " 'investors': 825,\n",
       " 'showed': 826,\n",
       " 'private': 827,\n",
       " 'department': 828,\n",
       " 'yen': 829,\n",
       " 'japanese': 830,\n",
       " 'train': 831,\n",
       " '48': 832,\n",
       " 'bond': 833,\n",
       " 'egypt': 834,\n",
       " 'iran': 835,\n",
       " 'level': 836,\n",
       " 'air': 837,\n",
       " 'spot': 838,\n",
       " 'armed': 839,\n",
       " '76': 840,\n",
       " 'rally': 841,\n",
       " 'unless': 842,\n",
       " 'prix': 843,\n",
       " 'better': 844,\n",
       " 'croft': 845,\n",
       " '0-0': 846,\n",
       " 'cincinnati': 847,\n",
       " '61': 848,\n",
       " 'montreal': 849,\n",
       " 'philadelphia': 850,\n",
       " 'got': 851,\n",
       " 'medical': 852,\n",
       " 'groups': 853,\n",
       " 'although': 854,\n",
       " 'trip': 855,\n",
       " 'stage': 856,\n",
       " 'process': 857,\n",
       " 'policy': 858,\n",
       " 'see': 859,\n",
       " 'conditions': 860,\n",
       " 'believed': 861,\n",
       " 'bonn': 862,\n",
       " 'sold': 863,\n",
       " 'tax': 864,\n",
       " 'port': 865,\n",
       " 'coming': 866,\n",
       " 'board': 867,\n",
       " 'seeding': 868,\n",
       " 'holiday': 869,\n",
       " 'waqar': 870,\n",
       " 'younis': 871,\n",
       " 'mushtaq': 872,\n",
       " 'qualifier': 873,\n",
       " 'johnson': 874,\n",
       " 'milwaukee': 875,\n",
       " 'young': 876,\n",
       " 'psv': 877,\n",
       " 'defence': 878,\n",
       " 'swiss': 879,\n",
       " 'violence': 880,\n",
       " 'even': 881,\n",
       " 'school': 882,\n",
       " 'university': 883,\n",
       " 'prison': 884,\n",
       " 'information': 885,\n",
       " 'rise': 886,\n",
       " 'independence': 887,\n",
       " 'pound': 888,\n",
       " 'serb': 889,\n",
       " '6-0': 890,\n",
       " 'sell': 891,\n",
       " 'here': 892,\n",
       " 'message': 893,\n",
       " 'baghdad': 894,\n",
       " 'border': 895,\n",
       " 'scheduled': 896,\n",
       " 'data': 897,\n",
       " 'keep': 898,\n",
       " 'winner': 899,\n",
       " 's.': 900,\n",
       " 'rule': 901,\n",
       " 'within': 902,\n",
       " 'homer': 903,\n",
       " 'jose': 904,\n",
       " 'making': 905,\n",
       " 'system': 906,\n",
       " 'levels': 907,\n",
       " '56': 908,\n",
       " 'municipal': 909,\n",
       " 'deputy': 910,\n",
       " 'comment': 911,\n",
       " 'nuclear': 912,\n",
       " 'change': 913,\n",
       " 'buy': 914,\n",
       " 'polish': 915,\n",
       " 'based': 916,\n",
       " 'met': 917,\n",
       " 'rebel': 918,\n",
       " 'continue': 919,\n",
       " 'land': 920,\n",
       " 'saudi': 921,\n",
       " 'flight': 922,\n",
       " 'street': 923,\n",
       " '53': 924,\n",
       " 'know': 925,\n",
       " 'course': 926,\n",
       " 'scorers': 927,\n",
       " 'director': 928,\n",
       " 'khan': 929,\n",
       " 'mullally': 930,\n",
       " 'great': 931,\n",
       " 'canadian': 932,\n",
       " 'switzerland': 933,\n",
       " 'pittsburgh': 934,\n",
       " 'inning': 935,\n",
       " 'tried': 936,\n",
       " '55': 937,\n",
       " 'letter': 938,\n",
       " 'grozny': 939,\n",
       " 'completed': 940,\n",
       " 'charged': 941,\n",
       " 'wife': 942,\n",
       " 'volume': 943,\n",
       " 'illegal': 944,\n",
       " 'position': 945,\n",
       " 'indian': 946,\n",
       " 'economy': 947,\n",
       " 'reached': 948,\n",
       " 'struck': 949,\n",
       " 'bid': 950,\n",
       " 'king': 951,\n",
       " 'chinese': 952,\n",
       " 'goal': 953,\n",
       " 'talk': 954,\n",
       " 'netanyahu': 955,\n",
       " 'source': 956,\n",
       " 'kept': 957,\n",
       " 'movement': 958,\n",
       " 'areas': 959,\n",
       " 'outside': 960,\n",
       " 'puk': 961,\n",
       " 'parties': 962,\n",
       " 'attacks': 963,\n",
       " 'problem': 964,\n",
       " 'turkish': 965,\n",
       " 'step': 966,\n",
       " '45': 967,\n",
       " '2-1': 968,\n",
       " '1-1': 969,\n",
       " 'prefix': 970,\n",
       " 'opened': 971,\n",
       " 'tie': 972,\n",
       " 'percentage': 973,\n",
       " '58': 974,\n",
       " 'ground': 975,\n",
       " 'stocks': 976,\n",
       " 'able': 977,\n",
       " 'deficit': 978,\n",
       " 'arrived': 979,\n",
       " \"'m\": 980,\n",
       " 'poor': 981,\n",
       " 'village': 982,\n",
       " 'coast': 983,\n",
       " 'osce': 984,\n",
       " 'leave': 985,\n",
       " 'having': 986,\n",
       " 'drug': 987,\n",
       " 'lot': 988,\n",
       " 'eu': 989,\n",
       " 'needed': 990,\n",
       " 'similar': 991,\n",
       " 'denied': 992,\n",
       " 'paper': 993,\n",
       " 'beijing': 994,\n",
       " 'province': 995,\n",
       " 'executive': 996,\n",
       " 'compared': 997,\n",
       " '300': 998,\n",
       " 'schedule': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cross-sodium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "solved-airport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[989,\n",
       " 10951,\n",
       " 205,\n",
       " 629,\n",
       " 7,\n",
       " 3939,\n",
       " 216,\n",
       " 5774,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode_tokens(tokens, max_seq_len=128):\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in list(token_to_id.keys()):\n",
    "            tokens[i] = token_to_id[token]\n",
    "        else:\n",
    "            tokens[i] = token_to_id['[UNK]'] # unknown token\n",
    "    # padding\n",
    "    if len(tokens) < max_seq_len:\n",
    "        tokens = tokens + [0] * (max_seq_len - len(tokens))\n",
    "    # truncate\n",
    "    elif len(tokens) >= max_seq_len:\n",
    "        tokens = tokens[:max_seq_len]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "# map_record_to_training_data('9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0')\n",
    "encode_tokens(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "theoretical-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, data_path, max_length):\n",
    "            self.data_path = data_path\n",
    "            self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        f = open(self.data_path, \"r\")\n",
    "        return len(f.readlines())\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            tokens, tags = map_record_to_training_data(lines[idx])\n",
    "            token_ids = encode_tokens(tokens)\n",
    "            \n",
    "            # padding\n",
    "            if len(tags) < self.max_length:\n",
    "                tags = tags + [0] * (self.max_length - len(tags))\n",
    "                \n",
    "            # truncate\n",
    "            elif len(tags) >= self.max_length:\n",
    "                tags = tags[:self.max_length]\n",
    "            output = {'input' : torch.tensor(token_ids),\n",
    "                     'target' : torch.tensor(tags)}\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "mineral-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset('datasets/conll2003/conll_train.txt', max_length=128)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "amazing-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = NERDataset('datasets/conll2003/conll_val.txt', max_length=128)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "identified-spectacular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2041,   203,    23,  ...,     0,     0,     0],\n",
      "        [ 1592,    65,    43,  ...,     0,     0,     0],\n",
      "        [ 6697,    33,  3269,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  846,     3,     0,  ...,     0,     0,     0],\n",
      "        [17396,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 1054,    25,   285,  ...,     0,     0,     0]])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(iter(train_loader).next()['input'])\n",
    "print(iter(train_loader).next()['input'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "together-marine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [4, 5, 1,  ..., 0, 0, 0],\n",
      "        [4, 5, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 2, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [2, 1, 1,  ..., 0, 0, 0]])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "print(iter(train_loader).next()['target'])\n",
    "print(iter(train_loader).next()['target'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-truth",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-membership",
   "metadata": {},
   "source": [
    "**PyTorch 구현 과정 중 알게 된 개념**\n",
    "\n",
    "* model.zero_grad(), optimizer.zero_grad() 차이 [link](https://minsuksung-ai.tistory.com/24)\n",
    "> 내가 학습하고자 하는 가중치만 zero_grad할 경우에는 optimizer.zero_grad()\n",
    "> 모델의 모든 가중치에 대해 적용할 때에는 model.zero_grad()\n",
    "\n",
    "* Keras의 sparsecategoricalCE\n",
    "\n",
    "```\n",
    "keras.losses.SparseCategoricalCrossentropy(\n",
    "             from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "         )\n",
    "\n",
    "```\n",
    "1) 훈련 데이터의 label(target)이 one-hot vector 이면 CategoricalCrossentropy\n",
    "2) 훈련 데이터의 label(target)이 정수이면 SparseCategoricalCrossentropy\n",
    "3) One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector. [link](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)\n",
    "\n",
    "* PyTorch의 `nn.CrossEntropyLoss()`, `nn.NLLLoss()` ! [link](https://stackoverflow.com/questions/65408027/how-to-correctly-use-cross-entropy-loss-vs-softmax-for-classification)\n",
    "\n",
    "    * 모델 학습이 잘 되지 않는다면, 그리고 데이터나 모델 자체에 문제가 없는 것 같다면, loss 계산 과정도 의심해보자!\n",
    "    1) The input given through a forward call is expected to contain log-probabilities of each class. [Doc](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
    "    2) This criterion combines LogSoftmax and NLLLoss in one single class. [Doc](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-bennett",
   "metadata": {},
   "source": [
    "**Appendix 참조**\n",
    "* CE loss\n",
    "Keras의 CE Loss 구현 상세\n",
    "    * 아래 두 개는 동일한 결과를 출력하는데, keras는 2번 방식으로 구현해 둠\n",
    "    1) nn.CrossEntropyLoss(ignore_index=0)\n",
    "    2) nn.CrossEntropyLoss(reduce=False) + padding masking\n",
    "\n",
    "* rearrange\n",
    "    * reshpae =/= transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-driver",
   "metadata": {},
   "source": [
    "##### AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print ('Current device : ', device)\n",
    "model = NERModel(num_tags, vocab_size, max_seq_len=128, d_model=32, heads=4, d_ff=64).to(device)\n",
    "\n",
    "num_epochs=10\n",
    "total_step = len(train_loader)\n",
    "learning_rate = 0.001 #0.0005\n",
    "\n",
    "# TODO : change to nn.CrossEntropyLoss(ignore_index=0) and remove masking phase during training.\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index=0, reduce=False)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cooperative-bahamas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a7a9ff85bb4cec8be984f9701412e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [9/439], Loss: 1.9491\n",
      "Epoch [1/10], Step [19/439], Loss: 1.3715\n",
      "Epoch [1/10], Step [29/439], Loss: 0.9002\n",
      "Epoch [1/10], Step [39/439], Loss: 0.6570\n",
      "Epoch [1/10], Step [49/439], Loss: 0.8392\n",
      "Epoch [1/10], Step [59/439], Loss: 0.7199\n",
      "Epoch [1/10], Step [69/439], Loss: 0.7005\n",
      "Epoch [1/10], Step [79/439], Loss: 0.7052\n",
      "Epoch [1/10], Step [89/439], Loss: 0.8737\n",
      "Epoch [1/10], Step [99/439], Loss: 0.6994\n",
      "Epoch [1/10], Step [109/439], Loss: 0.6701\n",
      "Epoch [1/10], Step [119/439], Loss: 0.7754\n",
      "Epoch [1/10], Step [129/439], Loss: 0.7733\n",
      "Epoch [1/10], Step [139/439], Loss: 0.6467\n",
      "Epoch [1/10], Step [149/439], Loss: 0.8148\n",
      "Epoch [1/10], Step [159/439], Loss: 0.8317\n",
      "Epoch [1/10], Step [169/439], Loss: 0.8076\n",
      "Epoch [1/10], Step [179/439], Loss: 0.6993\n",
      "Epoch [1/10], Step [189/439], Loss: 0.7098\n",
      "Epoch [1/10], Step [199/439], Loss: 0.6655\n",
      "Epoch [1/10], Step [209/439], Loss: 0.7074\n",
      "Epoch [1/10], Step [219/439], Loss: 0.8586\n",
      "Epoch [1/10], Step [229/439], Loss: 0.7728\n",
      "Epoch [1/10], Step [239/439], Loss: 0.7021\n",
      "Epoch [1/10], Step [249/439], Loss: 0.7895\n",
      "Epoch [1/10], Step [259/439], Loss: 0.7334\n",
      "Epoch [1/10], Step [269/439], Loss: 0.6334\n",
      "Epoch [1/10], Step [279/439], Loss: 0.6242\n",
      "Epoch [1/10], Step [289/439], Loss: 0.8302\n",
      "Epoch [1/10], Step [299/439], Loss: 0.8266\n",
      "Epoch [1/10], Step [309/439], Loss: 0.5755\n",
      "Epoch [1/10], Step [319/439], Loss: 0.5783\n",
      "Epoch [1/10], Step [329/439], Loss: 0.5738\n",
      "Epoch [1/10], Step [339/439], Loss: 0.6387\n",
      "Epoch [1/10], Step [349/439], Loss: 0.5747\n",
      "Epoch [1/10], Step [359/439], Loss: 0.7064\n",
      "Epoch [1/10], Step [369/439], Loss: 0.6675\n",
      "Epoch [1/10], Step [379/439], Loss: 0.6351\n",
      "Epoch [1/10], Step [389/439], Loss: 0.7905\n",
      "Epoch [1/10], Step [399/439], Loss: 0.6029\n",
      "Epoch [1/10], Step [409/439], Loss: 0.6156\n",
      "Epoch [1/10], Step [419/439], Loss: 0.6963\n",
      "Epoch [1/10], Step [429/439], Loss: 0.6659\n",
      "Epoch [2/10], Step [9/439], Loss: 0.5487\n",
      "Epoch [2/10], Step [19/439], Loss: 0.7172\n",
      "Epoch [2/10], Step [29/439], Loss: 0.5785\n",
      "Epoch [2/10], Step [39/439], Loss: 0.6431\n",
      "Epoch [2/10], Step [49/439], Loss: 0.7107\n",
      "Epoch [2/10], Step [59/439], Loss: 0.5342\n",
      "Epoch [2/10], Step [69/439], Loss: 0.6627\n",
      "Epoch [2/10], Step [79/439], Loss: 0.7105\n",
      "Epoch [2/10], Step [89/439], Loss: 0.5332\n",
      "Epoch [2/10], Step [99/439], Loss: 0.7267\n",
      "Epoch [2/10], Step [109/439], Loss: 0.5653\n",
      "Epoch [2/10], Step [119/439], Loss: 0.5489\n",
      "Epoch [2/10], Step [129/439], Loss: 0.6897\n",
      "Epoch [2/10], Step [139/439], Loss: 0.4849\n",
      "Epoch [2/10], Step [149/439], Loss: 0.6030\n",
      "Epoch [2/10], Step [159/439], Loss: 0.5859\n",
      "Epoch [2/10], Step [169/439], Loss: 0.6137\n",
      "Epoch [2/10], Step [179/439], Loss: 0.5419\n",
      "Epoch [2/10], Step [189/439], Loss: 0.6083\n",
      "Epoch [2/10], Step [199/439], Loss: 0.6235\n",
      "Epoch [2/10], Step [209/439], Loss: 0.7406\n",
      "Epoch [2/10], Step [219/439], Loss: 0.5604\n",
      "Epoch [2/10], Step [229/439], Loss: 0.5266\n",
      "Epoch [2/10], Step [239/439], Loss: 0.8430\n",
      "Epoch [2/10], Step [249/439], Loss: 0.6356\n",
      "Epoch [2/10], Step [259/439], Loss: 0.5542\n",
      "Epoch [2/10], Step [269/439], Loss: 0.4508\n",
      "Epoch [2/10], Step [279/439], Loss: 0.5730\n",
      "Epoch [2/10], Step [289/439], Loss: 0.6091\n",
      "Epoch [2/10], Step [299/439], Loss: 0.6968\n",
      "Epoch [2/10], Step [309/439], Loss: 0.6840\n",
      "Epoch [2/10], Step [319/439], Loss: 0.6619\n",
      "Epoch [2/10], Step [329/439], Loss: 0.6124\n",
      "Epoch [2/10], Step [339/439], Loss: 0.5112\n",
      "Epoch [2/10], Step [349/439], Loss: 0.6011\n",
      "Epoch [2/10], Step [359/439], Loss: 0.5941\n",
      "Epoch [2/10], Step [369/439], Loss: 0.5118\n",
      "Epoch [2/10], Step [379/439], Loss: 0.5460\n",
      "Epoch [2/10], Step [389/439], Loss: 0.6964\n",
      "Epoch [2/10], Step [399/439], Loss: 0.6771\n",
      "Epoch [2/10], Step [409/439], Loss: 0.7470\n",
      "Epoch [2/10], Step [419/439], Loss: 0.5187\n",
      "Epoch [2/10], Step [429/439], Loss: 0.6914\n",
      "Epoch [3/10], Step [9/439], Loss: 0.4904\n",
      "Epoch [3/10], Step [19/439], Loss: 0.5083\n",
      "Epoch [3/10], Step [29/439], Loss: 0.4035\n",
      "Epoch [3/10], Step [39/439], Loss: 0.5656\n",
      "Epoch [3/10], Step [49/439], Loss: 0.5520\n",
      "Epoch [3/10], Step [59/439], Loss: 0.5243\n",
      "Epoch [3/10], Step [69/439], Loss: 0.5689\n",
      "Epoch [3/10], Step [79/439], Loss: 0.6922\n",
      "Epoch [3/10], Step [89/439], Loss: 0.5146\n",
      "Epoch [3/10], Step [99/439], Loss: 0.7896\n",
      "Epoch [3/10], Step [109/439], Loss: 0.6319\n",
      "Epoch [3/10], Step [119/439], Loss: 0.6218\n",
      "Epoch [3/10], Step [129/439], Loss: 0.4753\n",
      "Epoch [3/10], Step [139/439], Loss: 0.6332\n",
      "Epoch [3/10], Step [149/439], Loss: 0.4421\n",
      "Epoch [3/10], Step [159/439], Loss: 0.5908\n",
      "Epoch [3/10], Step [169/439], Loss: 0.5407\n",
      "Epoch [3/10], Step [179/439], Loss: 0.3419\n",
      "Epoch [3/10], Step [189/439], Loss: 0.5323\n",
      "Epoch [3/10], Step [199/439], Loss: 0.4938\n",
      "Epoch [3/10], Step [209/439], Loss: 0.5636\n",
      "Epoch [3/10], Step [219/439], Loss: 0.6312\n",
      "Epoch [3/10], Step [229/439], Loss: 0.5566\n",
      "Epoch [3/10], Step [239/439], Loss: 0.5016\n",
      "Epoch [3/10], Step [249/439], Loss: 0.4609\n",
      "Epoch [3/10], Step [259/439], Loss: 0.4705\n",
      "Epoch [3/10], Step [269/439], Loss: 0.5607\n",
      "Epoch [3/10], Step [279/439], Loss: 0.5334\n",
      "Epoch [3/10], Step [289/439], Loss: 0.5686\n",
      "Epoch [3/10], Step [299/439], Loss: 0.5174\n",
      "Epoch [3/10], Step [309/439], Loss: 0.5215\n",
      "Epoch [3/10], Step [319/439], Loss: 0.5425\n",
      "Epoch [3/10], Step [329/439], Loss: 0.5868\n",
      "Epoch [3/10], Step [339/439], Loss: 0.5224\n",
      "Epoch [3/10], Step [349/439], Loss: 0.4514\n",
      "Epoch [3/10], Step [359/439], Loss: 0.4277\n",
      "Epoch [3/10], Step [369/439], Loss: 0.5598\n",
      "Epoch [3/10], Step [379/439], Loss: 0.3684\n",
      "Epoch [3/10], Step [389/439], Loss: 0.4275\n",
      "Epoch [3/10], Step [399/439], Loss: 0.4532\n",
      "Epoch [3/10], Step [409/439], Loss: 0.5394\n",
      "Epoch [3/10], Step [419/439], Loss: 0.6120\n",
      "Epoch [3/10], Step [429/439], Loss: 0.5985\n",
      "Epoch [4/10], Step [9/439], Loss: 0.5318\n",
      "Epoch [4/10], Step [19/439], Loss: 0.5727\n",
      "Epoch [4/10], Step [29/439], Loss: 0.3804\n",
      "Epoch [4/10], Step [39/439], Loss: 0.5030\n",
      "Epoch [4/10], Step [49/439], Loss: 0.4943\n",
      "Epoch [4/10], Step [59/439], Loss: 0.4652\n",
      "Epoch [4/10], Step [69/439], Loss: 0.4614\n",
      "Epoch [4/10], Step [79/439], Loss: 0.5174\n",
      "Epoch [4/10], Step [89/439], Loss: 0.3793\n",
      "Epoch [4/10], Step [99/439], Loss: 0.3917\n",
      "Epoch [4/10], Step [109/439], Loss: 0.4117\n",
      "Epoch [4/10], Step [119/439], Loss: 0.4436\n",
      "Epoch [4/10], Step [129/439], Loss: 0.3990\n",
      "Epoch [4/10], Step [139/439], Loss: 0.5430\n",
      "Epoch [4/10], Step [149/439], Loss: 0.4715\n",
      "Epoch [4/10], Step [159/439], Loss: 0.4527\n",
      "Epoch [4/10], Step [169/439], Loss: 0.4357\n",
      "Epoch [4/10], Step [179/439], Loss: 0.5241\n",
      "Epoch [4/10], Step [189/439], Loss: 0.5837\n",
      "Epoch [4/10], Step [199/439], Loss: 0.4673\n",
      "Epoch [4/10], Step [209/439], Loss: 0.5440\n",
      "Epoch [4/10], Step [219/439], Loss: 0.3817\n",
      "Epoch [4/10], Step [229/439], Loss: 0.4553\n",
      "Epoch [4/10], Step [239/439], Loss: 0.5116\n",
      "Epoch [4/10], Step [249/439], Loss: 0.4075\n",
      "Epoch [4/10], Step [259/439], Loss: 0.4895\n",
      "Epoch [4/10], Step [269/439], Loss: 0.4027\n",
      "Epoch [4/10], Step [279/439], Loss: 0.4676\n",
      "Epoch [4/10], Step [289/439], Loss: 0.4604\n",
      "Epoch [4/10], Step [299/439], Loss: 0.6383\n",
      "Epoch [4/10], Step [309/439], Loss: 0.4088\n",
      "Epoch [4/10], Step [319/439], Loss: 0.4646\n",
      "Epoch [4/10], Step [329/439], Loss: 0.4898\n",
      "Epoch [4/10], Step [339/439], Loss: 0.3804\n",
      "Epoch [4/10], Step [349/439], Loss: 0.4637\n",
      "Epoch [4/10], Step [359/439], Loss: 0.4161\n",
      "Epoch [4/10], Step [369/439], Loss: 0.4598\n",
      "Epoch [4/10], Step [379/439], Loss: 0.3944\n",
      "Epoch [4/10], Step [389/439], Loss: 0.4440\n",
      "Epoch [4/10], Step [399/439], Loss: 0.4285\n",
      "Epoch [4/10], Step [409/439], Loss: 0.3879\n",
      "Epoch [4/10], Step [419/439], Loss: 0.4705\n",
      "Epoch [4/10], Step [429/439], Loss: 0.6005\n",
      "Epoch [5/10], Step [9/439], Loss: 0.3902\n",
      "Epoch [5/10], Step [19/439], Loss: 0.5044\n",
      "Epoch [5/10], Step [29/439], Loss: 0.4328\n",
      "Epoch [5/10], Step [39/439], Loss: 0.5181\n",
      "Epoch [5/10], Step [49/439], Loss: 0.4003\n",
      "Epoch [5/10], Step [59/439], Loss: 0.4661\n",
      "Epoch [5/10], Step [69/439], Loss: 0.3390\n",
      "Epoch [5/10], Step [79/439], Loss: 0.3842\n",
      "Epoch [5/10], Step [89/439], Loss: 0.5424\n",
      "Epoch [5/10], Step [99/439], Loss: 0.3516\n",
      "Epoch [5/10], Step [109/439], Loss: 0.3991\n",
      "Epoch [5/10], Step [119/439], Loss: 0.4117\n",
      "Epoch [5/10], Step [129/439], Loss: 0.3392\n",
      "Epoch [5/10], Step [139/439], Loss: 0.4202\n",
      "Epoch [5/10], Step [149/439], Loss: 0.3174\n",
      "Epoch [5/10], Step [159/439], Loss: 0.3500\n",
      "Epoch [5/10], Step [169/439], Loss: 0.4467\n",
      "Epoch [5/10], Step [179/439], Loss: 0.5543\n",
      "Epoch [5/10], Step [189/439], Loss: 0.3568\n",
      "Epoch [5/10], Step [199/439], Loss: 0.4431\n",
      "Epoch [5/10], Step [209/439], Loss: 0.4920\n",
      "Epoch [5/10], Step [219/439], Loss: 0.3350\n",
      "Epoch [5/10], Step [229/439], Loss: 0.5070\n",
      "Epoch [5/10], Step [239/439], Loss: 0.2777\n",
      "Epoch [5/10], Step [249/439], Loss: 0.4991\n",
      "Epoch [5/10], Step [259/439], Loss: 0.5556\n",
      "Epoch [5/10], Step [269/439], Loss: 0.4078\n",
      "Epoch [5/10], Step [279/439], Loss: 0.5198\n",
      "Epoch [5/10], Step [289/439], Loss: 0.4145\n",
      "Epoch [5/10], Step [299/439], Loss: 0.4884\n",
      "Epoch [5/10], Step [309/439], Loss: 0.4694\n",
      "Epoch [5/10], Step [319/439], Loss: 0.4697\n",
      "Epoch [5/10], Step [329/439], Loss: 0.3545\n",
      "Epoch [5/10], Step [339/439], Loss: 0.5054\n",
      "Epoch [5/10], Step [349/439], Loss: 0.3337\n",
      "Epoch [5/10], Step [359/439], Loss: 0.3304\n",
      "Epoch [5/10], Step [369/439], Loss: 0.3020\n",
      "Epoch [5/10], Step [379/439], Loss: 0.4152\n",
      "Epoch [5/10], Step [389/439], Loss: 0.3795\n",
      "Epoch [5/10], Step [399/439], Loss: 0.4630\n",
      "Epoch [5/10], Step [409/439], Loss: 0.3726\n",
      "Epoch [5/10], Step [419/439], Loss: 0.4799\n",
      "Epoch [5/10], Step [429/439], Loss: 0.4971\n",
      "Epoch [6/10], Step [9/439], Loss: 0.3732\n",
      "Epoch [6/10], Step [19/439], Loss: 0.4133\n",
      "Epoch [6/10], Step [29/439], Loss: 0.4528\n",
      "Epoch [6/10], Step [39/439], Loss: 0.3294\n",
      "Epoch [6/10], Step [49/439], Loss: 0.3115\n",
      "Epoch [6/10], Step [59/439], Loss: 0.3354\n",
      "Epoch [6/10], Step [69/439], Loss: 0.4260\n",
      "Epoch [6/10], Step [79/439], Loss: 0.3887\n",
      "Epoch [6/10], Step [89/439], Loss: 0.3007\n",
      "Epoch [6/10], Step [99/439], Loss: 0.4046\n",
      "Epoch [6/10], Step [109/439], Loss: 0.3243\n",
      "Epoch [6/10], Step [119/439], Loss: 0.4979\n",
      "Epoch [6/10], Step [129/439], Loss: 0.3959\n",
      "Epoch [6/10], Step [139/439], Loss: 0.3165\n",
      "Epoch [6/10], Step [149/439], Loss: 0.2813\n",
      "Epoch [6/10], Step [159/439], Loss: 0.2915\n",
      "Epoch [6/10], Step [169/439], Loss: 0.3294\n",
      "Epoch [6/10], Step [179/439], Loss: 0.4868\n",
      "Epoch [6/10], Step [189/439], Loss: 0.4260\n",
      "Epoch [6/10], Step [199/439], Loss: 0.3315\n",
      "Epoch [6/10], Step [209/439], Loss: 0.3686\n",
      "Epoch [6/10], Step [219/439], Loss: 0.3714\n",
      "Epoch [6/10], Step [229/439], Loss: 0.3424\n",
      "Epoch [6/10], Step [239/439], Loss: 0.3487\n",
      "Epoch [6/10], Step [249/439], Loss: 0.4222\n",
      "Epoch [6/10], Step [259/439], Loss: 0.3218\n",
      "Epoch [6/10], Step [269/439], Loss: 0.3900\n",
      "Epoch [6/10], Step [279/439], Loss: 0.3983\n",
      "Epoch [6/10], Step [289/439], Loss: 0.4118\n",
      "Epoch [6/10], Step [299/439], Loss: 0.3586\n",
      "Epoch [6/10], Step [309/439], Loss: 0.3885\n",
      "Epoch [6/10], Step [319/439], Loss: 0.5194\n",
      "Epoch [6/10], Step [329/439], Loss: 0.3018\n",
      "Epoch [6/10], Step [339/439], Loss: 0.2902\n",
      "Epoch [6/10], Step [349/439], Loss: 0.3517\n",
      "Epoch [6/10], Step [359/439], Loss: 0.4680\n",
      "Epoch [6/10], Step [369/439], Loss: 0.3935\n",
      "Epoch [6/10], Step [379/439], Loss: 0.3815\n",
      "Epoch [6/10], Step [389/439], Loss: 0.5406\n",
      "Epoch [6/10], Step [399/439], Loss: 0.3830\n",
      "Epoch [6/10], Step [409/439], Loss: 0.4571\n",
      "Epoch [6/10], Step [419/439], Loss: 0.3641\n",
      "Epoch [6/10], Step [429/439], Loss: 0.3074\n",
      "Epoch [7/10], Step [9/439], Loss: 0.2835\n",
      "Epoch [7/10], Step [19/439], Loss: 0.4224\n",
      "Epoch [7/10], Step [29/439], Loss: 0.2779\n",
      "Epoch [7/10], Step [39/439], Loss: 0.3097\n",
      "Epoch [7/10], Step [49/439], Loss: 0.2217\n",
      "Epoch [7/10], Step [59/439], Loss: 0.4318\n",
      "Epoch [7/10], Step [69/439], Loss: 0.4552\n",
      "Epoch [7/10], Step [79/439], Loss: 0.4447\n",
      "Epoch [7/10], Step [89/439], Loss: 0.3918\n",
      "Epoch [7/10], Step [99/439], Loss: 0.2943\n",
      "Epoch [7/10], Step [109/439], Loss: 0.3001\n",
      "Epoch [7/10], Step [119/439], Loss: 0.4595\n",
      "Epoch [7/10], Step [129/439], Loss: 0.4406\n",
      "Epoch [7/10], Step [139/439], Loss: 0.4200\n",
      "Epoch [7/10], Step [149/439], Loss: 0.3971\n",
      "Epoch [7/10], Step [159/439], Loss: 0.4240\n",
      "Epoch [7/10], Step [169/439], Loss: 0.2535\n",
      "Epoch [7/10], Step [179/439], Loss: 0.3283\n",
      "Epoch [7/10], Step [189/439], Loss: 0.3778\n",
      "Epoch [7/10], Step [199/439], Loss: 0.4945\n",
      "Epoch [7/10], Step [209/439], Loss: 0.3342\n",
      "Epoch [7/10], Step [219/439], Loss: 0.4097\n",
      "Epoch [7/10], Step [229/439], Loss: 0.4078\n",
      "Epoch [7/10], Step [239/439], Loss: 0.3166\n",
      "Epoch [7/10], Step [249/439], Loss: 0.3767\n",
      "Epoch [7/10], Step [259/439], Loss: 0.3158\n",
      "Epoch [7/10], Step [269/439], Loss: 0.3371\n",
      "Epoch [7/10], Step [279/439], Loss: 0.4063\n",
      "Epoch [7/10], Step [289/439], Loss: 0.3888\n",
      "Epoch [7/10], Step [299/439], Loss: 0.2284\n",
      "Epoch [7/10], Step [309/439], Loss: 0.4723\n",
      "Epoch [7/10], Step [319/439], Loss: 0.4354\n",
      "Epoch [7/10], Step [329/439], Loss: 0.4703\n",
      "Epoch [7/10], Step [339/439], Loss: 0.5658\n",
      "Epoch [7/10], Step [349/439], Loss: 0.4552\n",
      "Epoch [7/10], Step [359/439], Loss: 0.4319\n",
      "Epoch [7/10], Step [369/439], Loss: 0.3376\n",
      "Epoch [7/10], Step [379/439], Loss: 0.4238\n",
      "Epoch [7/10], Step [389/439], Loss: 0.3303\n",
      "Epoch [7/10], Step [399/439], Loss: 0.4021\n",
      "Epoch [7/10], Step [409/439], Loss: 0.4970\n",
      "Epoch [7/10], Step [419/439], Loss: 0.2842\n",
      "Epoch [7/10], Step [429/439], Loss: 0.3424\n",
      "Epoch [8/10], Step [9/439], Loss: 0.2831\n",
      "Epoch [8/10], Step [19/439], Loss: 0.3475\n",
      "Epoch [8/10], Step [29/439], Loss: 0.3744\n",
      "Epoch [8/10], Step [39/439], Loss: 0.3531\n",
      "Epoch [8/10], Step [49/439], Loss: 0.2666\n",
      "Epoch [8/10], Step [59/439], Loss: 0.2412\n",
      "Epoch [8/10], Step [69/439], Loss: 0.3423\n",
      "Epoch [8/10], Step [79/439], Loss: 0.4059\n",
      "Epoch [8/10], Step [89/439], Loss: 0.3919\n",
      "Epoch [8/10], Step [99/439], Loss: 0.3485\n",
      "Epoch [8/10], Step [109/439], Loss: 0.3081\n",
      "Epoch [8/10], Step [119/439], Loss: 0.2894\n",
      "Epoch [8/10], Step [129/439], Loss: 0.3621\n",
      "Epoch [8/10], Step [139/439], Loss: 0.3372\n",
      "Epoch [8/10], Step [149/439], Loss: 0.2815\n",
      "Epoch [8/10], Step [159/439], Loss: 0.3230\n",
      "Epoch [8/10], Step [169/439], Loss: 0.2569\n",
      "Epoch [8/10], Step [179/439], Loss: 0.2830\n",
      "Epoch [8/10], Step [189/439], Loss: 0.3892\n",
      "Epoch [8/10], Step [199/439], Loss: 0.3347\n",
      "Epoch [8/10], Step [209/439], Loss: 0.3619\n",
      "Epoch [8/10], Step [219/439], Loss: 0.3780\n",
      "Epoch [8/10], Step [229/439], Loss: 0.4444\n",
      "Epoch [8/10], Step [239/439], Loss: 0.2578\n",
      "Epoch [8/10], Step [249/439], Loss: 0.3056\n",
      "Epoch [8/10], Step [259/439], Loss: 0.4220\n",
      "Epoch [8/10], Step [269/439], Loss: 0.4286\n",
      "Epoch [8/10], Step [279/439], Loss: 0.3458\n",
      "Epoch [8/10], Step [289/439], Loss: 0.3353\n",
      "Epoch [8/10], Step [299/439], Loss: 0.3234\n",
      "Epoch [8/10], Step [309/439], Loss: 0.2980\n",
      "Epoch [8/10], Step [319/439], Loss: 0.2507\n",
      "Epoch [8/10], Step [329/439], Loss: 0.3177\n",
      "Epoch [8/10], Step [339/439], Loss: 0.2524\n",
      "Epoch [8/10], Step [349/439], Loss: 0.3773\n",
      "Epoch [8/10], Step [359/439], Loss: 0.2034\n",
      "Epoch [8/10], Step [369/439], Loss: 0.2580\n",
      "Epoch [8/10], Step [379/439], Loss: 0.3481\n",
      "Epoch [8/10], Step [389/439], Loss: 0.3125\n",
      "Epoch [8/10], Step [399/439], Loss: 0.3487\n",
      "Epoch [8/10], Step [409/439], Loss: 0.3782\n",
      "Epoch [8/10], Step [419/439], Loss: 0.5144\n",
      "Epoch [8/10], Step [429/439], Loss: 0.3347\n",
      "Epoch [9/10], Step [9/439], Loss: 0.2758\n",
      "Epoch [9/10], Step [19/439], Loss: 0.2296\n",
      "Epoch [9/10], Step [29/439], Loss: 0.4405\n",
      "Epoch [9/10], Step [39/439], Loss: 0.2557\n",
      "Epoch [9/10], Step [49/439], Loss: 0.3088\n",
      "Epoch [9/10], Step [59/439], Loss: 0.3670\n",
      "Epoch [9/10], Step [69/439], Loss: 0.2120\n",
      "Epoch [9/10], Step [79/439], Loss: 0.3244\n",
      "Epoch [9/10], Step [89/439], Loss: 0.3455\n",
      "Epoch [9/10], Step [99/439], Loss: 0.3482\n",
      "Epoch [9/10], Step [109/439], Loss: 0.2442\n",
      "Epoch [9/10], Step [119/439], Loss: 0.2362\n",
      "Epoch [9/10], Step [129/439], Loss: 0.2395\n",
      "Epoch [9/10], Step [139/439], Loss: 0.2482\n",
      "Epoch [9/10], Step [149/439], Loss: 0.2913\n",
      "Epoch [9/10], Step [159/439], Loss: 0.2780\n",
      "Epoch [9/10], Step [169/439], Loss: 0.3687\n",
      "Epoch [9/10], Step [179/439], Loss: 0.3132\n",
      "Epoch [9/10], Step [189/439], Loss: 0.3011\n",
      "Epoch [9/10], Step [199/439], Loss: 0.3810\n",
      "Epoch [9/10], Step [209/439], Loss: 0.3228\n",
      "Epoch [9/10], Step [219/439], Loss: 0.3885\n",
      "Epoch [9/10], Step [229/439], Loss: 0.1491\n",
      "Epoch [9/10], Step [239/439], Loss: 0.3426\n",
      "Epoch [9/10], Step [249/439], Loss: 0.2242\n",
      "Epoch [9/10], Step [259/439], Loss: 0.2670\n",
      "Epoch [9/10], Step [269/439], Loss: 0.3801\n",
      "Epoch [9/10], Step [279/439], Loss: 0.3644\n",
      "Epoch [9/10], Step [289/439], Loss: 0.2590\n",
      "Epoch [9/10], Step [299/439], Loss: 0.2842\n",
      "Epoch [9/10], Step [309/439], Loss: 0.2120\n",
      "Epoch [9/10], Step [319/439], Loss: 0.3388\n",
      "Epoch [9/10], Step [329/439], Loss: 0.3362\n",
      "Epoch [9/10], Step [339/439], Loss: 0.1882\n",
      "Epoch [9/10], Step [349/439], Loss: 0.4174\n",
      "Epoch [9/10], Step [359/439], Loss: 0.3692\n",
      "Epoch [9/10], Step [369/439], Loss: 0.3480\n",
      "Epoch [9/10], Step [379/439], Loss: 0.3272\n",
      "Epoch [9/10], Step [389/439], Loss: 0.3809\n",
      "Epoch [9/10], Step [399/439], Loss: 0.3781\n",
      "Epoch [9/10], Step [409/439], Loss: 0.3736\n",
      "Epoch [9/10], Step [419/439], Loss: 0.2285\n",
      "Epoch [9/10], Step [429/439], Loss: 0.3051\n",
      "Epoch [10/10], Step [9/439], Loss: 0.2993\n",
      "Epoch [10/10], Step [19/439], Loss: 0.2051\n",
      "Epoch [10/10], Step [29/439], Loss: 0.2215\n",
      "Epoch [10/10], Step [39/439], Loss: 0.2975\n",
      "Epoch [10/10], Step [49/439], Loss: 0.2446\n",
      "Epoch [10/10], Step [59/439], Loss: 0.2608\n",
      "Epoch [10/10], Step [69/439], Loss: 0.2213\n",
      "Epoch [10/10], Step [79/439], Loss: 0.3065\n",
      "Epoch [10/10], Step [89/439], Loss: 0.3452\n",
      "Epoch [10/10], Step [99/439], Loss: 0.3060\n",
      "Epoch [10/10], Step [109/439], Loss: 0.3113\n",
      "Epoch [10/10], Step [119/439], Loss: 0.2723\n",
      "Epoch [10/10], Step [129/439], Loss: 0.2138\n",
      "Epoch [10/10], Step [139/439], Loss: 0.3599\n",
      "Epoch [10/10], Step [149/439], Loss: 0.3079\n",
      "Epoch [10/10], Step [159/439], Loss: 0.2339\n",
      "Epoch [10/10], Step [169/439], Loss: 0.3204\n",
      "Epoch [10/10], Step [179/439], Loss: 0.2948\n",
      "Epoch [10/10], Step [189/439], Loss: 0.1938\n",
      "Epoch [10/10], Step [199/439], Loss: 0.3733\n",
      "Epoch [10/10], Step [209/439], Loss: 0.3110\n",
      "Epoch [10/10], Step [219/439], Loss: 0.2066\n",
      "Epoch [10/10], Step [229/439], Loss: 0.3398\n",
      "Epoch [10/10], Step [239/439], Loss: 0.4567\n",
      "Epoch [10/10], Step [249/439], Loss: 0.2416\n",
      "Epoch [10/10], Step [259/439], Loss: 0.2714\n",
      "Epoch [10/10], Step [269/439], Loss: 0.3151\n",
      "Epoch [10/10], Step [279/439], Loss: 0.2389\n",
      "Epoch [10/10], Step [289/439], Loss: 0.2760\n",
      "Epoch [10/10], Step [299/439], Loss: 0.3018\n",
      "Epoch [10/10], Step [309/439], Loss: 0.2098\n",
      "Epoch [10/10], Step [319/439], Loss: 0.3927\n",
      "Epoch [10/10], Step [329/439], Loss: 0.3816\n",
      "Epoch [10/10], Step [339/439], Loss: 0.2923\n",
      "Epoch [10/10], Step [349/439], Loss: 0.3931\n",
      "Epoch [10/10], Step [359/439], Loss: 0.2592\n",
      "Epoch [10/10], Step [369/439], Loss: 0.3249\n",
      "Epoch [10/10], Step [379/439], Loss: 0.2846\n",
      "Epoch [10/10], Step [389/439], Loss: 0.2999\n",
      "Epoch [10/10], Step [399/439], Loss: 0.2316\n",
      "Epoch [10/10], Step [409/439], Loss: 0.2761\n",
      "Epoch [10/10], Step [419/439], Loss: 0.2762\n",
      "Epoch [10/10], Step [429/439], Loss: 0.3585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(0, num_epochs)):    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        \n",
    "        batch_inputs = sample_batched['input'].to(device)\n",
    "        batch_targets = sample_batched['target'].to(device)\n",
    "        \n",
    "        batch_size = batch_targets.size(0)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_inputs) # torch.Size([32, 128, 10])\n",
    "        \n",
    "        # Compute loss\n",
    "        # TODO: rearrange 설명\n",
    "        batch_predicts = rearrange(outputs, 'b c l -> b l c') # torch.Size([32, 10, 128])\n",
    "        loss = criterion(batch_predicts, batch_targets) # torch.Size([32, 128])\n",
    "        \n",
    "        # We will be using a custom loss function that will ignore the loss from padded tokens.\n",
    "        mask = torch.tensor(batch_targets > 0, dtype=float)\n",
    "        loss = loss * mask\n",
    "        loss = torch.sum(loss)/torch.sum(mask)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i_batch+1)%10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i_batch, total_step, loss.item())) \n",
    "    \n",
    "    # Save the model checkpoints\n",
    "    torch.save(model.state_dict(), './models/ner_transformer_encoder_adamW-{}.ckpt'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "protected-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# AdamW\n",
    "\n",
    "# Sample inference using the trained model\n",
    "tokens = map_record_to_training_data('9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0')[0]\n",
    "\n",
    "sample_input = encode_tokens(tokens)\n",
    "\n",
    "sample_input = torch.tensor(sample_input).reshape(1,-1)\n",
    "\n",
    "output = model(sample_input)\n",
    "output = output.detach().numpy()\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-reform",
   "metadata": {},
   "source": [
    "##### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "print ('Current device : ', device)\n",
    "model = NERModel(num_tags, vocab_size, max_seq_len=128, d_model=32, heads=4, d_ff=64).to(device)\n",
    "\n",
    "num_epochs=10\n",
    "total_step = len(train_loader)\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index=0, reduce=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-blind",
   "metadata": {},
   "source": [
    "* 20 epochs -> loss 0.1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "apparent-newman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a9df4a9eb74930827aacfb56c56ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [9/439], Loss: 0.2818\n",
      "Epoch [1/10], Step [19/439], Loss: 0.1901\n",
      "Epoch [1/10], Step [29/439], Loss: 0.2182\n",
      "Epoch [1/10], Step [39/439], Loss: 0.1845\n",
      "Epoch [1/10], Step [49/439], Loss: 0.1964\n",
      "Epoch [1/10], Step [59/439], Loss: 0.3194\n",
      "Epoch [1/10], Step [69/439], Loss: 0.3472\n",
      "Epoch [1/10], Step [79/439], Loss: 0.3391\n",
      "Epoch [1/10], Step [89/439], Loss: 0.2417\n",
      "Epoch [1/10], Step [99/439], Loss: 0.2686\n",
      "Epoch [1/10], Step [109/439], Loss: 0.3027\n",
      "Epoch [1/10], Step [119/439], Loss: 0.2418\n",
      "Epoch [1/10], Step [129/439], Loss: 0.3359\n",
      "Epoch [1/10], Step [139/439], Loss: 0.2458\n",
      "Epoch [1/10], Step [149/439], Loss: 0.3245\n",
      "Epoch [1/10], Step [159/439], Loss: 0.3471\n",
      "Epoch [1/10], Step [169/439], Loss: 0.2666\n",
      "Epoch [1/10], Step [179/439], Loss: 0.2484\n",
      "Epoch [1/10], Step [189/439], Loss: 0.2526\n",
      "Epoch [1/10], Step [199/439], Loss: 0.2453\n",
      "Epoch [1/10], Step [209/439], Loss: 0.2295\n",
      "Epoch [1/10], Step [219/439], Loss: 0.2842\n",
      "Epoch [1/10], Step [229/439], Loss: 0.3511\n",
      "Epoch [1/10], Step [239/439], Loss: 0.2315\n",
      "Epoch [1/10], Step [249/439], Loss: 0.3231\n",
      "Epoch [1/10], Step [259/439], Loss: 0.3494\n",
      "Epoch [1/10], Step [269/439], Loss: 0.2589\n",
      "Epoch [1/10], Step [279/439], Loss: 0.3154\n",
      "Epoch [1/10], Step [289/439], Loss: 0.2568\n",
      "Epoch [1/10], Step [299/439], Loss: 0.2244\n",
      "Epoch [1/10], Step [309/439], Loss: 0.2758\n",
      "Epoch [1/10], Step [319/439], Loss: 0.3656\n",
      "Epoch [1/10], Step [329/439], Loss: 0.4060\n",
      "Epoch [1/10], Step [339/439], Loss: 0.2406\n",
      "Epoch [1/10], Step [349/439], Loss: 0.2950\n",
      "Epoch [1/10], Step [359/439], Loss: 0.3003\n",
      "Epoch [1/10], Step [369/439], Loss: 0.2293\n",
      "Epoch [1/10], Step [379/439], Loss: 0.3240\n",
      "Epoch [1/10], Step [389/439], Loss: 0.2180\n",
      "Epoch [1/10], Step [399/439], Loss: 0.2884\n",
      "Epoch [1/10], Step [409/439], Loss: 0.2584\n",
      "Epoch [1/10], Step [419/439], Loss: 0.2057\n",
      "Epoch [1/10], Step [429/439], Loss: 0.3222\n",
      "Epoch [2/10], Step [9/439], Loss: 0.3540\n",
      "Epoch [2/10], Step [19/439], Loss: 0.2358\n",
      "Epoch [2/10], Step [29/439], Loss: 0.2376\n",
      "Epoch [2/10], Step [39/439], Loss: 0.3799\n",
      "Epoch [2/10], Step [49/439], Loss: 0.2177\n",
      "Epoch [2/10], Step [59/439], Loss: 0.2899\n",
      "Epoch [2/10], Step [69/439], Loss: 0.1883\n",
      "Epoch [2/10], Step [79/439], Loss: 0.3220\n",
      "Epoch [2/10], Step [89/439], Loss: 0.1675\n",
      "Epoch [2/10], Step [99/439], Loss: 0.2503\n",
      "Epoch [2/10], Step [109/439], Loss: 0.2845\n",
      "Epoch [2/10], Step [119/439], Loss: 0.2160\n",
      "Epoch [2/10], Step [129/439], Loss: 0.2142\n",
      "Epoch [2/10], Step [139/439], Loss: 0.2124\n",
      "Epoch [2/10], Step [149/439], Loss: 0.2668\n",
      "Epoch [2/10], Step [159/439], Loss: 0.2701\n",
      "Epoch [2/10], Step [169/439], Loss: 0.2125\n",
      "Epoch [2/10], Step [179/439], Loss: 0.1889\n",
      "Epoch [2/10], Step [189/439], Loss: 0.3151\n",
      "Epoch [2/10], Step [199/439], Loss: 0.1740\n",
      "Epoch [2/10], Step [209/439], Loss: 0.2970\n",
      "Epoch [2/10], Step [219/439], Loss: 0.1946\n",
      "Epoch [2/10], Step [229/439], Loss: 0.2914\n",
      "Epoch [2/10], Step [239/439], Loss: 0.2070\n",
      "Epoch [2/10], Step [249/439], Loss: 0.2533\n",
      "Epoch [2/10], Step [259/439], Loss: 0.2595\n",
      "Epoch [2/10], Step [269/439], Loss: 0.2646\n",
      "Epoch [2/10], Step [279/439], Loss: 0.2759\n",
      "Epoch [2/10], Step [289/439], Loss: 0.1985\n",
      "Epoch [2/10], Step [299/439], Loss: 0.1974\n",
      "Epoch [2/10], Step [309/439], Loss: 0.2254\n",
      "Epoch [2/10], Step [319/439], Loss: 0.3134\n",
      "Epoch [2/10], Step [329/439], Loss: 0.2273\n",
      "Epoch [2/10], Step [339/439], Loss: 0.3706\n",
      "Epoch [2/10], Step [349/439], Loss: 0.2652\n",
      "Epoch [2/10], Step [359/439], Loss: 0.2671\n",
      "Epoch [2/10], Step [369/439], Loss: 0.2177\n",
      "Epoch [2/10], Step [379/439], Loss: 0.5434\n",
      "Epoch [2/10], Step [389/439], Loss: 0.2285\n",
      "Epoch [2/10], Step [399/439], Loss: 0.2010\n",
      "Epoch [2/10], Step [409/439], Loss: 0.2847\n",
      "Epoch [2/10], Step [419/439], Loss: 0.2906\n",
      "Epoch [2/10], Step [429/439], Loss: 0.2853\n",
      "Epoch [3/10], Step [9/439], Loss: 0.2109\n",
      "Epoch [3/10], Step [19/439], Loss: 0.1312\n",
      "Epoch [3/10], Step [29/439], Loss: 0.1552\n",
      "Epoch [3/10], Step [39/439], Loss: 0.2493\n",
      "Epoch [3/10], Step [49/439], Loss: 0.2502\n",
      "Epoch [3/10], Step [59/439], Loss: 0.2370\n",
      "Epoch [3/10], Step [69/439], Loss: 0.2636\n",
      "Epoch [3/10], Step [79/439], Loss: 0.2589\n",
      "Epoch [3/10], Step [89/439], Loss: 0.2589\n",
      "Epoch [3/10], Step [99/439], Loss: 0.2490\n",
      "Epoch [3/10], Step [109/439], Loss: 0.1867\n",
      "Epoch [3/10], Step [119/439], Loss: 0.2343\n",
      "Epoch [3/10], Step [129/439], Loss: 0.2321\n",
      "Epoch [3/10], Step [139/439], Loss: 0.3128\n",
      "Epoch [3/10], Step [149/439], Loss: 0.2238\n",
      "Epoch [3/10], Step [159/439], Loss: 0.3109\n",
      "Epoch [3/10], Step [169/439], Loss: 0.2319\n",
      "Epoch [3/10], Step [179/439], Loss: 0.1865\n",
      "Epoch [3/10], Step [189/439], Loss: 0.2499\n",
      "Epoch [3/10], Step [199/439], Loss: 0.1824\n",
      "Epoch [3/10], Step [209/439], Loss: 0.3370\n",
      "Epoch [3/10], Step [219/439], Loss: 0.2163\n",
      "Epoch [3/10], Step [229/439], Loss: 0.2173\n",
      "Epoch [3/10], Step [239/439], Loss: 0.3784\n",
      "Epoch [3/10], Step [249/439], Loss: 0.1764\n",
      "Epoch [3/10], Step [259/439], Loss: 0.2085\n",
      "Epoch [3/10], Step [269/439], Loss: 0.2884\n",
      "Epoch [3/10], Step [279/439], Loss: 0.2503\n",
      "Epoch [3/10], Step [289/439], Loss: 0.2051\n",
      "Epoch [3/10], Step [299/439], Loss: 0.2496\n",
      "Epoch [3/10], Step [309/439], Loss: 0.1869\n",
      "Epoch [3/10], Step [319/439], Loss: 0.1843\n",
      "Epoch [3/10], Step [329/439], Loss: 0.2463\n",
      "Epoch [3/10], Step [339/439], Loss: 0.1967\n",
      "Epoch [3/10], Step [349/439], Loss: 0.2314\n",
      "Epoch [3/10], Step [359/439], Loss: 0.1703\n",
      "Epoch [3/10], Step [369/439], Loss: 0.2003\n",
      "Epoch [3/10], Step [379/439], Loss: 0.2897\n",
      "Epoch [3/10], Step [389/439], Loss: 0.1687\n",
      "Epoch [3/10], Step [399/439], Loss: 0.2130\n",
      "Epoch [3/10], Step [409/439], Loss: 0.2700\n",
      "Epoch [3/10], Step [419/439], Loss: 0.3087\n",
      "Epoch [3/10], Step [429/439], Loss: 0.2160\n",
      "Epoch [4/10], Step [9/439], Loss: 0.2059\n",
      "Epoch [4/10], Step [19/439], Loss: 0.3160\n",
      "Epoch [4/10], Step [29/439], Loss: 0.3310\n",
      "Epoch [4/10], Step [39/439], Loss: 0.2206\n",
      "Epoch [4/10], Step [49/439], Loss: 0.1889\n",
      "Epoch [4/10], Step [59/439], Loss: 0.4216\n",
      "Epoch [4/10], Step [69/439], Loss: 0.1914\n",
      "Epoch [4/10], Step [79/439], Loss: 0.2127\n",
      "Epoch [4/10], Step [89/439], Loss: 0.3000\n",
      "Epoch [4/10], Step [99/439], Loss: 0.1960\n",
      "Epoch [4/10], Step [109/439], Loss: 0.2658\n",
      "Epoch [4/10], Step [119/439], Loss: 0.2396\n",
      "Epoch [4/10], Step [129/439], Loss: 0.1574\n",
      "Epoch [4/10], Step [139/439], Loss: 0.1703\n",
      "Epoch [4/10], Step [149/439], Loss: 0.2135\n",
      "Epoch [4/10], Step [159/439], Loss: 0.2365\n",
      "Epoch [4/10], Step [169/439], Loss: 0.2841\n",
      "Epoch [4/10], Step [179/439], Loss: 0.1749\n",
      "Epoch [4/10], Step [189/439], Loss: 0.1933\n",
      "Epoch [4/10], Step [199/439], Loss: 0.1362\n",
      "Epoch [4/10], Step [209/439], Loss: 0.2389\n",
      "Epoch [4/10], Step [219/439], Loss: 0.2397\n",
      "Epoch [4/10], Step [229/439], Loss: 0.2728\n",
      "Epoch [4/10], Step [239/439], Loss: 0.1764\n",
      "Epoch [4/10], Step [249/439], Loss: 0.2092\n",
      "Epoch [4/10], Step [259/439], Loss: 0.2514\n",
      "Epoch [4/10], Step [269/439], Loss: 0.2116\n",
      "Epoch [4/10], Step [279/439], Loss: 0.3102\n",
      "Epoch [4/10], Step [289/439], Loss: 0.2488\n",
      "Epoch [4/10], Step [299/439], Loss: 0.3304\n",
      "Epoch [4/10], Step [309/439], Loss: 0.2522\n",
      "Epoch [4/10], Step [319/439], Loss: 0.2276\n",
      "Epoch [4/10], Step [329/439], Loss: 0.1989\n",
      "Epoch [4/10], Step [339/439], Loss: 0.2305\n",
      "Epoch [4/10], Step [349/439], Loss: 0.2990\n",
      "Epoch [4/10], Step [359/439], Loss: 0.1668\n",
      "Epoch [4/10], Step [369/439], Loss: 0.1404\n",
      "Epoch [4/10], Step [379/439], Loss: 0.2488\n",
      "Epoch [4/10], Step [389/439], Loss: 0.2769\n",
      "Epoch [4/10], Step [399/439], Loss: 0.2662\n",
      "Epoch [4/10], Step [409/439], Loss: 0.2120\n",
      "Epoch [4/10], Step [419/439], Loss: 0.1850\n",
      "Epoch [4/10], Step [429/439], Loss: 0.2240\n",
      "Epoch [5/10], Step [9/439], Loss: 0.1853\n",
      "Epoch [5/10], Step [19/439], Loss: 0.1673\n",
      "Epoch [5/10], Step [29/439], Loss: 0.1869\n",
      "Epoch [5/10], Step [39/439], Loss: 0.1915\n",
      "Epoch [5/10], Step [49/439], Loss: 0.2733\n",
      "Epoch [5/10], Step [59/439], Loss: 0.2262\n",
      "Epoch [5/10], Step [69/439], Loss: 0.2710\n",
      "Epoch [5/10], Step [79/439], Loss: 0.2666\n",
      "Epoch [5/10], Step [89/439], Loss: 0.1641\n",
      "Epoch [5/10], Step [99/439], Loss: 0.1475\n",
      "Epoch [5/10], Step [109/439], Loss: 0.2174\n",
      "Epoch [5/10], Step [119/439], Loss: 0.1685\n",
      "Epoch [5/10], Step [129/439], Loss: 0.1354\n",
      "Epoch [5/10], Step [139/439], Loss: 0.1962\n",
      "Epoch [5/10], Step [149/439], Loss: 0.2268\n",
      "Epoch [5/10], Step [159/439], Loss: 0.1517\n",
      "Epoch [5/10], Step [169/439], Loss: 0.1676\n",
      "Epoch [5/10], Step [179/439], Loss: 0.1840\n",
      "Epoch [5/10], Step [189/439], Loss: 0.2736\n",
      "Epoch [5/10], Step [199/439], Loss: 0.1878\n",
      "Epoch [5/10], Step [209/439], Loss: 0.2694\n",
      "Epoch [5/10], Step [219/439], Loss: 0.2170\n",
      "Epoch [5/10], Step [229/439], Loss: 0.2501\n",
      "Epoch [5/10], Step [239/439], Loss: 0.2394\n",
      "Epoch [5/10], Step [249/439], Loss: 0.3326\n",
      "Epoch [5/10], Step [259/439], Loss: 0.1852\n",
      "Epoch [5/10], Step [269/439], Loss: 0.1640\n",
      "Epoch [5/10], Step [279/439], Loss: 0.1613\n",
      "Epoch [5/10], Step [289/439], Loss: 0.3292\n",
      "Epoch [5/10], Step [299/439], Loss: 0.2197\n",
      "Epoch [5/10], Step [309/439], Loss: 0.2038\n",
      "Epoch [5/10], Step [319/439], Loss: 0.2874\n",
      "Epoch [5/10], Step [329/439], Loss: 0.2381\n",
      "Epoch [5/10], Step [339/439], Loss: 0.2832\n",
      "Epoch [5/10], Step [349/439], Loss: 0.2872\n",
      "Epoch [5/10], Step [359/439], Loss: 0.1935\n",
      "Epoch [5/10], Step [369/439], Loss: 0.1644\n",
      "Epoch [5/10], Step [379/439], Loss: 0.1945\n",
      "Epoch [5/10], Step [389/439], Loss: 0.2644\n",
      "Epoch [5/10], Step [399/439], Loss: 0.2035\n",
      "Epoch [5/10], Step [409/439], Loss: 0.1750\n",
      "Epoch [5/10], Step [419/439], Loss: 0.2023\n",
      "Epoch [5/10], Step [429/439], Loss: 0.1671\n",
      "Epoch [6/10], Step [9/439], Loss: 0.2558\n",
      "Epoch [6/10], Step [19/439], Loss: 0.4477\n",
      "Epoch [6/10], Step [29/439], Loss: 0.2401\n",
      "Epoch [6/10], Step [39/439], Loss: 0.1877\n",
      "Epoch [6/10], Step [49/439], Loss: 0.1449\n",
      "Epoch [6/10], Step [59/439], Loss: 0.1768\n",
      "Epoch [6/10], Step [69/439], Loss: 0.1328\n",
      "Epoch [6/10], Step [79/439], Loss: 0.1992\n",
      "Epoch [6/10], Step [89/439], Loss: 0.2725\n",
      "Epoch [6/10], Step [99/439], Loss: 0.1467\n",
      "Epoch [6/10], Step [109/439], Loss: 0.1582\n",
      "Epoch [6/10], Step [119/439], Loss: 0.1449\n",
      "Epoch [6/10], Step [129/439], Loss: 0.1495\n",
      "Epoch [6/10], Step [139/439], Loss: 0.2654\n",
      "Epoch [6/10], Step [149/439], Loss: 0.2724\n",
      "Epoch [6/10], Step [159/439], Loss: 0.1383\n",
      "Epoch [6/10], Step [169/439], Loss: 0.1412\n",
      "Epoch [6/10], Step [179/439], Loss: 0.1753\n",
      "Epoch [6/10], Step [189/439], Loss: 0.2199\n",
      "Epoch [6/10], Step [199/439], Loss: 0.3061\n",
      "Epoch [6/10], Step [209/439], Loss: 0.1306\n",
      "Epoch [6/10], Step [219/439], Loss: 0.2259\n",
      "Epoch [6/10], Step [229/439], Loss: 0.1573\n",
      "Epoch [6/10], Step [239/439], Loss: 0.2370\n",
      "Epoch [6/10], Step [249/439], Loss: 0.1774\n",
      "Epoch [6/10], Step [259/439], Loss: 0.1158\n",
      "Epoch [6/10], Step [269/439], Loss: 0.2840\n",
      "Epoch [6/10], Step [279/439], Loss: 0.1367\n",
      "Epoch [6/10], Step [289/439], Loss: 0.1854\n",
      "Epoch [6/10], Step [299/439], Loss: 0.1677\n",
      "Epoch [6/10], Step [309/439], Loss: 0.1336\n",
      "Epoch [6/10], Step [319/439], Loss: 0.1306\n",
      "Epoch [6/10], Step [329/439], Loss: 0.1191\n",
      "Epoch [6/10], Step [339/439], Loss: 0.1771\n",
      "Epoch [6/10], Step [349/439], Loss: 0.2477\n",
      "Epoch [6/10], Step [359/439], Loss: 0.2251\n",
      "Epoch [6/10], Step [369/439], Loss: 0.2494\n",
      "Epoch [6/10], Step [379/439], Loss: 0.1980\n",
      "Epoch [6/10], Step [389/439], Loss: 0.2554\n",
      "Epoch [6/10], Step [399/439], Loss: 0.3223\n",
      "Epoch [6/10], Step [409/439], Loss: 0.1450\n",
      "Epoch [6/10], Step [419/439], Loss: 0.1995\n",
      "Epoch [6/10], Step [429/439], Loss: 0.1686\n",
      "Epoch [7/10], Step [9/439], Loss: 0.2039\n",
      "Epoch [7/10], Step [19/439], Loss: 0.1966\n",
      "Epoch [7/10], Step [29/439], Loss: 0.1803\n",
      "Epoch [7/10], Step [39/439], Loss: 0.2156\n",
      "Epoch [7/10], Step [49/439], Loss: 0.1447\n",
      "Epoch [7/10], Step [59/439], Loss: 0.1854\n",
      "Epoch [7/10], Step [69/439], Loss: 0.1370\n",
      "Epoch [7/10], Step [79/439], Loss: 0.1554\n",
      "Epoch [7/10], Step [89/439], Loss: 0.3921\n",
      "Epoch [7/10], Step [99/439], Loss: 0.1722\n",
      "Epoch [7/10], Step [109/439], Loss: 0.1599\n",
      "Epoch [7/10], Step [119/439], Loss: 0.2467\n",
      "Epoch [7/10], Step [129/439], Loss: 0.2982\n",
      "Epoch [7/10], Step [139/439], Loss: 0.3039\n",
      "Epoch [7/10], Step [149/439], Loss: 0.1863\n",
      "Epoch [7/10], Step [159/439], Loss: 0.1122\n",
      "Epoch [7/10], Step [169/439], Loss: 0.2275\n",
      "Epoch [7/10], Step [179/439], Loss: 0.2392\n",
      "Epoch [7/10], Step [189/439], Loss: 0.1054\n",
      "Epoch [7/10], Step [199/439], Loss: 0.1114\n",
      "Epoch [7/10], Step [209/439], Loss: 0.2129\n",
      "Epoch [7/10], Step [219/439], Loss: 0.1062\n",
      "Epoch [7/10], Step [229/439], Loss: 0.1873\n",
      "Epoch [7/10], Step [239/439], Loss: 0.1737\n",
      "Epoch [7/10], Step [249/439], Loss: 0.2442\n",
      "Epoch [7/10], Step [259/439], Loss: 0.2405\n",
      "Epoch [7/10], Step [269/439], Loss: 0.2013\n",
      "Epoch [7/10], Step [279/439], Loss: 0.1279\n",
      "Epoch [7/10], Step [289/439], Loss: 0.1916\n",
      "Epoch [7/10], Step [299/439], Loss: 0.1482\n",
      "Epoch [7/10], Step [309/439], Loss: 0.3174\n",
      "Epoch [7/10], Step [319/439], Loss: 0.2301\n",
      "Epoch [7/10], Step [329/439], Loss: 0.2132\n",
      "Epoch [7/10], Step [339/439], Loss: 0.1287\n",
      "Epoch [7/10], Step [349/439], Loss: 0.1716\n",
      "Epoch [7/10], Step [359/439], Loss: 0.2630\n",
      "Epoch [7/10], Step [369/439], Loss: 0.1778\n",
      "Epoch [7/10], Step [379/439], Loss: 0.1960\n",
      "Epoch [7/10], Step [389/439], Loss: 0.1683\n",
      "Epoch [7/10], Step [399/439], Loss: 0.1540\n",
      "Epoch [7/10], Step [409/439], Loss: 0.2309\n",
      "Epoch [7/10], Step [419/439], Loss: 0.2696\n",
      "Epoch [7/10], Step [429/439], Loss: 0.1464\n",
      "Epoch [8/10], Step [9/439], Loss: 0.1625\n",
      "Epoch [8/10], Step [19/439], Loss: 0.2109\n",
      "Epoch [8/10], Step [29/439], Loss: 0.1507\n",
      "Epoch [8/10], Step [39/439], Loss: 0.2112\n",
      "Epoch [8/10], Step [49/439], Loss: 0.1822\n",
      "Epoch [8/10], Step [59/439], Loss: 0.2544\n",
      "Epoch [8/10], Step [69/439], Loss: 0.2353\n",
      "Epoch [8/10], Step [79/439], Loss: 0.1617\n",
      "Epoch [8/10], Step [89/439], Loss: 0.2076\n",
      "Epoch [8/10], Step [99/439], Loss: 0.2083\n",
      "Epoch [8/10], Step [109/439], Loss: 0.2462\n",
      "Epoch [8/10], Step [119/439], Loss: 0.2297\n",
      "Epoch [8/10], Step [129/439], Loss: 0.1348\n",
      "Epoch [8/10], Step [139/439], Loss: 0.1625\n",
      "Epoch [8/10], Step [149/439], Loss: 0.1844\n",
      "Epoch [8/10], Step [159/439], Loss: 0.2518\n",
      "Epoch [8/10], Step [169/439], Loss: 0.2480\n",
      "Epoch [8/10], Step [179/439], Loss: 0.2141\n",
      "Epoch [8/10], Step [189/439], Loss: 0.1591\n",
      "Epoch [8/10], Step [199/439], Loss: 0.0954\n",
      "Epoch [8/10], Step [209/439], Loss: 0.2007\n",
      "Epoch [8/10], Step [219/439], Loss: 0.1417\n",
      "Epoch [8/10], Step [229/439], Loss: 0.2196\n",
      "Epoch [8/10], Step [239/439], Loss: 0.1901\n",
      "Epoch [8/10], Step [249/439], Loss: 0.1527\n",
      "Epoch [8/10], Step [259/439], Loss: 0.1503\n",
      "Epoch [8/10], Step [269/439], Loss: 0.2198\n",
      "Epoch [8/10], Step [279/439], Loss: 0.2581\n",
      "Epoch [8/10], Step [289/439], Loss: 0.2208\n",
      "Epoch [8/10], Step [299/439], Loss: 0.2553\n",
      "Epoch [8/10], Step [309/439], Loss: 0.1438\n",
      "Epoch [8/10], Step [319/439], Loss: 0.2507\n",
      "Epoch [8/10], Step [329/439], Loss: 0.1784\n",
      "Epoch [8/10], Step [339/439], Loss: 0.1971\n",
      "Epoch [8/10], Step [349/439], Loss: 0.1389\n",
      "Epoch [8/10], Step [359/439], Loss: 0.2259\n",
      "Epoch [8/10], Step [369/439], Loss: 0.1985\n",
      "Epoch [8/10], Step [379/439], Loss: 0.1784\n",
      "Epoch [8/10], Step [389/439], Loss: 0.1508\n",
      "Epoch [8/10], Step [399/439], Loss: 0.2235\n",
      "Epoch [8/10], Step [409/439], Loss: 0.2758\n",
      "Epoch [8/10], Step [419/439], Loss: 0.2240\n",
      "Epoch [8/10], Step [429/439], Loss: 0.2732\n",
      "Epoch [9/10], Step [9/439], Loss: 0.1849\n",
      "Epoch [9/10], Step [19/439], Loss: 0.1326\n",
      "Epoch [9/10], Step [29/439], Loss: 0.2428\n",
      "Epoch [9/10], Step [39/439], Loss: 0.1664\n",
      "Epoch [9/10], Step [49/439], Loss: 0.1448\n",
      "Epoch [9/10], Step [59/439], Loss: 0.1582\n",
      "Epoch [9/10], Step [69/439], Loss: 0.1682\n",
      "Epoch [9/10], Step [79/439], Loss: 0.1362\n",
      "Epoch [9/10], Step [89/439], Loss: 0.1684\n",
      "Epoch [9/10], Step [99/439], Loss: 0.1654\n",
      "Epoch [9/10], Step [109/439], Loss: 0.3094\n",
      "Epoch [9/10], Step [119/439], Loss: 0.1573\n",
      "Epoch [9/10], Step [129/439], Loss: 0.1477\n",
      "Epoch [9/10], Step [139/439], Loss: 0.2220\n",
      "Epoch [9/10], Step [149/439], Loss: 0.1934\n",
      "Epoch [9/10], Step [159/439], Loss: 0.2167\n",
      "Epoch [9/10], Step [169/439], Loss: 0.1523\n",
      "Epoch [9/10], Step [179/439], Loss: 0.2297\n",
      "Epoch [9/10], Step [189/439], Loss: 0.1534\n",
      "Epoch [9/10], Step [199/439], Loss: 0.1968\n",
      "Epoch [9/10], Step [209/439], Loss: 0.2291\n",
      "Epoch [9/10], Step [219/439], Loss: 0.1498\n",
      "Epoch [9/10], Step [229/439], Loss: 0.1749\n",
      "Epoch [9/10], Step [239/439], Loss: 0.2474\n",
      "Epoch [9/10], Step [249/439], Loss: 0.2312\n",
      "Epoch [9/10], Step [259/439], Loss: 0.1204\n",
      "Epoch [9/10], Step [269/439], Loss: 0.2460\n",
      "Epoch [9/10], Step [279/439], Loss: 0.2664\n",
      "Epoch [9/10], Step [289/439], Loss: 0.1883\n",
      "Epoch [9/10], Step [299/439], Loss: 0.1728\n",
      "Epoch [9/10], Step [309/439], Loss: 0.1486\n",
      "Epoch [9/10], Step [319/439], Loss: 0.1223\n",
      "Epoch [9/10], Step [329/439], Loss: 0.1865\n",
      "Epoch [9/10], Step [339/439], Loss: 0.1505\n",
      "Epoch [9/10], Step [349/439], Loss: 0.1842\n",
      "Epoch [9/10], Step [359/439], Loss: 0.2338\n",
      "Epoch [9/10], Step [369/439], Loss: 0.1508\n",
      "Epoch [9/10], Step [379/439], Loss: 0.1739\n",
      "Epoch [9/10], Step [389/439], Loss: 0.1260\n",
      "Epoch [9/10], Step [399/439], Loss: 0.1257\n",
      "Epoch [9/10], Step [409/439], Loss: 0.1730\n",
      "Epoch [9/10], Step [419/439], Loss: 0.1748\n",
      "Epoch [9/10], Step [429/439], Loss: 0.1183\n",
      "Epoch [10/10], Step [9/439], Loss: 0.1212\n",
      "Epoch [10/10], Step [19/439], Loss: 0.2638\n",
      "Epoch [10/10], Step [29/439], Loss: 0.1172\n",
      "Epoch [10/10], Step [39/439], Loss: 0.0884\n",
      "Epoch [10/10], Step [49/439], Loss: 0.2159\n",
      "Epoch [10/10], Step [59/439], Loss: 0.3718\n",
      "Epoch [10/10], Step [69/439], Loss: 0.1587\n",
      "Epoch [10/10], Step [79/439], Loss: 0.2498\n",
      "Epoch [10/10], Step [89/439], Loss: 0.1878\n",
      "Epoch [10/10], Step [99/439], Loss: 0.1586\n",
      "Epoch [10/10], Step [109/439], Loss: 0.1708\n",
      "Epoch [10/10], Step [119/439], Loss: 0.1457\n",
      "Epoch [10/10], Step [129/439], Loss: 0.1129\n",
      "Epoch [10/10], Step [139/439], Loss: 0.1536\n",
      "Epoch [10/10], Step [149/439], Loss: 0.2001\n",
      "Epoch [10/10], Step [159/439], Loss: 0.1591\n",
      "Epoch [10/10], Step [169/439], Loss: 0.1703\n",
      "Epoch [10/10], Step [179/439], Loss: 0.2256\n",
      "Epoch [10/10], Step [189/439], Loss: 0.1948\n",
      "Epoch [10/10], Step [199/439], Loss: 0.1135\n",
      "Epoch [10/10], Step [209/439], Loss: 0.1237\n",
      "Epoch [10/10], Step [219/439], Loss: 0.2101\n",
      "Epoch [10/10], Step [229/439], Loss: 0.1775\n",
      "Epoch [10/10], Step [239/439], Loss: 0.2037\n",
      "Epoch [10/10], Step [249/439], Loss: 0.1776\n",
      "Epoch [10/10], Step [259/439], Loss: 0.1352\n",
      "Epoch [10/10], Step [269/439], Loss: 0.1987\n",
      "Epoch [10/10], Step [279/439], Loss: 0.1424\n",
      "Epoch [10/10], Step [289/439], Loss: 0.2400\n",
      "Epoch [10/10], Step [299/439], Loss: 0.1577\n",
      "Epoch [10/10], Step [309/439], Loss: 0.1136\n",
      "Epoch [10/10], Step [319/439], Loss: 0.1610\n",
      "Epoch [10/10], Step [329/439], Loss: 0.1576\n",
      "Epoch [10/10], Step [339/439], Loss: 0.1410\n",
      "Epoch [10/10], Step [349/439], Loss: 0.2103\n",
      "Epoch [10/10], Step [359/439], Loss: 0.1312\n",
      "Epoch [10/10], Step [369/439], Loss: 0.1547\n",
      "Epoch [10/10], Step [379/439], Loss: 0.2098\n",
      "Epoch [10/10], Step [389/439], Loss: 0.1087\n",
      "Epoch [10/10], Step [399/439], Loss: 0.1542\n",
      "Epoch [10/10], Step [409/439], Loss: 0.1788\n",
      "Epoch [10/10], Step [419/439], Loss: 0.3071\n",
      "Epoch [10/10], Step [429/439], Loss: 0.2228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(0, num_epochs)):    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        \n",
    "        batch_inputs = sample_batched['input'].to(device)\n",
    "        batch_targets = sample_batched['target'].to(device)\n",
    "        \n",
    "        batch_size = batch_targets.size(0)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_inputs) # torch.Size([32, 128, 10])\n",
    "        \n",
    "        # Compute loss\n",
    "        batch_predicts = rearrange(outputs, 'b c l -> b l c') # torch.Size([32, 10, 128])\n",
    "        loss = criterion(batch_predicts, batch_targets) # torch.Size([32, 128])\n",
    "        \n",
    "        mask = torch.tensor(batch_targets > 0, dtype=float)\n",
    "        loss = loss * mask\n",
    "        loss = torch.sum(loss)/torch.sum(mask)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i_batch+1)%10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i_batch, total_step, loss.item())) \n",
    "    \n",
    "    # Save the model checkpoints\n",
    "    torch.save(model.state_dict(), './models/ner_transformer_encoder_adam-{}.ckpt'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "unnecessary-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Adam\n",
    "\n",
    "# Sample inference using the trained model\n",
    "tokens = map_record_to_training_data('9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0')[0]\n",
    "\n",
    "sample_input = encode_tokens(tokens)\n",
    "sample_input = torch.tensor(sample_input).reshape(1,-1)\n",
    "\n",
    "output = model(sample_input)\n",
    "output = output.detach().numpy()\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]\n",
    "\n",
    "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-edition",
   "metadata": {},
   "source": [
    "## Metrics calculation\n",
    "\n",
    "Here is a function to calculate the metrics. The function calculates F1 score for the\n",
    "overall NER dataset as well as individual scores for each NER tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "equal-sector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 6368 phrases; correct: 3035.\n",
      "accuracy:  49.88%; (non-O)\n",
      "accuracy:  89.86%; precision:  47.66%; recall:  51.08%; FB1:  49.31\n",
      "              LOC: precision:  73.34%; recall:  69.03%; FB1:  71.12  1729\n",
      "             MISC: precision:  54.96%; recall:  60.09%; FB1:  57.41  1008\n",
      "              ORG: precision:  40.13%; recall:  46.09%; FB1:  42.90  1540\n",
      "              PER: precision:  28.46%; recall:  32.30%; FB1:  30.26  2091\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for i, batch_sample in enumerate(dataset):\n",
    "        \n",
    "        x = batch_sample['input'].reshape(1,-1)\n",
    "        y = batch_sample['target'].reshape(1,-1)\n",
    "\n",
    "        output = model(x) # [32, len, 10]\n",
    "        output = output.detach().numpy()\n",
    "        \n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y.detach().numpy(), [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-soldier",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-poison",
   "metadata": {},
   "source": [
    "* CE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "instructional-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3545, -1.2197,  0.5757, -1.4133,  0.8292],\n",
      "        [ 0.3147, -1.1512, -1.1049, -1.0452,  1.3127],\n",
      "        [-1.1328,  0.8679, -1.5172, -0.1275,  0.0885]], requires_grad=True)\n",
      "tensor([3, 0, 0])\n",
      "tensor(2.4318, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.tensor([3,0,0])\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "interesting-flour",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3545, -1.2197,  0.5757, -1.4133,  0.8292],\n",
      "        [ 0.3147, -1.1512, -1.1049, -1.0452,  1.3127],\n",
      "        [-1.1328,  0.8679, -1.5172, -0.1275,  0.0885]], requires_grad=True)\n",
      "tensor(3.0829, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "print(input)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "global-wheat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3545, -1.2197,  0.5757, -1.4133,  0.8292],\n",
      "        [ 0.3147, -1.1512, -1.1049, -1.0452,  1.3127],\n",
      "        [-1.1328,  0.8679, -1.5172, -0.1275,  0.0885]], requires_grad=True)\n",
      "tensor([3.0829, 0.0000, 0.0000], grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(ignore_index=0, reduce=False)\n",
    "print(input)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "geographic-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3545, -1.2197,  0.5757, -1.4133,  0.8292],\n",
      "        [ 0.3147, -1.1512, -1.1049, -1.0452,  1.3127],\n",
      "        [-1.1328,  0.8679, -1.5172, -0.1275,  0.0885]], requires_grad=True)\n",
      "tensor([3.0829, 1.4911, 2.7213], grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduce=False)\n",
    "print(input)\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-buyer",
   "metadata": {},
   "source": [
    "* rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "powerful-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> original input\n",
      "tensor([[[-0.3545,  0.3147, -1.1328, -1.2197, -1.1512],\n",
      "         [ 0.8679,  0.5757, -1.1049, -1.5172, -1.4133],\n",
      "         [-1.0452, -0.1275,  0.8292,  1.3127,  0.0885]]],\n",
      "       grad_fn=<ViewBackward>)\n",
      ">> by rearrange\n",
      "tensor([[[-0.3545,  0.8679, -1.0452],\n",
      "         [ 0.3147,  0.5757, -0.1275],\n",
      "         [-1.1328, -1.1049,  0.8292],\n",
      "         [-1.2197, -1.5172,  1.3127],\n",
      "         [-1.1512, -1.4133,  0.0885]]], grad_fn=<ViewBackward>)\n",
      ">> by reshape\n",
      "tensor([[[-0.3545,  0.3147, -1.1328],\n",
      "         [-1.2197, -1.1512,  0.8679],\n",
      "         [ 0.5757, -1.1049, -1.5172],\n",
      "         [-1.4133, -1.0452, -0.1275],\n",
      "         [ 0.8292,  1.3127,  0.0885]]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = input.reshape(1,3,5)\n",
    "# input = torch.tensor(np.transpose(input.detach().numpy(), (0,2,1)))\n",
    "print(\">> original input\")\n",
    "print(input)\n",
    "input_rearranged = rearrange(input, 'b c l -> b l c')\n",
    "print(\">> by rearrange\")\n",
    "print(input_rearranged)\n",
    "\n",
    "print(\">> by reshape\")\n",
    "input_reshaped = input.reshape(1,5,3)\n",
    "print(input_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-ireland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
